{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasources.smp\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys,os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))\n",
    "os.chdir(Path.cwd().parent / 'extracao')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serviço Móvel Pessoal\n",
    "> Módulo para encapsular a extração e processamento do Serviço Móvel Pessoal - Telefonia e Banda Larga Móvel - 2G, 3G, 4G e 5G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from functools import cached_property\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "from extracao.constants import (\n",
    "    AGG_SMP,\n",
    "    CHANNELS,\n",
    "    COLS_LICENCIAMENTO,\n",
    "    DICT_LICENCIAMENTO,\n",
    "    IBGE,\n",
    "    MONGO_SMP,\n",
    "    PROJECTION_LICENCIAMENTO,\n",
    ")\n",
    "from extracao.datasources.mosaico import Mosaico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide: true\n",
    "#| eval:false\n",
    "__file__ = Path.cwd().parent / 'extracao' / 'datasources.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMP(Mosaico):\n",
    "    \"\"\"Classe para encapsular a lógica de extração do SMP\"\"\"\n",
    "\n",
    "    def __init__(self, mongo_uri: str = MONGO_URI, limit: int = 0) -> None:\n",
    "        super().__init__(mongo_uri)\n",
    "        self.limit = limit\n",
    "\n",
    "    @property\n",
    "    def stem(self):\n",
    "        return \"smp\"\n",
    "\n",
    "    @property\n",
    "    def collection(self):\n",
    "        return \"licenciamento\"\n",
    "\n",
    "    @property\n",
    "    def query(self):\n",
    "        return MONGO_SMP\n",
    "\n",
    "    @property\n",
    "    def projection(self):\n",
    "        return PROJECTION_LICENCIAMENTO\n",
    "\n",
    "    @property\n",
    "    def columns(self):\n",
    "        return COLS_LICENCIAMENTO\n",
    "\n",
    "    @property\n",
    "    def cols_mapping(self):\n",
    "        return DICT_LICENCIAMENTO\n",
    "\n",
    "    @cached_property\n",
    "    def extraction(self) -> pd.DataFrame:\n",
    "        pipeline = [{\"$match\": self.query}, {\"$project\": self.projection}]\n",
    "        if self.limit > 0:\n",
    "            pipeline.append({\"$limit\": self.limit})\n",
    "        df = self._extract(self.collection, pipeline)\n",
    "        df[\"Log\"] = \"\"\n",
    "        return df\n",
    "\n",
    "    def exclude_duplicated(\n",
    "        self,\n",
    "        df: pd.DataFrame,  # DataFrame com os dados de Estações\n",
    "    ) -> pd.DataFrame:  # DataFrame com os dados duplicados excluídos\n",
    "        f\"\"\"Exclui os registros duplicados\n",
    "        O subconjunto de colunas consideradas é {AGG_SMP}\n",
    "        \"\"\"\n",
    "        df[\"Número_Estação\"] = df[\"Número_Estação\"].astype(\"int\")\n",
    "        df = df.sort_values(\"Número_Estação\", ignore_index=True)\n",
    "        df[\"Largura_Emissão(kHz)\"] = pd.to_numeric(\n",
    "            df[\"Largura_Emissão(kHz)\"], errors=\"coerce\"\n",
    "        )\n",
    "        df[\"Largura_Emissão(kHz)\"] = df[\"Largura_Emissão(kHz)\"].fillna(0)\n",
    "        df[\"Classe_Emissão\"] = df[\"Classe_Emissão\"].fillna(\"NI\")\n",
    "        df[\"Tecnologia\"] = df[\"Tecnologia\"].fillna(\"NI\")\n",
    "        duplicated = df.duplicated(subset=AGG_SMP, keep=\"first\")\n",
    "        df_sub = df[~duplicated].copy().reset_index(drop=True)\n",
    "        discarded = df[duplicated].copy().reset_index(drop=True)\n",
    "        log = f\"\"\"[(\"Colunas\", {AGG_SMP}),  \n",
    "                (\"Processamento\", \"Registro agrupado e descartado do arquivo final\")]\"\"\"\n",
    "        self.append2discarded(self.register_log(discarded, log))\n",
    "        for col in AGG_SMP:\n",
    "            discarded_with_na = df_sub[df_sub[col].isna()]\n",
    "            log = f\"\"\"[(\"Colunas\", {col}),  \n",
    "                    (\"Processamento\", \"Registro com valor nulo presente\")]\"\"\"\n",
    "            self.append2discarded(self.register_log(discarded_with_na, log))\n",
    "        df_sub = df_sub.dropna(subset=AGG_SMP)\n",
    "        df_sub[\"Multiplicidade\"] = df.groupby(AGG_SMP, sort=False).size().tolist()\n",
    "        df_sub[\"Status\"] = \"L\"\n",
    "        df_sub[\"Fonte\"] = \"MOSAICO\"\n",
    "        log = f'[(\"Colunas\", {AGG_SMP}), (\"Processamento\", \"Agrupamento\")]'\n",
    "        return self.register_log(df_sub, log, df_sub[\"Multiplicidade\"] > 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_channels():\n",
    "        channels = pd.read_csv(CHANNELS, dtype=\"string\")\n",
    "        cols = [\"Downlink_Inicial\", \"Downlink_Final\", \"Uplink_Inicial\", \"Uplink_Final\"]\n",
    "        channels[cols] = channels[cols].astype(\"float\")\n",
    "        channels = channels.sort_values([\"Downlink_Inicial\"], ignore_index=True)\n",
    "        channels[\"N_Bloco\"] = channels[\"N_Bloco\"].str.strip()\n",
    "        channels[\"Faixa\"] = channels[\"Faixa\"].str.strip()\n",
    "        return channels\n",
    "\n",
    "    def exclude_invalid_channels(\n",
    "        self,\n",
    "        df: pd.DataFrame,  # DataFrame de Origem\n",
    "    ) -> pd.DataFrame:  # DataFrame com os canais inválidos excluídos\n",
    "        df_sub = df[df.Canalização == \"Downlink\"].copy().reset_index(drop=True)\n",
    "        for flag in [\"Uplink\", \"Inválida\"]:\n",
    "            discarded = df[df.Canalização == flag].copy()\n",
    "            if not discarded.empty:\n",
    "                log = f\"\"\"[(\"Colunas\", (\"Frequência\", \"Largura_Emissão(kHz)\")),  \n",
    "                         (\"Processamento\", \"Canalização {flag}\")]\"\"\"\n",
    "                self.append2discarded(self.register_log(discarded, log))\n",
    "        return df_sub\n",
    "\n",
    "    def validate_channels(\n",
    "        self,\n",
    "        df: pd.DataFrame,  # DataFrame with the original channels info\n",
    "    ) -> pd.DataFrame:  # DataFrame with the channels validated and added info\n",
    "        \"\"\"Read the SMP channels file, validate and merge the channels present in df\"\"\"\n",
    "        bw = df[\"Largura_Emissão(kHz)\"].astype(\"float\") / 2000  # Unidade em kHz\n",
    "        df[\"Início_Canal_Down\"] = df.Frequência.astype(float) - bw\n",
    "        df[\"Fim_Canal_Down\"] = df.Frequência.astype(float) + bw\n",
    "        channels = self.read_channels()\n",
    "        grouped_channels = df.groupby(\n",
    "            [\"Início_Canal_Down\", \"Fim_Canal_Down\"], as_index=False\n",
    "        ).size()\n",
    "        grouped_channels.sort_values(\n",
    "            \"size\", ascending=False, inplace=True, ignore_index=True\n",
    "        )\n",
    "        grouped_channels[\"Canalização\"] = \"Inválida\"\n",
    "        grouped_channels.loc[:, \"Offset\"] = np.nan\n",
    "        grouped_channels.loc[:, [\"Blocos_Downlink\", \"Faixas\"]] = pd.NA\n",
    "        grouped_channels.loc[\n",
    "            :, [\"Blocos_Downlink\", \"Faixas\", \"Canalização\"]\n",
    "        ] = grouped_channels.loc[\n",
    "            :, [\"Blocos_Downlink\", \"Faixas\", \"Canalização\"]\n",
    "        ].astype(\n",
    "            \"string\"\n",
    "        )\n",
    "        grouped_channels.loc[:, \"Offset\"] = grouped_channels.loc[:, \"Offset\"].astype(\n",
    "            \"float\"\n",
    "        )\n",
    "\n",
    "        for row in grouped_channels.itertuples():\n",
    "            interval = channels[\n",
    "                (row.Início_Canal_Down < channels[\"Downlink_Final\"])\n",
    "                & (row.Fim_Canal_Down > channels[\"Downlink_Inicial\"])\n",
    "            ]\n",
    "            faixa = \"Downlink\"\n",
    "            if interval.empty:\n",
    "                interval = channels[\n",
    "                    (row.Início_Canal_Down < channels[\"Uplink_Final\"])\n",
    "                    & (row.Fim_Canal_Down > channels[\"Uplink_Inicial\"])\n",
    "                ]\n",
    "                if interval.empty:\n",
    "                    continue\n",
    "                faixa = \"Uplink\"\n",
    "\n",
    "            down = \" | \".join(\n",
    "                interval[[\"Downlink_Inicial\", \"Downlink_Final\"]].apply(\n",
    "                    lambda x: f\"{x.iloc[0]}-{x.iloc[1]}\", axis=1\n",
    "                )\n",
    "            )\n",
    "            faixas = \" | \".join(interval.Faixa.unique())\n",
    "            if len(offset := interval.Offset.unique()) != 1:\n",
    "                continue\n",
    "            grouped_channels.loc[\n",
    "                row.Index, [\"Blocos_Downlink\", \"Faixas\", \"Canalização\", \"Offset\"]\n",
    "            ] = (down, faixas, faixa, float(offset[0]))\n",
    "        grouped_channels = grouped_channels[\n",
    "            [\n",
    "                \"Início_Canal_Down\",\n",
    "                \"Fim_Canal_Down\",\n",
    "                \"Blocos_Downlink\",\n",
    "                \"Faixas\",\n",
    "                \"Canalização\",\n",
    "                \"Offset\",\n",
    "            ]\n",
    "        ]\n",
    "        df = pd.merge(\n",
    "            df, grouped_channels, how=\"left\", on=[\"Início_Canal_Down\", \"Fim_Canal_Down\"]\n",
    "        )\n",
    "        return self.exclude_invalid_channels(df)\n",
    "\n",
    "    def generate_uplink(\n",
    "        self,\n",
    "        df: pd.DataFrame,  # DataFrame de Origem\n",
    "    ) -> pd.DataFrame:  # DataFrame com os canais de Uplink adicionados\n",
    "        df[\"Offset\"] = pd.to_numeric(df[\"Offset\"], errors=\"coerce\").astype(\"float\")\n",
    "        df[\"Largura_Emissão(kHz)\"] = pd.to_numeric(\n",
    "            df[\"Largura_Emissão(kHz)\"], errors=\"coerce\"\n",
    "        ).astype(\"float\")\n",
    "        valid = (\n",
    "            (df.Offset.notna())\n",
    "            & (~np.isclose(df.Offset, 0))\n",
    "            & (df[\"Largura_Emissão(kHz)\"].notna())\n",
    "            & (~np.isclose(df[\"Largura_Emissão(kHz)\"], 0))\n",
    "        )\n",
    "        df.loc[:, [\"Frequência\", \"Offset\"]] = df.loc[\n",
    "            :, [\"Frequência\", \"Offset\"]\n",
    "        ].astype(\"float\")\n",
    "        df.loc[valid, \"Frequência_Recepção\"] = (\n",
    "            df.loc[valid, \"Frequência\"] - df.loc[valid, \"Offset\"]\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def substitute_coordenates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        ibge = pd.read_csv(\n",
    "            IBGE,\n",
    "            dtype=\"string\",\n",
    "            usecols=[\"codigo_ibge\", \"nome\", \"latitude\", \"longitude\"],\n",
    "        )\n",
    "        ibge.columns = [\"Código_Município\", \"Município\", \"Latitude\", \"Longitude\"]\n",
    "        coords = pd.merge(\n",
    "            df.loc[df.Multiplicidade > 1, \"Código_Município\"],\n",
    "            ibge[[\"Código_Município\", \"Latitude\", \"Longitude\"]],\n",
    "            on=\"Código_Município\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        df.loc[df.Multiplicidade > 1, [\"Latitude\", \"Longitude\"]] = coords[\n",
    "            [\"Latitude\", \"Longitude\"]\n",
    "        ].values\n",
    "        log = \"\"\"[(\"Colunas\", (\"Latitude\", \"Longitude\")), \n",
    "        (\"Processamento\", \"Substituição por Coordenadas do Município (Agrupamento)\")]\"\"\"\n",
    "        return self.register_log(df, log, df.Multiplicidade > 1)\n",
    "\n",
    "    def input_fixed_columns(\n",
    "        self,\n",
    "        df: pd.DataFrame,  # DataFrame de Origem\n",
    "    ) -> (\n",
    "        pd.DataFrame\n",
    "    ):  # DataFrame com os canais de downlink e uplink contenados e formatados\n",
    "        df[\"Status\"] = \"L\"\n",
    "        df[\"Num_Serviço\"] = \"010\"\n",
    "        down = df.drop(\"Frequência_Recepção\", axis=1)\n",
    "        down[\"Fonte\"] = \"MOSAICO\"\n",
    "        down[\"Classe\"] = \"FB\"\n",
    "        up = df.drop(\"Frequência\", axis=1)\n",
    "        up = up.rename(columns={\"Frequência_Recepção\": \"Frequência\"})\n",
    "        up.dropna(subset=\"Frequência\", inplace=True)\n",
    "        up[\"Fonte\"] = \"CANALIZACAO-SMP\"\n",
    "        up[\"Classe\"] = \"ML\"\n",
    "        return pd.concat([down, up], ignore_index=True)\n",
    "\n",
    "    def _format(\n",
    "        self,\n",
    "        df: pd.DataFrame,  # DataFrame com os dados de Estações e Plano_Básico mesclados\n",
    "    ) -> pd.DataFrame:  # DataFrame com os dados mesclados e limpos\n",
    "        \"\"\"Clean the merged dataframe with the data from the MOSAICO page\"\"\"\n",
    "        df = df.rename(columns=self.cols_mapping)\n",
    "        df = self.split_designacao(df)\n",
    "        df = self.exclude_duplicated(df)\n",
    "        df = self.validate_channels(df)\n",
    "        df = self.generate_uplink(df)\n",
    "        df = self.substitute_coordenates(df)\n",
    "        df = self.input_fixed_columns(df)\n",
    "        return df.loc[:, self.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1457/2230402201.py\", line 9, in <module>\n",
      "    data.update()\n",
      "  File \"/mnt/c/Users/rsilva/OneDrive - ANATEL/Code/anateldb/extracao/datasources/base.py\", line 117, in update\n",
      "    self.df = self._format(self.extraction)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/functools.py\", line 967, in __get__\n",
      "    val = self.func(instance)\n",
      "  File \"/tmp/ipykernel_1457/2279196232.py\", line 38, in extraction\n",
      "    df = self._extract(self.collection, pipeline)\n",
      "  File \"/mnt/c/Users/rsilva/OneDrive - ANATEL/Code/anateldb/extracao/datasources/mosaico.py\", line 49, in _extract\n",
      "    result = collection.aggregate(pipeline)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/collection.py\", line 2721, in aggregate\n",
      "    with self.__database.client._tmp_session(session, close=False) as s:\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/contextlib.py\", line 113, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/mongo_client.py\", line 1885, in _tmp_session\n",
      "    s = self._ensure_session(session)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/mongo_client.py\", line 1868, in _ensure_session\n",
      "    return self.__start_session(True, causal_consistency=False)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/mongo_client.py\", line 1811, in __start_session\n",
      "    self._topology._check_implicit_session_support()\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/topology.py\", line 583, in _check_implicit_session_support\n",
      "    self._check_session_support()\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/topology.py\", line 599, in _check_session_support\n",
      "    self._select_servers_loop(\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/topology.py\", line 280, in _select_servers_loop\n",
      "    self._condition.wait(common.MIN_HEARTBEAT_INTERVAL)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/threading.py\", line 306, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1030, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1098, in get_records\n",
      "    mod = inspect.getmodule(cf.tb_frame)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/inspect.py\", line 699, in getsourcefile\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/inspect.py\", line 699, in <genexpr>\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "AttributeError: 'PosixPath' object has no attribute 'endswith'\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    data = SMP()\n",
    "\n",
    "    data.update()\n",
    "\n",
    "    print(\"DATA\")\n",
    "\n",
    "    display(data.df)\n",
    "\n",
    "    print(150 * \"=\")\n",
    "\n",
    "    print(\"DISCARDED!\")\n",
    "\n",
    "    display(data.discarded[[\"Frequência\", \"Entidade\", \"Log\"]])\n",
    "\n",
    "    print(150 * \"=\")\n",
    "\n",
    "    print(data.df.Multiplicidade.sum())\n",
    "\n",
    "    data.save()\n",
    "\n",
    "    print(f\"Elapsed time: {time.perf_counter() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
