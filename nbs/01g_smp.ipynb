{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasources.smp\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys,os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))\n",
    "os.chdir(Path.cwd().parent / 'extracao')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serviço Móvel Pessoal\n",
    "> Módulo para encapsular a extração e processamento do Serviço Móvel Pessoal - Telefonia e Banda Larga Móvel - 2G, 3G, 4G e 5G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "from extracao.constants import (\n",
    "\tAGG_SMP,\n",
    "\tCHANNELS,\n",
    "\tCOLUNAS,\n",
    "\tDICT_LICENCIAMENTO,\n",
    "\tMONGO_SMP,\n",
    "\tPROJECTION_LICENCIAMENTO,\n",
    ")\n",
    "from extracao.datasources.mosaico import Mosaico\n",
    "from extracao.location import Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide: true\n",
    "#| eval:false\n",
    "__file__ = Path.cwd().parent / 'extracao' / 'datasources.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "MONGO_URI = os.environ.get(\"MONGO_URI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMP(Mosaico):\n",
    "\t\"\"\"Classe para encapsular a lógica de extração do SMP\"\"\"\n",
    "\n",
    "\tdef __init__(self, mongo_uri: str = MONGO_URI, limit: int = 0) -> None:\n",
    "\t\tsuper().__init__(mongo_uri)\n",
    "\t\tself.limit = limit\n",
    "\n",
    "\t@property\n",
    "\tdef stem(self):\n",
    "\t\treturn 'smp'\n",
    "\n",
    "\t@property\n",
    "\tdef collection(self):\n",
    "\t\treturn 'licenciamento'\n",
    "\n",
    "\t@property\n",
    "\tdef query(self):\n",
    "\t\treturn MONGO_SMP\n",
    "\n",
    "\t@property\n",
    "\tdef projection(self):\n",
    "\t\treturn PROJECTION_LICENCIAMENTO\n",
    "\n",
    "\t@property\n",
    "\tdef columns(self):\n",
    "\t\treturn COLUNAS\n",
    "\n",
    "\t@property\n",
    "\tdef cols_mapping(self):\n",
    "\t\treturn DICT_LICENCIAMENTO\n",
    "\n",
    "\tdef extraction(self) -> pd.DataFrame:\n",
    "\t\t\"\"\"This method returns a DataFrame with the results of the mongo query\"\"\"\n",
    "\t\tpipeline = [{'$match': self.query}, {'$project': self.projection}]\n",
    "\t\tif self.limit > 0:\n",
    "\t\t\tpipeline.append({'$limit': self.limit})\n",
    "\t\tdf = self._extract(self.collection, pipeline)\n",
    "\t\tdf['Log'] = ''\n",
    "\t\treturn df\n",
    "\n",
    "\tdef exclude_duplicated(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame com os dados de Estações\n",
    "\t) -> pd.DataFrame:  # DataFrame com os dados duplicados excluídos\n",
    "\t\tf\"\"\"Exclude the duplicated rows\n",
    "        Columns considered are defined by the AGG_SMP constant\n",
    "        \"\"\"\n",
    "\t\tdf['Estação'] = (\n",
    "\t\t\tdf['Estação'].astype('string', copy=False).fillna('-1').astype('int', copy=False)\n",
    "\t\t)\n",
    "\t\tdf = df.sort_values('Estação', ignore_index=True)\n",
    "\t\tdf['Largura_Emissão(kHz)'] = pd.to_numeric(df['Largura_Emissão(kHz)'], errors='coerce')\n",
    "\t\t# df['Largura_Emissão(kHz)'] = df['Largura_Emissão(kHz)'].fillna(0)\n",
    "\t\t# df['Classe_Emissão'] = df['Classe_Emissão'].fillna('NI')\n",
    "\t\t# df['Tecnologia'] = df['Tecnologia'].fillna('NI')\n",
    "\t\tduplicated = df.duplicated(subset=AGG_SMP, keep='first')\n",
    "\t\tdf_sub = df[~duplicated].copy().reset_index(drop=True)\n",
    "\t\t# discarded = df[duplicated].copy().reset_index(drop=True)\n",
    "\t\t# log = f\"\"\"[(\"Colunas\", {AGG_SMP}),\n",
    "\t\t#         (\"Processamento\", \"Registro agrupado e descartado do arquivo final\")]\"\"\"\n",
    "\t\t# self.append2discarded(self.register_log(discarded, log))\n",
    "\t\t# for col in AGG_SMP:\n",
    "\t\t#     discarded_with_na = df_sub[df_sub[col].isna()]\n",
    "\t\t#     log = f\"\"\"[(\"Colunas\", {col}),\n",
    "\t\t#             (\"Processamento\", \"Registro com valor nulo presente\")]\"\"\"\n",
    "\t\t#     self.append2discarded(self.register_log(discarded_with_na, log))\n",
    "\t\tdf_sub.dropna(subset=AGG_SMP, inplace=True)\n",
    "\t\tdf_sub['Multiplicidade'] = (\n",
    "\t\t\tdf.groupby(AGG_SMP, dropna=True, sort=False, observed=True).size().values\n",
    "\t\t)\n",
    "\t\tlog = f'[(\"Colunas\", {AGG_SMP}), (\"Processamento\", \"Agrupamento\")]'\n",
    "\t\treturn self.register_log(df_sub, log, df_sub['Multiplicidade'] > 1)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef read_channels():\n",
    "\t\t\"\"\"Reads and formats the SMP channels files\"\"\"\n",
    "\t\tchannels = pd.read_csv(CHANNELS, dtype='string')\n",
    "\t\tcols = ['Downlink_Inicial', 'Downlink_Final', 'Uplink_Inicial', 'Uplink_Final']\n",
    "\t\tchannels[cols] = channels[cols].astype('float')\n",
    "\t\tchannels = channels.sort_values(['Downlink_Inicial'], ignore_index=True)\n",
    "\t\tchannels['N_Bloco'] = channels['N_Bloco'].str.strip()\n",
    "\t\tchannels['Faixa'] = channels['Faixa'].str.strip()\n",
    "\t\treturn channels\n",
    "\n",
    "\tdef exclude_invalid_channels(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame de Origem\n",
    "\t) -> pd.DataFrame:  # DataFrame com os canais inválidos excluídos\n",
    "\t\t\"\"\"Helper function to keep only the valid downlink channels\"\"\"\n",
    "\t\tdf_sub = df[df.Canalização == 'Downlink'].reset_index(drop=True)\n",
    "\t\t# for flag in [\"Uplink\", \"Inválida\"]:\n",
    "\t\t#     discarded = df[df.Canalização == flag]\n",
    "\t\t#     if not discarded.empty:\n",
    "\t\t#         log = f\"\"\"[(\"Colunas\", (\"Frequência\", \"Largura_Emissão(kHz)\")),\n",
    "\t\t#                  (\"Processamento\", \"Canalização {flag}\")]\"\"\"\n",
    "\t\t#         self.append2discarded(self.register_log(discarded, log))\n",
    "\t\treturn df_sub\n",
    "\n",
    "\tdef validate_channels(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame with the original channels info\n",
    "\t) -> pd.DataFrame:  # DataFrame with the channels validated and added info\n",
    "\t\t\"\"\"Read the SMP channels file, validate and merge the channels present in df\"\"\"\n",
    "\t\tbw = df['Largura_Emissão(kHz)'].astype('float') / 2000  # Unidade em kHz\n",
    "\t\tdf['Início_Canal_Down'] = df.Frequência.astype(float) - bw\n",
    "\t\tdf['Fim_Canal_Down'] = df.Frequência.astype(float) + bw\n",
    "\t\tchannels = self.read_channels()\n",
    "\t\tgrouped_channels = df.groupby(\n",
    "\t\t\t['Início_Canal_Down', 'Fim_Canal_Down'], as_index=False\n",
    "\t\t).size()\n",
    "\t\tgrouped_channels.sort_values('size', ascending=False, inplace=True, ignore_index=True)\n",
    "\t\tgrouped_channels['Canalização'] = 'Inválida'\n",
    "\t\tgrouped_channels['Offset'] = np.nan\n",
    "\t\tgrouped_channels['Blocos_Downlink'] = pd.NA\n",
    "\t\tgrouped_channels['Faixas'] = pd.NA\n",
    "\t\tgrouped_channels[['Blocos_Downlink', 'Faixas', 'Canalização']] = grouped_channels[\n",
    "\t\t\t['Blocos_Downlink', 'Faixas', 'Canalização']\n",
    "\t\t].astype('string', copy=False)\n",
    "\t\tgrouped_channels['Offset'] = grouped_channels['Offset'].astype('float', copy=False)\n",
    "\n",
    "\t\tfor row in grouped_channels.itertuples():\n",
    "\t\t\tinterval = channels[\n",
    "\t\t\t\t(row.Início_Canal_Down < channels['Downlink_Final'])\n",
    "\t\t\t\t& (row.Fim_Canal_Down > channels['Downlink_Inicial'])\n",
    "\t\t\t]\n",
    "\t\t\tfaixa = 'Downlink'\n",
    "\t\t\tif interval.empty:\n",
    "\t\t\t\tinterval = channels[\n",
    "\t\t\t\t\t(row.Início_Canal_Down < channels['Uplink_Final'])\n",
    "\t\t\t\t\t& (row.Fim_Canal_Down > channels['Uplink_Inicial'])\n",
    "\t\t\t\t]\n",
    "\t\t\t\tif interval.empty:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tfaixa = 'Uplink'\n",
    "\n",
    "\t\t\tdown = ' | '.join(\n",
    "\t\t\t\tinterval[['Downlink_Inicial', 'Downlink_Final']].apply(\n",
    "\t\t\t\t\tlambda x: f'{x.iloc[0]}-{x.iloc[1]}', axis=1\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\t\t\tfaixas = ' | '.join(interval.Faixa.unique())\n",
    "\t\t\tif len(offset := interval.Offset.unique()) != 1:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tgrouped_channels.loc[\n",
    "\t\t\t\trow.Index, ['Blocos_Downlink', 'Faixas', 'Canalização', 'Offset']\n",
    "\t\t\t] = (down, faixas, faixa, float(offset[0]))\n",
    "\t\tgrouped_channels = grouped_channels[\n",
    "\t\t\t[\n",
    "\t\t\t\t'Início_Canal_Down',\n",
    "\t\t\t\t'Fim_Canal_Down',\n",
    "\t\t\t\t'Blocos_Downlink',\n",
    "\t\t\t\t'Faixas',\n",
    "\t\t\t\t'Canalização',\n",
    "\t\t\t\t'Offset',\n",
    "\t\t\t]\n",
    "\t\t]\n",
    "\t\tdf = pd.merge(df, grouped_channels, how='left', on=['Início_Canal_Down', 'Fim_Canal_Down'])\n",
    "\t\treturn self.exclude_invalid_channels(df)\n",
    "\n",
    "\tdef generate_uplink(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # Source dataFrame with downlink frequencies and offset\n",
    "\t) -> pd.DataFrame:  # DataFrame with the uplink frequencies added\n",
    "\t\t\"\"\"Generate the respective Uplink channels based on the Downlink frequencies and Offset\"\"\"\n",
    "\t\tdf['Offset'] = pd.to_numeric(df['Offset'], errors='coerce').astype('float')\n",
    "\t\tdf['Largura_Emissão(kHz)'] = pd.to_numeric(\n",
    "\t\t\tdf['Largura_Emissão(kHz)'], errors='coerce'\n",
    "\t\t).astype('float')\n",
    "\t\tvalid = (\n",
    "\t\t\t(df.Offset.notna())\n",
    "\t\t\t& (~np.isclose(df.Offset, 0))\n",
    "\t\t\t& (df['Largura_Emissão(kHz)'].notna())\n",
    "\t\t\t& (~np.isclose(df['Largura_Emissão(kHz)'], 0))\n",
    "\t\t)\n",
    "\t\tdf[['Frequência', 'Offset']] = df[['Frequência', 'Offset']].astype('float')\n",
    "\t\tdf.loc[valid, 'Frequência_Recepção'] = df.loc[valid, 'Frequência'] - df.loc[valid, 'Offset']\n",
    "\t\treturn df\n",
    "\n",
    "\tdef substitute_coordinates(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # Source dataframe\n",
    "\t) -> pd.DataFrame:  # Source dataframe with coordinates replace for the city one\n",
    "\t\t\"\"\"Substitute the coordinates for the central coordinates of the municipality\n",
    "\t\tOnly does it for the grouped rows (Multiplicity > 1) since for these rows the\n",
    "\t\tcoordinate values are no longer valid.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tgeo = Geography(df)\n",
    "\t\tdf = geo.merge_df_with_ibge(df)\n",
    "\t\trows = df.Multiplicidade > 1\n",
    "\t\tdf.loc[rows, 'Latitude'] = df.loc[rows, 'Latitude_IBGE'].copy()\n",
    "\t\tdf.loc[rows, 'Longitude'] = df.loc[rows, 'Longitude_IBGE'].copy()\n",
    "\t\tlog = \"\"\"[(\"Colunas\", (\"Latitude\", \"Longitude\")), \n",
    "        (\"Processamento\", \"Substituição por Coordenadas do Município (Agrupamento)\")]\"\"\"\n",
    "\t\treturn self.register_log(df, log, df.Multiplicidade > 1)\n",
    "\n",
    "\tdef input_fixed_columns(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # Source dataframe\n",
    "\t) -> pd.DataFrame:  # Cleaned dataframe with some additional columns added\n",
    "\t\t\"\"\"Formats and adds some helper columns to the dataframe\"\"\"\n",
    "\t\tdf['Status'] = 'L'\n",
    "\t\tdf['Serviço'] = '010'\n",
    "\t\tdown = df.drop('Frequência_Recepção', axis=1)\n",
    "\t\tdown['Fonte'] = 'MOSAICO-LICENCIAMENTO'\n",
    "\t\tdown['Classe'] = 'FB'\n",
    "\t\tup = df.drop('Frequência', axis=1)\n",
    "\t\tup = up.rename(columns={'Frequência_Recepção': 'Frequência'})\n",
    "\t\tup.dropna(subset='Frequência', inplace=True)\n",
    "\t\tup['Fonte'] = 'CANALIZACAO-SMP'\n",
    "\t\tup['Classe'] = 'ML'\n",
    "\t\treturn pd.concat([down, up], ignore_index=True)\n",
    "\n",
    "\tdef _format(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # Source dataframe\n",
    "\t) -> pd.DataFrame:  # Final processed dataframe\n",
    "\t\t\"\"\"Formats, cleans, groups, adds and standardizes the queried data from the database\"\"\"\n",
    "\t\tdf = df.rename(columns=self.cols_mapping)\n",
    "\t\tdf = self.split_designacao(df)\n",
    "\t\tdf = self.exclude_duplicated(df)\n",
    "\t\tdf = self.validate_channels(df)\n",
    "\t\tdf = self.generate_uplink(df)\n",
    "\t\tdf = self.substitute_coordenates(df)\n",
    "\t\tdf = self.input_fixed_columns(df)\n",
    "\t\treturn df.loc[:, self.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval:false\n",
    "smp = SMP(limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_665453/3139869699.py\", line 1, in <module>\n",
      "    e = smp.extraction()\n",
      "  File \"/tmp/ipykernel_665453/1092737479.py\", line 38, in extraction\n",
      "    df = self._extract(self.collection, pipeline)\n",
      "  File \"/home/ronaldo/code/rfdatahub/extracao/datasources/mosaico.py\", line 47, in _extract\n",
      "    df = pd.DataFrame(list(db_collection.aggregate(pipeline)), copy=False, dtype='string')\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/pymongo/collection.py\", line 2719, in aggregate\n",
      "    with self.__database.client._tmp_session(session, close=False) as s:\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/contextlib.py\", line 113, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/pymongo/mongo_client.py\", line 1840, in _tmp_session\n",
      "    s = self._ensure_session(session)\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/pymongo/mongo_client.py\", line 1823, in _ensure_session\n",
      "    return self.__start_session(True, causal_consistency=False)\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/pymongo/mongo_client.py\", line 1766, in __start_session\n",
      "    self._topology._check_implicit_session_support()\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/pymongo/topology.py\", line 573, in _check_implicit_session_support\n",
      "    self._check_session_support()\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/pymongo/topology.py\", line 589, in _check_session_support\n",
      "    self._select_servers_loop(\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/pymongo/topology.py\", line 259, in _select_servers_loop\n",
      "    raise ServerSelectionTimeoutError(\n",
      "pymongo.errors.ServerSelectionTimeoutError: anatelbdro06:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 10000.0ms, connectTimeoutMS: 10000.0ms), Timeout: 5.0s, Topology Description: <TopologyDescription id: 65d34d016622ccf3643986e9, topology_type: Unknown, servers: [<ServerDescription ('anatelbdro06', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('anatelbdro06:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 10000.0ms, connectTimeoutMS: 10000.0ms)')>]>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1030, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1098, in get_records\n",
      "    mod = inspect.getmodule(cf.tb_frame)\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/inspect.py\", line 699, in getsourcefile\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "  File \"/home/ronaldo/micromamba/envs/db/lib/python3.8/inspect.py\", line 699, in <genexpr>\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "AttributeError: 'PosixPath' object has no attribute 'endswith'\n"
     ]
    }
   ],
   "source": [
    "# |eval:false\n",
    "e = smp.extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
