{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasources.smp\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys,os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))\n",
    "os.chdir(Path.cwd().parent / 'extracao')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serviço Móvel Pessoal\n",
    "> Módulo para encapsular a extração e processamento do Serviço Móvel Pessoal - Telefonia e Banda Larga Móvel - 2G, 3G, 4G e 5G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from functools import cached_property\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "from extracao.constants import (\n",
    "\tAGG_SMP,\n",
    "\tCHANNELS,\n",
    "\tCOLS_LICENCIAMENTO,\n",
    "\tDICT_LICENCIAMENTO,\n",
    "\tIBGE_MUNICIPIOS,\n",
    "\tMONGO_SMP,\n",
    "\tPROJECTION_LICENCIAMENTO,\n",
    ")\n",
    "from extracao.datasources.mosaico import Mosaico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide: true\n",
    "#| eval:false\n",
    "__file__ = Path.cwd().parent / 'extracao' / 'datasources.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "MONGO_URI = os.environ.get('MONGO_URI')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SMP(Mosaico):\n",
    "\t\"\"\"Classe para encapsular a lógica de extração do SMP\"\"\"\n",
    "\n",
    "\tdef __init__(self, mongo_uri: str = MONGO_URI, limit: int = 0) -> None:\n",
    "\t\tsuper().__init__(mongo_uri)\n",
    "\t\tself.limit = limit\n",
    "\n",
    "\t@property\n",
    "\tdef stem(self):\n",
    "\t\treturn 'smp'\n",
    "\n",
    "\t@property\n",
    "\tdef collection(self):\n",
    "\t\treturn 'licenciamento'\n",
    "\n",
    "\t@property\n",
    "\tdef query(self):\n",
    "\t\treturn MONGO_SMP\n",
    "\n",
    "\t@property\n",
    "\tdef projection(self):\n",
    "\t\treturn PROJECTION_LICENCIAMENTO\n",
    "\n",
    "\t@property\n",
    "\tdef columns(self):\n",
    "\t\treturn COLS_LICENCIAMENTO\n",
    "\n",
    "\t@property\n",
    "\tdef cols_mapping(self):\n",
    "\t\treturn DICT_LICENCIAMENTO\n",
    "\n",
    "\t@cached_property\n",
    "\tdef extraction(self) -> pd.DataFrame:\n",
    "\t\tpipeline = [{'$match': self.query}, {'$project': self.projection}]\n",
    "\t\tif self.limit > 0:\n",
    "\t\t\tpipeline.append({'$limit': self.limit})\n",
    "\t\tdf = self._extract(self.collection, pipeline)\n",
    "\t\tdf['Log'] = ''\n",
    "\t\treturn df\n",
    "\n",
    "\tdef exclude_duplicated(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame com os dados de Estações\n",
    "\t) -> pd.DataFrame:  # DataFrame com os dados duplicados excluídos\n",
    "\t\tf\"\"\"Exclui os registros duplicados\n",
    "        O subconjunto de colunas consideradas é {AGG_SMP}\n",
    "        \"\"\"\n",
    "\t\tdf['Número_Estação'] = df['Número_Estação'].astype('int')\n",
    "\t\tdf = df.sort_values('Número_Estação', ignore_index=True)\n",
    "\t\tdf['Largura_Emissão(kHz)'] = pd.to_numeric(df['Largura_Emissão(kHz)'], errors='coerce')\n",
    "\t\tdf['Largura_Emissão(kHz)'] = df['Largura_Emissão(kHz)'].fillna(0)\n",
    "\t\tdf['Classe_Emissão'] = df['Classe_Emissão'].fillna('NI')\n",
    "\t\tdf['Tecnologia'] = df['Tecnologia'].fillna('NI')\n",
    "\t\tduplicated = df.duplicated(subset=AGG_SMP, keep='first')\n",
    "\t\tdf_sub = df[~duplicated].copy().reset_index(drop=True)\n",
    "\t\t# discarded = df[duplicated].copy().reset_index(drop=True)\n",
    "\t\t# log = f\"\"\"[(\"Colunas\", {AGG_SMP}),\n",
    "\t\t#         (\"Processamento\", \"Registro agrupado e descartado do arquivo final\")]\"\"\"\n",
    "\t\t# self.append2discarded(self.register_log(discarded, log))\n",
    "\t\t# for col in AGG_SMP:\n",
    "\t\t#     discarded_with_na = df_sub[df_sub[col].isna()]\n",
    "\t\t#     log = f\"\"\"[(\"Colunas\", {col}),\n",
    "\t\t#             (\"Processamento\", \"Registro com valor nulo presente\")]\"\"\"\n",
    "\t\t#     self.append2discarded(self.register_log(discarded_with_na, log))\n",
    "\t\t# df_sub = df_sub.dropna(subset=AGG_SMP)\n",
    "\t\tdf_sub['Multiplicidade'] = df.groupby(AGG_SMP, dropna=True, sort=False).size().tolist()\n",
    "\t\tdf_sub['Status'] = 'L'\n",
    "\t\tdf_sub['Fonte'] = 'MOSAICO'\n",
    "\t\tlog = f'[(\"Colunas\", {AGG_SMP}), (\"Processamento\", \"Agrupamento\")]'\n",
    "\t\treturn self.register_log(df_sub, log, df_sub['Multiplicidade'] > 1)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef read_channels():\n",
    "\t\tchannels = pd.read_csv(CHANNELS, dtype='string')\n",
    "\t\tcols = ['Downlink_Inicial', 'Downlink_Final', 'Uplink_Inicial', 'Uplink_Final']\n",
    "\t\tchannels[cols] = channels[cols].astype('float')\n",
    "\t\tchannels = channels.sort_values(['Downlink_Inicial'], ignore_index=True)\n",
    "\t\tchannels['N_Bloco'] = channels['N_Bloco'].str.strip()\n",
    "\t\tchannels['Faixa'] = channels['Faixa'].str.strip()\n",
    "\t\treturn channels\n",
    "\n",
    "\tdef exclude_invalid_channels(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame de Origem\n",
    "\t) -> pd.DataFrame:  # DataFrame com os canais inválidos excluídos\n",
    "\t\tdf_sub = df[df.Canalização == 'Downlink'].copy().reset_index(drop=True)\n",
    "\t\tfor flag in ['Uplink', 'Inválida']:\n",
    "\t\t\tdiscarded = df[df.Canalização == flag].copy()\n",
    "\t\t\tif not discarded.empty:\n",
    "\t\t\t\tlog = f\"\"\"[(\"Colunas\", (\"Frequência\", \"Largura_Emissão(kHz)\")),  \n",
    "                         (\"Processamento\", \"Canalização {flag}\")]\"\"\"\n",
    "\t\t\t\tself.append2discarded(self.register_log(discarded, log))\n",
    "\t\treturn df_sub\n",
    "\n",
    "\tdef validate_channels(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame with the original channels info\n",
    "\t) -> pd.DataFrame:  # DataFrame with the channels validated and added info\n",
    "\t\t\"\"\"Read the SMP channels file, validate and merge the channels present in df\"\"\"\n",
    "\t\tbw = df['Largura_Emissão(kHz)'].astype('float') / 2000  # Unidade em kHz\n",
    "\t\tdf['Início_Canal_Down'] = df.Frequência.astype(float) - bw\n",
    "\t\tdf['Fim_Canal_Down'] = df.Frequência.astype(float) + bw\n",
    "\t\tchannels = self.read_channels()\n",
    "\t\tgrouped_channels = df.groupby(\n",
    "\t\t\t['Início_Canal_Down', 'Fim_Canal_Down'], as_index=False\n",
    "\t\t).size()\n",
    "\t\tgrouped_channels.sort_values('size', ascending=False, inplace=True, ignore_index=True)\n",
    "\t\tgrouped_channels['Canalização'] = 'Inválida'\n",
    "\t\tgrouped_channels.loc[:, 'Offset'] = np.nan\n",
    "\t\tgrouped_channels.loc[:, ['Blocos_Downlink', 'Faixas']] = pd.NA\n",
    "\t\tgrouped_channels.loc[\n",
    "\t\t\t:, ['Blocos_Downlink', 'Faixas', 'Canalização']\n",
    "\t\t] = grouped_channels.loc[:, ['Blocos_Downlink', 'Faixas', 'Canalização']].astype('string')\n",
    "\t\tgrouped_channels.loc[:, 'Offset'] = grouped_channels.loc[:, 'Offset'].astype('float')\n",
    "\n",
    "\t\tfor row in grouped_channels.itertuples():\n",
    "\t\t\tinterval = channels[\n",
    "\t\t\t\t(row.Início_Canal_Down < channels['Downlink_Final'])\n",
    "\t\t\t\t& (row.Fim_Canal_Down > channels['Downlink_Inicial'])\n",
    "\t\t\t]\n",
    "\t\t\tfaixa = 'Downlink'\n",
    "\t\t\tif interval.empty:\n",
    "\t\t\t\tinterval = channels[\n",
    "\t\t\t\t\t(row.Início_Canal_Down < channels['Uplink_Final'])\n",
    "\t\t\t\t\t& (row.Fim_Canal_Down > channels['Uplink_Inicial'])\n",
    "\t\t\t\t]\n",
    "\t\t\t\tif interval.empty:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tfaixa = 'Uplink'\n",
    "\n",
    "\t\t\tdown = ' | '.join(\n",
    "\t\t\t\tinterval[['Downlink_Inicial', 'Downlink_Final']].apply(\n",
    "\t\t\t\t\tlambda x: f'{x.iloc[0]}-{x.iloc[1]}', axis=1\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\t\t\tfaixas = ' | '.join(interval.Faixa.unique())\n",
    "\t\t\tif len(offset := interval.Offset.unique()) != 1:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tgrouped_channels.loc[\n",
    "\t\t\t\trow.Index, ['Blocos_Downlink', 'Faixas', 'Canalização', 'Offset']\n",
    "\t\t\t] = (down, faixas, faixa, float(offset[0]))\n",
    "\t\tgrouped_channels = grouped_channels[\n",
    "\t\t\t[\n",
    "\t\t\t\t'Início_Canal_Down',\n",
    "\t\t\t\t'Fim_Canal_Down',\n",
    "\t\t\t\t'Blocos_Downlink',\n",
    "\t\t\t\t'Faixas',\n",
    "\t\t\t\t'Canalização',\n",
    "\t\t\t\t'Offset',\n",
    "\t\t\t]\n",
    "\t\t]\n",
    "\t\tdf = pd.merge(df, grouped_channels, how='left', on=['Início_Canal_Down', 'Fim_Canal_Down'])\n",
    "\t\treturn self.exclude_invalid_channels(df)\n",
    "\n",
    "\tdef generate_uplink(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame de Origem\n",
    "\t) -> pd.DataFrame:  # DataFrame com os canais de Uplink adicionados\n",
    "\t\tdf['Offset'] = pd.to_numeric(df['Offset'], errors='coerce').astype('float')\n",
    "\t\tdf['Largura_Emissão(kHz)'] = pd.to_numeric(\n",
    "\t\t\tdf['Largura_Emissão(kHz)'], errors='coerce'\n",
    "\t\t).astype('float')\n",
    "\t\tvalid = (\n",
    "\t\t\t(df.Offset.notna())\n",
    "\t\t\t& (~np.isclose(df.Offset, 0))\n",
    "\t\t\t& (df['Largura_Emissão(kHz)'].notna())\n",
    "\t\t\t& (~np.isclose(df['Largura_Emissão(kHz)'], 0))\n",
    "\t\t)\n",
    "\t\tdf.loc[:, ['Frequência', 'Offset']] = df.loc[:, ['Frequência', 'Offset']].astype('float')\n",
    "\t\tdf.loc[valid, 'Frequência_Recepção'] = df.loc[valid, 'Frequência'] - df.loc[valid, 'Offset']\n",
    "\t\treturn df\n",
    "\n",
    "\tdef substitute_coordenates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "\t\tibge = pd.read_csv(\n",
    "\t\t\tIBGE_MUNICIPIOS,\n",
    "\t\t\tdtype='string',\n",
    "\t\t\tusecols=['Código_Município', 'Município', 'Latitude', 'Longitude'],\n",
    "\t\t)\n",
    "\t\tibge.columns = ['Código_Município', 'Município', 'Latitude', 'Longitude']\n",
    "\t\tcoords = pd.merge(\n",
    "\t\t\tdf.loc[df.Multiplicidade > 1, 'Código_Município'],\n",
    "\t\t\tibge[['Código_Município', 'Latitude', 'Longitude']],\n",
    "\t\t\ton='Código_Município',\n",
    "\t\t\thow='left',\n",
    "\t\t)\n",
    "\t\tdf.loc[df.Multiplicidade > 1, ['Latitude', 'Longitude']] = coords[\n",
    "\t\t\t['Latitude', 'Longitude']\n",
    "\t\t].values\n",
    "\t\tlog = \"\"\"[(\"Colunas\", (\"Latitude\", \"Longitude\")), \n",
    "        (\"Processamento\", \"Substituição por Coordenadas do Município (Agrupamento)\")]\"\"\"\n",
    "\t\treturn self.register_log(df, log, df.Multiplicidade > 1)\n",
    "\n",
    "\tdef input_fixed_columns(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame de Origem\n",
    "\t) -> pd.DataFrame:  # DataFrame com os canais de downlink e uplink contenados e formatados\n",
    "\t\tdf['Status'] = 'L'\n",
    "\t\tdf['Num_Serviço'] = '010'\n",
    "\t\tdown = df.drop('Frequência_Recepção', axis=1)\n",
    "\t\tdown['Fonte'] = 'MOSAICO'\n",
    "\t\tdown['Classe'] = 'FB'\n",
    "\t\tup = df.drop('Frequência', axis=1)\n",
    "\t\tup = up.rename(columns={'Frequência_Recepção': 'Frequência'})\n",
    "\t\tup.dropna(subset='Frequência', inplace=True)\n",
    "\t\tup['Fonte'] = 'CANALIZACAO-SMP'\n",
    "\t\tup['Classe'] = 'ML'\n",
    "\t\treturn pd.concat([down, up], ignore_index=True)\n",
    "\n",
    "\tdef _format(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame com os dados de Estações e Plano_Básico mesclados\n",
    "\t) -> pd.DataFrame:  # DataFrame com os dados mesclados e limpos\n",
    "\t\t\"\"\"Clean the merged dataframe with the data from the MOSAICO page\"\"\"\n",
    "\t\tdf = df.rename(columns=self.cols_mapping)\n",
    "\t\tdf = self.split_designacao(df)\n",
    "\t\tdf = self.exclude_duplicated(df)\n",
    "\t\tdf = self.validate_channels(df)\n",
    "\t\tdf = self.generate_uplink(df)\n",
    "\t\tdf = self.substitute_coordenates(df)\n",
    "\t\tdf = self.input_fixed_columns(df)\n",
    "\t\treturn df.loc[:, self.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1457/2230402201.py\", line 9, in <module>\n",
      "    data.update()\n",
      "  File \"/mnt/c/Users/rsilva/OneDrive - ANATEL/Code/anateldb/extracao/datasources/base.py\", line 117, in update\n",
      "    self.df = self._format(self.extraction)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/functools.py\", line 967, in __get__\n",
      "    val = self.func(instance)\n",
      "  File \"/tmp/ipykernel_1457/2279196232.py\", line 38, in extraction\n",
      "    df = self._extract(self.collection, pipeline)\n",
      "  File \"/mnt/c/Users/rsilva/OneDrive - ANATEL/Code/anateldb/extracao/datasources/mosaico.py\", line 49, in _extract\n",
      "    result = collection.aggregate(pipeline)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/collection.py\", line 2721, in aggregate\n",
      "    with self.__database.client._tmp_session(session, close=False) as s:\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/contextlib.py\", line 113, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/mongo_client.py\", line 1885, in _tmp_session\n",
      "    s = self._ensure_session(session)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/mongo_client.py\", line 1868, in _ensure_session\n",
      "    return self.__start_session(True, causal_consistency=False)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/mongo_client.py\", line 1811, in __start_session\n",
      "    self._topology._check_implicit_session_support()\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/topology.py\", line 583, in _check_implicit_session_support\n",
      "    self._check_session_support()\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/topology.py\", line 599, in _check_session_support\n",
      "    self._select_servers_loop(\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/pymongo/topology.py\", line 280, in _select_servers_loop\n",
      "    self._condition.wait(common.MIN_HEARTBEAT_INTERVAL)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/threading.py\", line 306, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1030, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1098, in get_records\n",
      "    mod = inspect.getmodule(cf.tb_frame)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/inspect.py\", line 699, in getsourcefile\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "  File \"/home/melinda/micromamba/envs/dados/lib/python3.8/inspect.py\", line 699, in <genexpr>\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "AttributeError: 'PosixPath' object has no attribute 'endswith'\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "\timport time\n",
    "\n",
    "\tstart = time.perf_counter()\n",
    "\n",
    "\tdata = SMP()\n",
    "\n",
    "\tdata.update()\n",
    "\n",
    "\tprint('DATA')\n",
    "\n",
    "\tdisplay(data.df)\n",
    "\n",
    "\tprint(150 * '=')\n",
    "\n",
    "\tprint('DISCARDED!')\n",
    "\n",
    "\tdisplay(data.discarded[['Frequência', 'Entidade', 'Log']])\n",
    "\n",
    "\tprint(150 * '=')\n",
    "\n",
    "\tprint(data.df.Multiplicidade.sum())\n",
    "\n",
    "\tdata.save()\n",
    "\n",
    "\tprint(f'Elapsed time: {time.perf_counter() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
