{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp updates\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atualização\n",
    "\n",
    "> Este módulo atualiza as bases. Executa as queries sql do STEL, RADCOM e baixa os arquivos de estações e plano básico do MOSAICO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from decimal import Decimal, getcontext\n",
    "from typing import Union\n",
    "from urllib.request import urlretrieve, URLError\n",
    "import xmltodict\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "from rich.console import Console\n",
    "from pyarrow import ArrowInvalid, ArrowTypeError\n",
    "from unidecode import unidecode\n",
    "from fastcore.xtras import Path\n",
    "from fastcore.foundation import L\n",
    "from fastcore.test import test_eq\n",
    "import pyodbc\n",
    "\n",
    "\n",
    "from anateldb.constants import *\n",
    "from anateldb.format import parse_bw, format_types, input_coordenates\n",
    "\n",
    "getcontext().prec = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexão com o banco de dados\n",
    "A função a seguir é um `wrapper` simples que utiliza o `pyodbc` para se conectar ao banco de dados base da Anatel e retorna o objeto da conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def connect_db(server: str = 'ANATELBDRO05', # Servidor do Banco de Dados\n",
    "               database: str = 'SITARWEB', # Nome do Banco de Dados\n",
    "               trusted_conn: str = 'yes', # Conexão Segura: yes | no\n",
    "               mult_results: bool = True, # Múltiplos Resultados\n",
    "              )->pyodbc.Connection:\n",
    "    \"\"\"Conecta ao Banco `server` e retorna o 'cursor' (iterador) do Banco\"\"\"\n",
    "    return pyodbc.connect(\n",
    "        \"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "        f\"Server={server};\"\n",
    "        f\"Database={database};\"\n",
    "        f\"Trusted_Connection={trusted_conn};\"\n",
    "        f\"MultipleActiveResultSets={mult_results};\",\n",
    "        timeout=TIMEOUT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#echo: false\n",
    "def test_connection():\n",
    "    conn = connect_db()\n",
    "    cursor = conn.cursor()\n",
    "    for query in (RADCOM,STEL):\n",
    "        cursor.execute(query)\n",
    "        test_eq(type(cursor.fetchone()), pyodbc.Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _parse_estações(row: dict)->dict:\n",
    "    \"\"\"Given a row in a MongoDB ( a dict of dicts ), it travels some keys and return a subset dict\"\"\"\n",
    "    \n",
    "    d = {k.replace('@', '').lower():row[k] for k in (\"@SiglaServico\", \"@id\", \"@state\",\n",
    "        \"@entidade\",\n",
    "        \"@fistel\",\n",
    "        \"@cnpj\",\n",
    "        \"@municipio\",\n",
    "        \"@uf\")}\n",
    "    entidade = row.get('entidade', {})\n",
    "    d.update({k.replace('@', '').lower():entidade[k] for k in ('@num_servico', '@habilitacao_DataValFreq')})\n",
    "    administrativo = row.get('administrativo', {})\n",
    "    d['numero_estacao'] = administrativo.get('@numero_estacao')\n",
    "    estacao = row.get('estacao_principal', {})\n",
    "    d.update({k.replace('@', '').lower():estacao[k] for k in ('@latitude', '@longitude')})\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _read_estações(path: Union[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"Read the zipped xml file `Estações.zip` from MOSAICO and returns a dataframe\"\"\"\n",
    "    \n",
    "    with ZipFile(path) as myzip:\n",
    "        with myzip.open('estacao_rd.xml') as myfile:\n",
    "            estacoes = xmltodict.parse(myfile.read())\n",
    "            \n",
    "    assert 'estacao_rd' in estacoes, \"The xml file inside estacoes.zip is not in the expected format\"\n",
    "    assert 'row' in estacoes['estacao_rd'], \"The xml file inside estacoes.zip is not in the expected format\"\n",
    "    \n",
    "    df = pd.DataFrame(L(estacoes['estacao_rd']['row']).map(_parse_estações))\n",
    "    df = df[df.state.str.contains(\"-C1$|-C2$|-C3$|-C4$|-C7|-C98$\")].reset_index(drop=True)\n",
    "    df = df.loc[:, COL_ESTACOES]\n",
    "    df.columns = NEW_ESTACOES    \n",
    "    for c in df.columns:\n",
    "        df.loc[df[c] == \"\", c] = pd.NA\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _parse_pb(row: dict)->dict:\n",
    "    \"\"\"Given a row in the MongoDB file canais.zip ( a dict of dicts ), it travels some keys and return a subset dict\"\"\"\n",
    "    return {unidecode(k).lower().replace(\"@\", \"\"): v  for k,v in row.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _read_plano_basico(path: Union[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"Read the zipped xml file `Plano_Básico.zip` from MOSAICO and returns a dataframe\"\"\"    \n",
    "    df = L()\n",
    "    with ZipFile(path) as myzip:\n",
    "        with myzip.open('plano_basicoTVFM.xml') as myfile:\n",
    "            pbtvfm = xmltodict.parse(myfile.read())\n",
    "        with myzip.open('plano_basicoAM.xml') as myfile:\n",
    "            pbam = xmltodict.parse(myfile.read())\n",
    "        with myzip.open('secundariosTVFM.xml') as myfile:\n",
    "            stvfm = xmltodict.parse(myfile.read())\n",
    "        with myzip.open('secundariosAM.xml') as myfile:\n",
    "            sam = xmltodict.parse(myfile.read())    \n",
    "            \n",
    "    for base in (pbtvfm, stvfm, pbam, sam):\n",
    "        assert 'plano_basico' in base, \"The xml files inside canais.zip is not in the expected format\"\n",
    "        assert 'row' in base['plano_basico'], \"The xml file inside canais.zip is not in the expected format\"\n",
    "        df.extend(L(base['plano_basico']['row']).map(_parse_pb))\n",
    "        \n",
    "    df = pd.DataFrame(df)\n",
    "    df = df.loc[df.pais == \"BRA\", COL_PB].reset_index(drop=True)    \n",
    "    df.columns = NEW_PB\n",
    "    df = df[df.Status.str.contains(\"-C1$|-C2$|-C3$|-C4$|-C7|-C98$\")].reset_index(drop=True)\n",
    "    df.loc[:, 'Frequência'] = df.Frequência.str.replace(',', '.')\n",
    "    for c in df.columns:\n",
    "        df.loc[df[c] == '', c] = pd.NA\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clean_mosaico(df: pd.DataFrame, # DataFrame com os dados de Estações e Plano_Básico mesclados \n",
    "                pasta: Union[str, Path], # Pasta com os dados de municípios para imputar coordenadas ausentes\n",
    ") -> pd.DataFrame: # DataFrame com os dados mesclados e limpos\n",
    "    \"\"\"Clean the merged dataframe with the data from the MOSAICO page\"\"\"\n",
    "    COLS = [c for c in df.columns if \"_x\" in c]\n",
    "    for col in COLS:\n",
    "        col_x = col\n",
    "        col_y = col.split(\"_\")[0] + \"_y\"\n",
    "        df.loc[df[col_x].isna(), col_x] = df.loc[df[col_x].isna(), col_y]\n",
    "        df.loc[df[col_y].isna(), col_y] = df.loc[df[col_y].isna(), col_x]\n",
    "        if df[col_x].notna().sum() > df[col_y].notna().sum():\n",
    "            a, b = col_x, col_y\n",
    "        else:\n",
    "            a, b = col_y, col_x\n",
    "        df.drop(b, axis=1, inplace=True)\n",
    "        df.rename({a: a[:-2]}, axis=1, inplace=True)\n",
    "\n",
    "    df.loc[df.Latitude_Transmissor == \"\", \"Latitude_Transmissor\"] = df.loc[\n",
    "        df.Latitude_Transmissor == \"\", \"Latitude_Estação\"\n",
    "    ]\n",
    "    df.loc[df.Longitude_Transmissor == \"\", \"Longitude_Transmissor\"] = df.loc[\n",
    "        df.Longitude_Transmissor == \"\", \"Longitude_Estação\"\n",
    "    ]\n",
    "    df.loc[df.Latitude_Transmissor.isna(), \"Latitude_Transmissor\"] = df.loc[\n",
    "        df.Latitude_Transmissor.isna(), \"Latitude_Estação\"\n",
    "    ]\n",
    "    df.loc[df.Longitude_Transmissor.isna(), \"Longitude_Transmissor\"] = df.loc[\n",
    "        df.Longitude_Transmissor.isna(), \"Longitude_Estação\"\n",
    "    ]\n",
    "    df.drop([\"Latitude_Estação\", \"Longitude_Estação\"], axis=1, inplace=True)\n",
    "    df.rename(\n",
    "        columns={\n",
    "            \"Latitude_Transmissor\": \"Latitude\",\n",
    "            \"Longitude_Transmissor\": \"Longitude\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    df = input_coordenates(df, pasta)\n",
    "    df.loc[:, \"Frequência\"] = df.Frequência.str.replace(\",\", \".\")\n",
    "    if freq_nans := df[df.Frequência.isna()].Id.tolist():\n",
    "        complement_df = scrape_dataframe(freq_nans)\n",
    "        df.loc[\n",
    "            df.Frequência.isna(),\n",
    "            [\n",
    "                \"Status\",\n",
    "                \"Entidade\",\n",
    "                \"Fistel\",\n",
    "                \"Frequência\",\n",
    "                \"Classe\",\n",
    "                \"Num_Serviço\",\n",
    "                \"Município\",\n",
    "                \"UF\",\n",
    "            ],\n",
    "        ] = complement_df.values\n",
    "\n",
    "    df.loc[:, \"Frequência\"] = df.Frequência.astype(\"float\")\n",
    "    df.loc[df.Serviço == \"OM\", \"Frequência\"] = df.loc[\n",
    "        df.Serviço == \"OM\", \"Frequência\"\n",
    "    ].apply(lambda x: Decimal(x) / Decimal(1000))\n",
    "    df.loc[:, \"Validade_RF\"] = df.Validade_RF.astype(\"string\").str.slice(0, 10)\n",
    "    return df.drop_duplicates(keep=\"first\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atualização das bases de dados\n",
    "As bases de dados são atualizadas atráves das funções a seguir, o único argumento passado em todas elas é a pasta na qual os arquivos locais processados serão salvos, os nomes dos arquivos são padronizados e não podem ser editados para que as funções de leitura e processamento recebam somente a pasta na qual esses arquivos foram salvos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _save_df(df: pd.DataFrame, folder: Union[str, Path], stem: str) -> pd.DataFrame:\n",
    "    \"\"\"Format, Save and return a dataframe\"\"\"\n",
    "    df = format_types(df, stem)\n",
    "    df = df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    df = df.dropna(subset=['Latitude', 'Longitude']).reset_index(drop=True)\n",
    "    try:\n",
    "        file = Path(f\"{folder}/{stem}.parquet.gzip\")\n",
    "        df.to_parquet(file, compression=\"gzip\", index=False)\n",
    "    except (ArrowInvalid, ArrowTypeError):\n",
    "        file.unlink(missing_ok=True)\n",
    "        try:\n",
    "            file = Path(f\"{folder}/{stem}.fth\")\n",
    "            df.to_feather(file)\n",
    "        except (ArrowInvalid, ArrowTypeError):\n",
    "            file.unlink(missing_ok=True)\n",
    "            try:\n",
    "                file = Path(f\"{folder}/{stem}.xlsx\")\n",
    "                with pd.ExcelWriter(file) as wb:\n",
    "                    df.to_excel(\n",
    "                        wb, sheet_name=\"DataBase\", engine=\"openpyxl\", index=False\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Could not save {stem} to {file}\") from e\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def update_radcom(\n",
    "        conn: pyodbc.Connection, # Objeto de conexão de banco\n",
    "        folder: Union[str, Path] # Pasta onde salvar os arquivos\n",
    "        \n",
    ") -> pd.DataFrame: # DataFrame com os dados atualizados\n",
    "    \"\"\"Atualiza a tabela local retornada pela query `RADCOM`\"\"\"\n",
    "    console = Console()\n",
    "    with console.status(\n",
    "        \"[cyan]Lendo o Banco de Dados de Radcom...\", spinner=\"earth\"\n",
    "    ) as status:\n",
    "        try:            \n",
    "            df = pd.read_sql_query(RADCOM, conn)\n",
    "            return _save_df(df, folder, \"radcom\")\n",
    "        except pyodbc.OperationalError as e:\n",
    "            status.console.log(\n",
    "                \"Não foi possível abrir uma conexão com o SQL Server. Esta conexão somente funciona da rede cabeada!\"\n",
    "            )\n",
    "            raise ConnectionError from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def update_stel(\n",
    "        conn: pyodbc.Connection, # Objeto de conexão de banco\n",
    "        folder:Union[str, Path] # Pasta onde salvar os arquivos        \n",
    ") -> pd.DataFrame: # DataFrame com os dados atualizados\n",
    "    \"\"\"Atualiza a tabela local retornada pela query `STEL`\"\"\"\n",
    "    console = Console()\n",
    "    with console.status(\n",
    "        \"[red]Lendo o Banco de Dados do STEL. Processo Lento, aguarde...\",\n",
    "        spinner=\"bouncingBall\",\n",
    "    ) as status:\n",
    "        try:            \n",
    "            df = pd.read_sql_query(STEL, conn)\n",
    "            return _save_df(df, folder, \"stel\")\n",
    "        except pyodbc.OperationalError as e:\n",
    "            status.console.log(\n",
    "                \"Não foi possível abrir uma conexão com o SQL Server. Esta conexão somente funciona da rede cabeada!\"\n",
    "            )\n",
    "            raise ConnectionError from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">(  ●   )</span> <span style=\"color: #800000; text-decoration-color: #800000\">Lendo o Banco de Dados do STEL. Processo Lento, aguarde...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m(  ●   )\u001b[0m \u001b[31mLendo o Banco de Dados do STEL. Processo Lento, aguarde...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stel = update_stel(Path.cwd().parent / 'dados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stel.to_parquet(Path.cwd().parent / 'dados' / 'stel.parquet.gzip', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def update_mosaico(folder: Union[str, Path], # Pasta onde salvar os arquivos\n",
    ") -> pd.DataFrame: # DataFrame com os dados atualizados\n",
    "    \"\"\"Atualiza a tabela local do Mosaico. É baixado e processado arquivos xml zipados da página pública do Spectrum E\"\"\"\n",
    "    console = Console()\n",
    "    with console.status(\n",
    "        \"[blue]Baixando e consolidando os dados do Mosaico...\", spinner=\"clock\"\n",
    "    ) as status:  \n",
    "        try:\n",
    "            stations, _ = urlretrieve(ESTACOES, f\"{folder}/estações.zip\")\n",
    "            pb, _ = urlretrieve(PLANO_BASICO, f\"{folder}/canais.zip\")\n",
    "            estações = _read_estações(stations)\n",
    "            plano_basico = _read_plano_basico(pb)\n",
    "            df = estações.merge(plano_basico, on=\"Id\", how=\"left\")\n",
    "            df = clean_mosaico(df, folder)\n",
    "            return _save_df(df, folder, \"mosaico\")\n",
    "        except URLError as e:\n",
    "            status.console.log(\"[red]Não foi possível baixar os dados do Mosaico\")\n",
    "            raise ConnectionError from e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def update_base(\n",
    "    conn: pyodbc.Connection, # Objeto de conexão de banco\n",
    "    folder: Union[str, Path] # Pasta onde salvar os arquivos    \n",
    ") -> pd.DataFrame: # DataFrame com os dados atualizados\n",
    "    # sourcery skip: use-fstring-for-concatenation\n",
    "    \"\"\"Wrapper que atualiza opcionalmente lê e atualiza as três bases indicadas anteriormente, as combina e salva o arquivo consolidado na folder `folder`\"\"\"\n",
    "    try:\n",
    "        stel = update_stel(conn, folder, ).loc[:, TELECOM]\n",
    "    except ConnectionError:\n",
    "        from anateldb.reading import read_stel\n",
    "        stel = read_stel(folder).loc[:, TELECOM]\n",
    "    try:\n",
    "        radcom = update_radcom(conn, folder).loc[:, SRD]\n",
    "    except ConnectionError:\n",
    "        from anateldb.reading import read_radcom\n",
    "        radcom = read_radcom(folder).loc[:, SRD]\n",
    "    try:\n",
    "        mosaico = update_mosaico(folder).loc[:, RADIODIFUSAO]\n",
    "    except ConnectionError:\n",
    "        from anateldb.reading import read_mosaico\n",
    "        mosaico = read_mosaico(folder).loc[:, RADIODIFUSAO]\n",
    "    radcom[\"Num_Serviço\"] = \"231\"\n",
    "    radcom[\"Status\"] = \"RADCOM\"\n",
    "    radcom[\"Classe_Emissão\"] = pd.NA\n",
    "    radcom[\"Largura_Emissão\"] = BW_MAP[\"231\"]\n",
    "    radcom[\"Entidade\"] = radcom.Entidade.str.rstrip().str.lstrip()\n",
    "    radcom[\"Validade_RF\"] = pd.NA\n",
    "    radcom[\"Fonte\"] = \"SRD\"\n",
    "    stel[\"Status\"] = \"L\"\n",
    "    stel[\"Entidade\"] = stel.Entidade.str.rstrip().str.lstrip()\n",
    "    stel[\"Fonte\"] = \"STEL\"\n",
    "    mosaico[\"Fonte\"] = \"MOS\"\n",
    "    mosaico[\"Classe_Emissão\"] = pd.NA\n",
    "    mosaico[\"Largura_Emissão\"] = mosaico.Num_Serviço.map(BW_MAP)\n",
    "    rd = (\n",
    "        pd.concat([mosaico, radcom, stel])\n",
    "        .sort_values([\"Frequência\", \"Latitude\", \"Longitude\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    rd = rd.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "    rd[\"BW(kHz)\"] = rd.Largura_Emissão.astype('string').fillna('-1').apply(parse_bw)\n",
    "    return _save_df(rd, folder, \"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "folder = Path.cwd().parent / 'dados'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import Request, urlopen\n",
    "# from urllib.error import URLError\n",
    "# req = Request(ESTACAO)\n",
    "# try:\n",
    "#     response = urlopen(req)\n",
    "# except URLError as e:\n",
    "#     if hasattr(e, 'reason'):\n",
    "#         print('We failed to reach a server.')\n",
    "#         print('Reason: ', e.reason)\n",
    "#     elif hasattr(e, 'code'):\n",
    "#         print('The server couldn\\'t fulfill the request.')\n",
    "#         print('Error code: ', e.code)\n",
    "# else:\n",
    "#     Path.cwd().joinpath('estações.zip').write_bytes(response.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('dev-sentinela-py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "d640be867678a5ad7400949fa86ceae4d3eb6c7031441864f708f81404f1a3e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
