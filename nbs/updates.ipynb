{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp updates\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atualização\n",
    "\n",
    "> Este módulo atualiza as bases. Executa as queries sql do STEL, RADCOM e baixa os arquivos de estações e plano básico do MOSAICO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from decimal import Decimal, getcontext\n",
    "from typing import Union\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "from rich.console import Console\n",
    "from pyarrow import ArrowInvalid, ArrowTypeError\n",
    "from fastcore.xtras import Path\n",
    "from fastcore.test import test_eq\n",
    "from fastcore.foundation import L\n",
    "from tqdm.auto import tqdm\n",
    "import pyodbc\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from extracao.constants import *\n",
    "from extracao.format import parse_bw, format_types, input_coordenates\n",
    "\n",
    "getcontext().prec = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexão com o banco de dados\n",
    "A função a seguir é um `wrapper` simples que utiliza o `pyodbc` para se conectar ao banco de dados base da Anatel e retorna o objeto da conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def connect_db(server: str = 'ANATELBDRO05', # Servidor do Banco de Dados\n",
    "               database: str = 'SITARWEB', # Nome do Banco de Dados\n",
    "               trusted_conn: str = 'yes', # Conexão Segura: yes | no\n",
    "               mult_results: bool = True, # Múltiplos Resultados\n",
    "              )->pyodbc.Connection:\n",
    "    \"\"\"Conecta ao Banco `server` e retorna o 'cursor' (iterador) do Banco\"\"\"\n",
    "    return pyodbc.connect(\n",
    "        \"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "        f\"Server={server};\"\n",
    "        f\"Database={database};\"\n",
    "        f\"Trusted_Connection={trusted_conn};\"\n",
    "        f\"MultipleActiveResultSets={mult_results};\",\n",
    "        timeout=TIMEOUT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#echo: false\n",
    "def test_connection():\n",
    "    conn = connect_db()\n",
    "    cursor = conn.cursor()\n",
    "    for query in (RADCOM,STEL):\n",
    "        cursor.execute(query)\n",
    "        test_eq(type(cursor.fetchone()), pyodbc.Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clean_mosaico(df: pd.DataFrame, # DataFrame com os dados de Estações e Plano_Básico mesclados \n",
    "                pasta: Union[str, Path], # Pasta com os dados de municípios para imputar coordenadas ausentes\n",
    ") -> pd.DataFrame: # DataFrame com os dados mesclados e limpos\n",
    "    \"\"\"Clean the merged dataframe with the data from the MOSAICO page\"\"\"\n",
    "    COLS = [c for c in df.columns if \"_x\" in c]\n",
    "    for col in COLS:\n",
    "        col_x = col\n",
    "        col_y = col.split(\"_\")[0] + \"_y\"\n",
    "        df.loc[df[col_x].isna(), col_x] = df.loc[df[col_x].isna(), col_y]\n",
    "        df.loc[df[col_y].isna(), col_y] = df.loc[df[col_y].isna(), col_x]\n",
    "        if df[col_x].notna().sum() > df[col_y].notna().sum():\n",
    "            a, b = col_x, col_y\n",
    "        else:\n",
    "            a, b = col_y, col_x\n",
    "        df.drop(b, axis=1, inplace=True)\n",
    "        df.rename({a: a[:-2]}, axis=1, inplace=True)\n",
    "\n",
    "    df = input_coordenates(df, pasta)\n",
    "    df.loc[\"Frequência\"] = df.Frequência.str.replace(\",\", \".\")    \n",
    "    df = df[df.Frequência.notna()].reset_index(drop=True)\n",
    "    df.loc[\"Frequência\"] = df.Frequência.astype(\"float\")\n",
    "    df.loc[df.Serviço == \"OM\", \"Frequência\"] = df.loc[\n",
    "        df.Serviço == \"OM\", \"Frequência\"\n",
    "    ].apply(lambda x: Decimal(x) / Decimal(1000))\n",
    "    df.loc[:, \"Validade_RF\"] = df.Validade_RF.astype(\"string\").str.slice(0, 10)\n",
    "    return df.drop_duplicates(keep=\"first\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atualização das bases de dados\n",
    "As bases de dados são atualizadas atráves das funções a seguir, o único argumento passado em todas elas é a pasta na qual os arquivos locais processados serão salvos, os nomes dos arquivos são padronizados e não podem ser editados para que as funções de leitura e processamento recebam somente a pasta na qual esses arquivos foram salvos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _save_df(df: pd.DataFrame, folder: Union[str, Path], stem: str) -> pd.DataFrame:\n",
    "    \"\"\"Format, Save and return a dataframe\"\"\"\n",
    "    df = format_types(df, stem)\n",
    "    df = df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    df = df.dropna(subset=['Latitude', 'Longitude']).reset_index(drop=True)\n",
    "    try:\n",
    "        file = Path(f\"{folder}/{stem}.parquet.gzip\")\n",
    "        df.to_parquet(file, compression=\"gzip\", index=False)\n",
    "    except (ArrowInvalid, ArrowTypeError):\n",
    "        file.unlink(missing_ok=True)\n",
    "        try:\n",
    "            file = Path(f\"{folder}/{stem}.fth\")\n",
    "            df.to_feather(file)\n",
    "        except (ArrowInvalid, ArrowTypeError):\n",
    "            file.unlink(missing_ok=True)\n",
    "            try:\n",
    "                file = Path(f\"{folder}/{stem}.xlsx\")\n",
    "                with pd.ExcelWriter(file) as wb:\n",
    "                    df.to_excel(\n",
    "                        wb, sheet_name=\"DataBase\", engine=\"openpyxl\", index=False\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Could not save {stem} to {file}\") from e\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def update_radcom(\n",
    "        conn: pyodbc.Connection, # Objeto de conexão de banco\n",
    "        folder: Union[str, Path] # Pasta onde salvar os arquivos\n",
    "        \n",
    ") -> pd.DataFrame: # DataFrame com os dados atualizados\n",
    "    \"\"\"Atualiza a tabela local retornada pela query `RADCOM`\"\"\"\n",
    "    console = Console()\n",
    "    with console.status(\n",
    "        \"[cyan]Lendo o Banco de Dados de Radcom...\", spinner=\"earth\"\n",
    "    ) as status:\n",
    "        try:            \n",
    "            df = pd.read_sql_query(RADCOM, conn)\n",
    "            return _save_df(df, folder, \"radcom\")\n",
    "        except pyodbc.OperationalError as e:\n",
    "            status.console.log(\n",
    "                \"Não foi possível abrir uma conexão com o SQL Server. Esta conexão somente funciona da rede cabeada!\"\n",
    "            )\n",
    "            raise ConnectionError from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def update_stel(\n",
    "        conn: pyodbc.Connection, # Objeto de conexão de banco\n",
    "        folder:Union[str, Path] # Pasta onde salvar os arquivos        \n",
    ") -> pd.DataFrame: # DataFrame com os dados atualizados\n",
    "    \"\"\"Atualiza a tabela local retornada pela query `STEL`\"\"\"\n",
    "    console = Console()\n",
    "    with console.status(\n",
    "        \"[red]Lendo o Banco de Dados do STEL. Processo Lento, aguarde...\",\n",
    "        spinner=\"bouncingBall\",\n",
    "    ) as status:\n",
    "        try:            \n",
    "            df = pd.read_sql_query(STEL, conn)\n",
    "            return _save_df(df, folder, \"stel\")\n",
    "        except pyodbc.OperationalError as e:\n",
    "            status.console.log(\n",
    "                \"Não foi possível abrir uma conexão com o SQL Server. Esta conexão somente funciona da rede cabeada!\"\n",
    "            )\n",
    "            raise ConnectionError from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def update_mosaico(        \n",
    "        mongo_client: MongoClient, # Objeto de conexão com o MongoDB\n",
    "        folder: Union[str, Path] # Pasta onde salvar os arquivos\n",
    ") -> pd.DataFrame: # DataFrame com os dados atualizados\n",
    "    \"\"\"Efetua a query na tabela de Radiodifusão no banco mongoDB `mongo_client` e atualiza o arquivo local\"\"\"\n",
    "    console = Console()\n",
    "    with console.status(\n",
    "        \"Consolidando os dados do Mosaico...\", spinner=\"clock\"\n",
    "    ) as status:  \n",
    "        \n",
    "        database = mongo_client[\"sms\"]\n",
    "        # Database com as informações de Radio e difusão\n",
    "        collection = database[\"srd\"]\n",
    "\n",
    "        query = {}\n",
    "        projection = {\"SiglaServico\": 1.0, \"Status.state\": 1.0, \"licensee\": 1.0, \"NumFistel\": 1.0, \"frequency\": 1.0, \"stnClass\": 1.0, \"srd_planobasico.NomeMunicipio\": 1.0, \"srd_planobasico.SiglaUF\": 1.0, \"NumServico\": 1.0, \"estacao.NumEstacao\": 1.0, \"estacao.MedLatitudeDecimal\": 1.0, \"estacao.MedLongitudeDecimal\": 1.0, \"habilitacao.DataValFreq\": 1.0}\n",
    "\n",
    "        list_data = list(collection.find(query, projection = projection))\n",
    "        mosaico_df = pd.json_normalize(list_data)\n",
    "        mosaico_df = mosaico_df.drop(columns=['estacao'])\n",
    "        mosaico_df = mosaico_df[[\"frequency\",\n",
    "                                \"licensee\",\n",
    "                                \"NumFistel\",\n",
    "                                \"estacao.NumEstacao\",\n",
    "                                \"srd_planobasico.NomeMunicipio\",\n",
    "                                \"srd_planobasico.SiglaUF\",\n",
    "                                \"estacao.MedLatitudeDecimal\",\n",
    "                                \"estacao.MedLongitudeDecimal\",\n",
    "                                \"stnClass\",\n",
    "                                \"NumServico\",\n",
    "                                \"habilitacao.DataValFreq\",\n",
    "                                \"Status.state\"]]\n",
    "\n",
    "        mosaico_df.columns = RADIODIFUSAO\n",
    "        mosaico_df = mosaico_df[mosaico_df.Status.str.contains(\"-C1$|-C2$|-C3$|-C4$|-C7|-C98$\", na=False)].reset_index(drop=True)\n",
    "        for c in mosaico_df.columns:\n",
    "            mosaico_df.loc[mosaico_df[c] == \"\", c] = pd.NA\n",
    "        mosaico_df = mosaico_df.dropna(subset=['UF'])\n",
    "        mosaico_df = mosaico_df[mosaico_df.Frequência.notna()].reset_index(drop=True)\n",
    "\n",
    "        df = clean_mosaico(mosaico_df, folder)\n",
    "    return _save_df(df, folder, \"mosaico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def update_licenciamento(mongo_client: MongoClient, # Objeto de conexão com o MongoDB\n",
    "                         folder: Union[str, Path] # Pasta onde salvar os arquivos\n",
    ")-> pd.DataFrame: # DataFrame com os dados atualizados\n",
    "    \"\"\"Efetua a query na tabela `licenciamento` no banco mongoDB `mongo_client` e atualiza o arquivo local\"\"\"\n",
    "\n",
    "\n",
    "    console = Console()\n",
    "    with console.status(\n",
    "        \"Consolidando os dados do Licenciamento...\", spinner=\"clock\"\n",
    "    ) as status:\n",
    "\n",
    "        database = mongo_client[\"sms\"]\n",
    "        collection = database[\"licenciamento\"]    \n",
    "        c = collection.find(MONGO_LIC, projection={k:1.0 for k in LICENCIAMENTO.keys()})\n",
    "        result = L()\n",
    "        for doc in tqdm(c):\n",
    "            result.append(doc)\n",
    "        df = pd.json_normalize(result)\n",
    "        df.drop('_id', axis=1, inplace=True)\n",
    "        df.rename(LICENCIAMENTO, axis=1, inplace=True)\n",
    "        df['Designacao_Emissão'] = df.Designacao_Emissão.str.replace(',', ' ')\n",
    "        df['Designacao_Emissão'] = df.Designacao_Emissão.str.strip().str.lstrip().str.rstrip().str.upper()\n",
    "        df['Designacao_Emissão'] = df.Designacao_Emissão.str.split(' ')\n",
    "        df = df.explode('Designacao_Emissão')\n",
    "        df.loc[df.Designacao_Emissão == '/', 'Designacao_Emissão'] = ''\n",
    "        df.loc[:, ['Largura_Emissão', 'Classe_Emissão']]  = df.Designacao_Emissão.apply(parse_bw).tolist()\n",
    "        df.drop('Designacao_Emissão', axis=1, inplace=True)\n",
    "        subset = ['Entidade', 'Longitude', 'Latitude', 'Classe', 'Frequência', 'Num_Serviço', 'Largura_Emissão', 'Classe_Emissão']\n",
    "        df_sub = df[~df.duplicated(subset=subset, keep='first')].reset_index(drop=True).copy()\n",
    "        df_sub = df_sub.set_index(subset).sort_index()\n",
    "        df_sub['Count'] = (df.groupby(subset).count()['Número_Estação']).tolist()\n",
    "        del df ; gc.collect()\n",
    "        df_sub = df_sub.reset_index()\n",
    "    return _save_df(df_sub, folder, 'licenciamento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': '4.0.5',\n",
       " 'gitVersion': '3739429dd92b92d1b0ab120911a23d50bf03c412',\n",
       " 'targetMinOS': 'Windows 7/Windows Server 2008 R2',\n",
       " 'modules': [],\n",
       " 'allocator': 'tcmalloc',\n",
       " 'javascriptEngine': 'mozjs',\n",
       " 'sysInfo': 'deprecated',\n",
       " 'versionArray': [4, 0, 5, 0],\n",
       " 'openssl': {'running': 'Windows SChannel'},\n",
       " 'buildEnvironment': {'distmod': '2008plus-ssl',\n",
       "  'distarch': 'x86_64',\n",
       "  'cc': 'cl: Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24223 for x64',\n",
       "  'ccflags': '/nologo /EHsc /W3 /wd4355 /wd4800 /wd4267 /wd4244 /wd4290 /wd4068 /wd4351 /wd4373 /we4013 /we4099 /we4930 /WX /errorReport:none /MD /O2 /Oy- /bigobj /utf-8 /Zc:rvalueCast /Zc:strictStrings /volatile:iso /Gw /Gy /Zc:inline',\n",
       "  'cxx': 'cl: Microsoft (R) C/C++ Optimizing Compiler Version 19.00.24223 for x64',\n",
       "  'cxxflags': '/TP',\n",
       "  'linkflags': '/nologo /DEBUG /INCREMENTAL:NO /LARGEADDRESSAWARE /OPT:REF',\n",
       "  'target_arch': 'x86_64',\n",
       "  'target_os': 'windows'},\n",
       " 'bits': 64,\n",
       " 'debug': False,\n",
       " 'maxBsonObjectSize': 16777216,\n",
       " 'storageEngines': ['devnull', 'ephemeralForTest', 'mmapv1', 'wiredTiger'],\n",
       " 'ok': 1.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "uri = input()\n",
    "mongo_client = MongoClient(uri)\n",
    "mongo_client.server_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message='install \"ipywidgets\" for Jupyter support')\n",
    "\n",
    "folder = Path.cwd().parent / 'dados'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b71ec28f884fa2994f805d89055222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = update_licenciamento(mongo_client, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def update_base(\n",
    "    conn: pyodbc.Connection, # Objeto de conexão de banco\n",
    "    clientMongoDB: MongoClient, # Ojeto de conexão com o MongoDB\n",
    "    folder: Union[str, Path] # Pasta onde salvar os arquivos    \n",
    ") -> pd.DataFrame: # DataFrame com os dados atualizados\n",
    "    # sourcery skip: use-fstring-for-concatenation\n",
    "    \"\"\"Wrapper que atualiza opcionalmente lê e atualiza as 4 bases indicadas anteriormente, as combina e salva o arquivo consolidado na folder `folder`\"\"\"\n",
    "    stel = update_stel(conn, folder, ).loc[:, TELECOM]\n",
    "    radcom = update_radcom(conn, folder).loc[:, SRD]\n",
    "    mosaico = update_mosaico(clientMongoDB, folder).loc[:, RADIODIFUSAO]    \n",
    "    licenciamento = update_licenciamento(clientMongoDB, folder) # .loc[:, LICENCIAMENTO]\n",
    "    # Filtrando RADCOM\n",
    "    radcom[\"Num_Serviço\"] = \"231\"\n",
    "    radcom[\"Status\"] = \"RADCOM\"\n",
    "    radcom[\"Classe_Emissão\"] = pd.NA\n",
    "    radcom[\"Largura_Emissão\"] = BW_MAP[\"231\"]\n",
    "    radcom[\"Entidade\"] = radcom.Entidade.str.rstrip().str.lstrip()\n",
    "    radcom[\"Validade_RF\"] = pd.NA\n",
    "    radcom[\"Fonte\"] = \"SRD\"\n",
    "    # Filtrando STEL\n",
    "    stel[\"Status\"] = \"L\"\n",
    "    stel[\"Entidade\"] = stel.Entidade.str.rstrip().str.lstrip()\n",
    "    stel[\"Fonte\"] = \"STEL\"\n",
    "    # Filtrando MOSAICO\n",
    "    mosaico[\"Fonte\"] = \"MOS\"\n",
    "    mosaico[\"Classe_Emissão\"] = pd.NA\n",
    "    mosaico[\"Largura_Emissão\"] = mosaico.Num_Serviço.map(BW_MAP)\n",
    "    # Filtrando LICENCIAMENTO\n",
    "    licenciamento.drop('Count', axis=1, inplace=True)\n",
    "    licenciamento[\"Fonte\"] = 'LIC'\n",
    "\n",
    "    rd = (\n",
    "        pd.concat([mosaico, radcom, stel, licenciamento])\n",
    "        .sort_values([\"Frequência\", \"Latitude\", \"Longitude\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    rd = rd.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "    rd[\"BW(kHz)\"] = rd.Largura_Emissão.astype('string').fillna('-1').apply(parse_bw)\n",
    "    return _save_df(rd, folder, \"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import Request, urlopen\n",
    "# from urllib.error import URLError\n",
    "# req = Request(ESTACAO)\n",
    "# try:\n",
    "#     response = urlopen(req)\n",
    "# except URLError as e:\n",
    "#     if hasattr(e, 'reason'):\n",
    "#         print('We failed to reach a server.')\n",
    "#         print('Reason: ', e.reason)\n",
    "#     elif hasattr(e, 'code'):\n",
    "#         print('The server couldn\\'t fulfill the request.')\n",
    "#         print('Error code: ', e.code)\n",
    "# else:\n",
    "#     Path.cwd().joinpath('estações.zip').write_bytes(response.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('anateldb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "37a0fa21125bc68b41a424f43d92a26fa97b5ebccfac6eecdcaf85c09668024f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
