{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp format\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "# os.chdir(Path.cwd().parent / 'extracao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | export\n",
    "import re\n",
    "from typing import Iterable, Tuple, Union, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.utils import listify\n",
    "from fastcore.xtras import Path\n",
    "from geopy.distance import geodesic\n",
    "from rich.progress import Progress\n",
    "from pyarrow import ArrowInvalid\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "from extracao.constants import BW, BW_pattern, APP_ANALISE_PT, APP_ANALISE_EN\n",
    "\n",
    "RE_BW = re.compile(BW_pattern)\n",
    "MAX_DIST = 10  # Km\n",
    "LIMIT_FREQ = 84812.50\n",
    "load_dotenv(find_dotenv(), override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _read_df(folder: Union[str, Path], stem: str) -> pd.DataFrame:\n",
    "    \"\"\"Lê o dataframe formado por folder / stem.[parquet.gzip | fth | xslx]\"\"\"\n",
    "    file = Path(f\"{folder}/{stem}.parquet.gzip\")\n",
    "    try:\n",
    "        df = pd.read_parquet(file)\n",
    "    except (ArrowInvalid, FileNotFoundError) as e:\n",
    "        raise e(f\"Error when reading {file}\") from e\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatação\n",
    "\n",
    "> Este módulo possui funções auxiliares de formatação dos dados das várias fontes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def parse_bw(\n",
    "    bw: str,  # Designação de Emissão (Largura + Classe) codificada como string\n",
    ") -> Tuple[str, str]:  # Largura e Classe de Emissão\n",
    "    \"\"\"Parse the bandwidth string\"\"\"\n",
    "    if match := re.match(RE_BW, bw):\n",
    "        multiplier = BW[match[2]]\n",
    "        if mantissa := match[3]:\n",
    "            number = float(f\"{match[1]}.{mantissa}\")\n",
    "        else:\n",
    "            number = float(match[1])\n",
    "        classe = match[4]\n",
    "        return str(multiplier * number), str(classe)\n",
    "    return \"-1\", \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _filter_matlab(\n",
    "    df: pd.DataFrame,  # Arquivo de Dados Base de Entrada\n",
    ") -> pd.DataFrame:  # Arquivo de Dados formatado para leitura no Matlab\n",
    "    \"\"\"Recebe a base de dados da Anatel e formata as colunas para leitura de acordo com os requisitos do Matlab\"\"\"\n",
    "    df[\"#Estação\"] = df[\"Número_Estação\"]\n",
    "    df.loc[df.Multiplicidade != \"1\", \"#Estação\"] = (\n",
    "        df.loc[df.Multiplicidade != \"1\", \"Número_Estação\"]\n",
    "        + \"+\"\n",
    "        + df.loc[df.Multiplicidade != \"1\", \"Multiplicidade\"]\n",
    "    )\n",
    "    cols_desc = [\n",
    "        \"Fonte\",\n",
    "        \"Status\",\n",
    "        \"Classe\",\n",
    "        \"Entidade\",\n",
    "        \"Fistel\",\n",
    "        \"#Estação\",\n",
    "        \"Município_IBGE\",\n",
    "        \"UF\",\n",
    "    ]\n",
    "    df.loc[:, cols_desc].fillna(\"NI\", inplace=True)\n",
    "\n",
    "    df[\"Descrição\"] = (\n",
    "        \"[\"\n",
    "        + df.Fonte\n",
    "        + \"] \"\n",
    "        + df.Status\n",
    "        + \", \"\n",
    "        + df.Classe\n",
    "        + \", \"\n",
    "        + df.Entidade.str.title()\n",
    "        + \" (\"\n",
    "        + df.Fistel\n",
    "        + \", \"\n",
    "        + df[\"#Estação\"]\n",
    "        + \"), \"\n",
    "        + df.Município_IBGE\n",
    "        + \"/\"\n",
    "        + df.UF\n",
    "    )\n",
    "\n",
    "    bad_coords = df.Coords_Valida_IBGE == \"0\"\n",
    "\n",
    "    df.loc[bad_coords, \"Descrição\"] = df.loc[bad_coords, \"Descrição\"] + \"*\"\n",
    "\n",
    "    df.loc[bad_coords, [\"Latitude\", \"Longitude\"]] = df.loc[\n",
    "        bad_coords, [\"Latitude_IBGE\", \"Longitude_IBGE\"]\n",
    "    ].values\n",
    "\n",
    "    df = df.loc[:, APP_ANALISE_PT]\n",
    "    df.columns = APP_ANALISE_EN\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _format_matlab(\n",
    "    df: pd.DataFrame,  # Arquivo de Dados Base de Entrada\n",
    ") -> pd.DataFrame:  # Arquivo de Dados formatado para leitura no Matlab\n",
    "    \"\"\"Formata o arquivo final de dados para o formato esperado pela aplicação em Matlab\"\"\"\n",
    "    df = df.astype(\"string\")\n",
    "    df.loc[len(df), :] = [\n",
    "        \"-1\",\n",
    "        \"-15.7801\",\n",
    "        \"-47.9292\",\n",
    "        \"[TEMP] L, FX, Estação do SMP licenciada (cadastro temporário)\",\n",
    "        \"10\",\n",
    "        \"999999999\",\n",
    "        \"NI\",\n",
    "        \"-1\",\n",
    "    ]  # Paliativo...\n",
    "    for c in [\"Latitude\", \"Longitude\"]:\n",
    "        df[c] = df[c].fillna(-1).astype(\"float32\")\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].astype(\"float64\")\n",
    "    df.loc[df.Service.isin([\"\", \"-1\"]), \"Service\"] = pd.NA\n",
    "    df[\"Service\"] = df.Service.fillna(\"-1\").astype(\"int16\")\n",
    "    df.loc[df.Station.isin([\"\", \"-1\"]), \"Station\"] = pd.NA\n",
    "    df[\"Station\"] = df.Station.fillna(\"-1\").astype(\"int32\")\n",
    "    df.loc[df.BW.isin([\"\", \"-1\"]), \"BW\"] = pd.NA\n",
    "    df[\"BW\"] = df[\"BW\"].astype(\"float32\").fillna(-1)\n",
    "    df.loc[df[\"Class\"].isin([\"\", \"-1\"]), \"Class\"] = pd.NA\n",
    "    df[\"Class\"] = df.Class.fillna(\"NI\").astype(\"category\")\n",
    "    df = df[df.Frequency <= LIMIT_FREQ]\n",
    "    df.sort_values(\n",
    "        by=[\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"], inplace=True\n",
    "    )\n",
    "    unique_columns = df.columns.tolist()\n",
    "    unique_columns.remove(\"Description\")\n",
    "    df = df.drop_duplicates(subset=unique_columns, keep=\"last\").reset_index(drop=True)\n",
    "    df[\"Id\"] = [f\"#{i+1}\" for i in df.index]\n",
    "    df[\"Id\"] = df.Id.astype(\"string\")\n",
    "    df.loc[df.Description == \"\", \"Description\"] = pd.NA\n",
    "    df[\"Description\"] = df[\"Description\"].astype(\"string\").fillna(\"NI\")\n",
    "    return df[[\"Id\"] + list(APP_ANALISE_EN)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesclagem\n",
    "Função auxiliar para mesclar registros que são iguais das diversas bases, i.e. estão a uma distância menor que `MAX_DIST` e verificar a validade da mesclagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_km_distance(row):\n",
    "    return geodesic((row[0], row[1]), (row[2], row[3])).km\n",
    "\n",
    "\n",
    "def merge_on_frequency(\n",
    "    df_left: pd.DataFrame,  # DataFrame da esquerda a ser mesclado\n",
    "    df_right: pd.DataFrame,  # DataFrame da direira a ser mesclado\n",
    "    on: str = \"Frequency\",  # Coluna usada como chave de mesclagem\n",
    "    coords: Tuple[str] = (\"Latitude\", \"Longitude\"),\n",
    "    description: str = \"Description\",\n",
    "    suffixes: Tuple[str] = (\"_x\", \"_y\"),  # Sufixo para as colunas que foram criadas\n",
    ") -> pd.DataFrame:  # DataFrame resultante da mesclagem\n",
    "    \"\"\"Mescla os dataframes baseados na frequência\n",
    "    É assumido que as colunas de ambos uma é subconjunto ou idêntica à outra, caso contrário os filtros não irão funcionar como esperado\n",
    "    \"\"\"\n",
    "    df_left = df_left.astype(\"string\").drop_duplicates(ignore_index=True)\n",
    "    df_right = df_right.astype(\"string\").drop_duplicates(ignore_index=True)\n",
    "    df: pd.DataFrame = pd.merge(\n",
    "        df_left,\n",
    "        df_right,\n",
    "        on=on,\n",
    "        how=\"outer\",\n",
    "        suffixes=suffixes,\n",
    "        indicator=True,\n",
    "        copy=False,\n",
    "    )\n",
    "\n",
    "    x, y = suffixes\n",
    "    lat, long = coords\n",
    "\n",
    "    left = df._merge == \"left_only\"\n",
    "    right = df._merge == \"right_only\"\n",
    "    both = df._merge == \"both\"\n",
    "    df = df.drop(columns=[\"_merge\"])\n",
    "\n",
    "    left_cols: List[str] = [c for c in df.columns if y not in c]\n",
    "    right_cols: List[str] = [c for c in df.columns if x not in c]\n",
    "\n",
    "    only_left = df.loc[left, left_cols].drop_duplicates(subset=left_cols, ignore_index=True)\n",
    "    only_left.columns = [c.removesuffix(x) for c in left_cols]\n",
    "\n",
    "    only_right = df.loc[right, right_cols].drop_duplicates(subset=right_cols, ignore_index=True)\n",
    "    only_right.columns = [c.removesuffix(y) for c in right_cols]\n",
    "\n",
    "    intersection_left = len(df_left) - len(only_left)\n",
    "    intersection_right = len(df_right) - len(only_right)\n",
    "\n",
    "    if not intersection_left or not intersection_right:\n",
    "        return pd.concat([df_left, df_right], ignore_index=True)\n",
    "\n",
    "    both_columns = [f\"{lat}{x}\", f\"{long}{x}\", f\"{lat}{y}\", f\"{long}{y}\"]\n",
    "    df.loc[both, \"Distance\"] = df.loc[both, both_columns].apply(get_km_distance, axis=1)\n",
    "\n",
    "    df_both = df[both].sort_values(\"Distance\", ignore_index=True)\n",
    "\n",
    "    filter_left_cols = df_both.columns[:len(df_left.columns)].to_list()\n",
    "    filter_right_cols = listify(on) + df_both.columns[len(df_left.columns):-1].to_list() # the -1 is to eliminate the distance\n",
    "   \n",
    "    df_both_left = df_both.groupby(filter_left_cols, as_index=False).first()\n",
    "    df_both_right = df_both.groupby(filter_right_cols, as_index=False).first()\n",
    "\n",
    "    assert len(df_both_left) == intersection_left, f\"O Agrupamento por colunas únicas não tem o comprimento esperado: {len(df_both_left)}!= {intersection_left}\"\n",
    "    \n",
    "    assert (\n",
    "        len(df_both_right) == intersection_right\n",
    "    ), f\"Error: {len(df_both_right)}!= {intersection_right}\"\n",
    "\n",
    "    df_both_far_left = df_both_left[df_both_left.Distance > MAX_DIST]\n",
    "    df_both_far_right = df_both_right[df_both_right.Distance > MAX_DIST]\n",
    "\n",
    "    df_both_left = df_both_left[df_both_left.Distance <= MAX_DIST]\n",
    "    df_both_right = df_both_right[df_both_right.Distance <= MAX_DIST]\n",
    "\n",
    "    merge_cols = df_both.columns.to_list()\n",
    "    merge_cols.remove(\"Distance\")\n",
    "    df_close_merge = (\n",
    "        pd.merge(df_both_left, df_both_right, how=\"inner\", on=merge_cols, copy=False)\n",
    "        .drop(\"Distance_y\", axis=1)\n",
    "        .rename(columns={\"Distance_x\": \"Distance\"})\n",
    "    )\n",
    "\n",
    "    df_both_left = _left_filter(df_both_left, df_close_merge, merge_cols)\n",
    "    df_both_right = _left_filter(df_both_right, df_close_merge, merge_cols)\n",
    "\n",
    "    assert pd.merge(\n",
    "        df_both_left, df_both_right, how=\"inner\", on=merge_cols\n",
    "    ).empty, \"Verifique os passos de mesclagem, df_both_left e df_both_right deveria ser disjuntos\"\n",
    "\n",
    "    df_final_merge = pd.concat(\n",
    "        [df_close_merge, df_both_left, df_both_right], ignore_index=True\n",
    "    )\n",
    "\n",
    "    assert len(df_both_far_left) + len(df_close_merge) + len(df_both_left) == (\n",
    "        len(df_left) - len(only_left)\n",
    "    ), \"Verifique os passos de mesclagem, validação falhou!\"\n",
    "    \n",
    "    assert len(df_both_far_right) + len(df_close_merge) + len(df_both_right) == (\n",
    "        len(df_right) - len(only_right)\n",
    "    ), \"Verifique os passos de mesclagem, validação falhou!\"\n",
    "\n",
    "    original_cols = df_both.columns.to_list()\n",
    "    df_both = (\n",
    "        pd.merge(\n",
    "            df_both, df_final_merge, how=\"left\", on=filter_left_cols, indicator=True\n",
    "        )\n",
    "        .loc[lambda x: x[\"_merge\"] == \"left_only\"]\n",
    "        .iloc[:, range(len(original_cols))]\n",
    "    )\n",
    "    df_both.columns = original_cols\n",
    "    \n",
    "    df_both = (\n",
    "        pd.merge(\n",
    "            df_both, df_final_merge, how=\"left\", on=filter_right_cols, indicator=True\n",
    "        )\n",
    "        .loc[lambda x: x[\"_merge\"] == \"left_only\"]\n",
    "        .iloc[:, range(len(original_cols))]\n",
    "    )\n",
    "    df_both.columns = original_cols\n",
    "\n",
    "    assert (\n",
    "        df_both.Distance > MAX_DIST\n",
    "    ).all(), \"Verifique os passos de mesclagem, validação falhou!\"\n",
    "    \n",
    "    assert pd.merge(\n",
    "        df_both, df_both_far_left, how=\"left\", on=filter_left_cols, indicator=True, copy=False\n",
    "    ).loc[lambda x: x[\"_merge\"] == \"left_only\"\n",
    "    ].iloc[:, range(len(original_cols))].empty, \"Verifique os passos de mesclagem, validação falhou!\"\n",
    "    \n",
    "    assert pd.merge(\n",
    "        df_both, df_both_far_right, how=\"left\", on=filter_right_cols, indicator=True, copy=False\n",
    "    ).loc[lambda x: x[\"_merge\"] == \"left_only\"\n",
    "    ].iloc[:, range(len(original_cols))].empty, \"Verifique os passos de mesclagem, validação falhou!\"\n",
    "    \n",
    "    df_final_merge[f\"{description}{x}\"] = (\n",
    "        df_final_merge[f\"{description}{x}\"]\n",
    "        + \" | \"\n",
    "        + df_final_merge[f\"{description}{y}\"]\n",
    "    )\n",
    "\n",
    "    df_final_merge = df_final_merge[left_cols]\n",
    "    df_final_merge.columns = only_left.columns\n",
    "\n",
    "    df_both_far_left = df_both_far_left[left_cols]\n",
    "    df_both_far_left.columns = only_left.columns\n",
    "\n",
    "    df_both_far_right = df_both_far_right[right_cols]\n",
    "    df_both_far_right.columns = only_right.columns\n",
    "\n",
    "    merged_df = pd.concat(\n",
    "        [only_left, df_both_far_left, df_final_merge, only_right, df_both_far_right],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    return merged_df.astype(\"string\").drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "def _left_filter(df, df_close_merge, merge_cols):\n",
    "    df = pd.merge(\n",
    "        df, df_close_merge, how=\"left\", on=merge_cols, indicator=True, copy=False\n",
    "    )\n",
    "    df = df.loc[df[\"_merge\"] == \"left_only\"]\n",
    "    return df.drop([\"_merge\", \"Distance_y\"], axis=1).rename(\n",
    "        columns={\"Distance_x\": \"Distance\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| \n",
    "folder = Path.cwd().parent / \"dados\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left = _filter_matlab(pd.read_parquet(folder / \"base.parquet.gzip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right = pd.read_parquet(folder / \"aero.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merge_on_frequency(df_left, df_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
