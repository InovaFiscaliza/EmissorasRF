{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp format\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "from typing import Iterable, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.utils import listify\n",
    "from fastcore.xtras import Path\n",
    "from geopy.distance import geodesic\n",
    "from rich.progress import Progress\n",
    "from pyarrow import ArrowInvalid\n",
    "\n",
    "from extracao.constants import BW, BW_pattern\n",
    "\n",
    "RE_BW = re.compile(BW_pattern)\n",
    "\n",
    "MAX_DIST = 10  # Km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _read_df(folder: Union[str, Path], stem: str) -> pd.DataFrame:\n",
    "    \"\"\"Lê o dataframe formado por folder / stem.[parquet.gzip | fth | xslx]\"\"\"\n",
    "    file = Path(f\"{folder}/{stem}.parquet.gzip\")\n",
    "    try:\n",
    "        df = pd.read_parquet(file)\n",
    "    except (ArrowInvalid, FileNotFoundError) as e:\n",
    "        raise e(f\"Error when reading {file}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatação\n",
    "\n",
    "> Este módulo possui funções auxiliares de formatação dos dados das várias fontes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_bw(\n",
    "    bw: str,  # Designação de Emissão (Largura + Classe) codificada como string\n",
    ") -> Tuple[str, str]:  # Largura e Classe de Emissão\n",
    "    \"\"\"Parse the bandwidth string\"\"\"\n",
    "    if match := re.match(RE_BW, bw):\n",
    "        multiplier = BW[match.group(2)]\n",
    "        if mantissa := match.group(3):\n",
    "            number = float(f\"{match.group(1)}.{mantissa}\")\n",
    "        else:\n",
    "            number = float(match.group(1))\n",
    "        classe = match.group(4)\n",
    "        return str(multiplier * number), str(classe)\n",
    "    return \"-1\", \"-1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesclagem\n",
    "Função auxiliar para mesclar registros que são iguais das diversas bases, i.e. estão a uma distância menor que `MAX_DIST` e verificar a validade da mesclagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def merge_close_rows(df_left, df_right):\n",
    "    \"\"\"Mescla os registros dos DataFrames `df_left` e `df_right` que estão a uma distância menor que MAX_DIST\"\"\"\n",
    "    df1 = df_left.copy().reset_index(drop=True)\n",
    "    df2 = df_right.copy().reset_index(drop=True)\n",
    "    columns = [\"Frequency\", \"Latitude\", \"Longitude\"]\n",
    "    for c in columns:\n",
    "        df1[c] = df1[c].astype(\"float\")\n",
    "        df2[c] = df2[c].astype(\"float\")\n",
    "    df1.sort_values(columns, inplace=True)\n",
    "    df2.sort_values(columns, inplace=True)\n",
    "    with Progress(transient=True, refresh_per_second=2) as progress:\n",
    "        task_left = progress.add_task(\n",
    "            \"[red]Iterando Tabela Principal...\", total=len(df1)\n",
    "        )\n",
    "        for left in df1.itertuples():\n",
    "            for right in df2[np.isclose(df2.Frequency, left.Frequency)].itertuples():\n",
    "                if (\n",
    "                    geodesic(\n",
    "                        (left.Latitude, left.Longitude),\n",
    "                        (right.Latitude, right.Longitude),\n",
    "                    ).km\n",
    "                    <= MAX_DIST\n",
    "                ):\n",
    "                    df1.loc[\n",
    "                        left.Index, \"Description\"\n",
    "                    ] = f\"{left.Description} | {right.Description}\"\n",
    "                    df2 = df2.drop(right.Index)\n",
    "                    break\n",
    "            progress.update(\n",
    "                task_left,\n",
    "                advance=1,\n",
    "                description=f\"[green] Comparando Frequências {left.Frequency}MHz\",\n",
    "            )\n",
    "        return pd.concat([df1, df2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimização dos Tipos de dados\n",
    "A serem criados dataframes, normalmente a tipo de data é aquele com maior resolução possível, nem sempre isso é necessário, os arquivos de espectro mesmo possuem somente uma casa decimal, portanto um `float16` já é suficiente para armazená-los. As funções a seguir fazem essa otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below borrowed from https://medium.com/bigdatarepublic/advanced-pandas-optimize-speed-and-memory-a654b53be6c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def optimize_floats(\n",
    "    df: pd.DataFrame,  # DataFrame a ser otimizado\n",
    "    exclude: Iterable[str] = None,  # Colunas a serem excluidas da otimização\n",
    ") -> pd.DataFrame:  # DataFrame com as colunas do tipo `float` otimizadas\n",
    "    \"\"\"Otimiza os floats do dataframe para reduzir o uso de memória\"\"\"\n",
    "    floats = df.select_dtypes(include=[\"float64\"]).columns.tolist()\n",
    "    floats = [c for c in floats if c not in listify(exclude)]\n",
    "    df[floats] = df[floats].apply(pd.to_numeric, downcast=\"float\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def optimize_ints(\n",
    "    df: pd.DataFrame,  # Dataframe a ser otimizado\n",
    "    exclude: Iterable[str] = None,  # Colunas a serem excluidas da otimização\n",
    ") -> pd.DataFrame:  # DataFrame com as colunas do tipo `int` otimizadas\n",
    "    \"\"\"Otimiza os ints do dataframe para reduzir o uso de memória\"\"\"\n",
    "    ints = df.select_dtypes(include=[\"int64\"]).columns.tolist()\n",
    "    ints = [c for c in ints if c not in listify(exclude)]\n",
    "    df[ints] = df[ints].apply(pd.to_numeric, downcast=\"integer\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def optimize_objects(\n",
    "    df: pd.DataFrame,  # DataFrame a ser otimizado\n",
    "    datetime_features: Iterable[\n",
    "        str\n",
    "    ] = None,  # Colunas que serão convertidas para datetime\n",
    "    exclude: Iterable[str] = None,  # Colunas que não serão convertidas\n",
    ") -> pd.DataFrame:  # DataFrame com as colunas do tipo `object` otimizadas\n",
    "    \"\"\"Otimiza as colunas do tipo `object` no DataFrame para `category` ou `string` para reduzir a memória e tamanho de arquivo\"\"\"\n",
    "    exclude = listify(exclude)\n",
    "    datetime_features = listify(datetime_features)\n",
    "    for col in df.select_dtypes(\n",
    "        include=[\"object\", \"string\", \"category\"]\n",
    "    ).columns.tolist():\n",
    "        if col not in datetime_features:\n",
    "            if col in exclude:\n",
    "                continue\n",
    "            num_unique_values = len(df[col].unique())\n",
    "            num_total_values = len(df[col])\n",
    "            if float(num_unique_values) / num_total_values < 0.5:\n",
    "                dtype = \"category\"\n",
    "            else:\n",
    "                dtype = \"string\"\n",
    "            df[col] = df[col].astype(dtype)\n",
    "        else:\n",
    "            df[col] = pd.to_datetime(df[col]).dt.date\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def df_optimize(\n",
    "    df: pd.DataFrame,  # DataFrame a ser otimizado\n",
    "    datetime_features: Iterable[\n",
    "        str\n",
    "    ] = None,  # Colunas que serão convertidas para datetime\n",
    "    exclude: Iterable[str] = None,  # Colunas que não serão convertidas\n",
    ") -> pd.DataFrame:  # DataFrame com as colunas com tipos de dados otimizados\n",
    "    \"\"\"Função que encapsula as anteriores para otimizar os tipos de dados e reduzir o tamanho do arquivo e uso de memória\"\"\"\n",
    "    if datetime_features is None:\n",
    "        datetime_features = []\n",
    "    return optimize_floats(\n",
    "        optimize_ints(optimize_objects(df, datetime_features, exclude), exclude),\n",
    "        exclude,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
