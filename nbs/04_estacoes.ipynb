{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp estacoes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatel\n",
    "\n",
    "> Este módulo consolida as bases da Anatel e realiza pós-processamento dos dados obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import urllib.request\n",
    "from typing import List, Union\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from fastcore.foundation import L\n",
    "from fastcore.parallel import parallel\n",
    "from fastcore.xtras import Path\n",
    "from pyarrow import ArrowInvalid, ArrowTypeError\n",
    "\n",
    "\n",
    "from extracao.constants import (\n",
    "\tCOLS_SRD,\n",
    "\tIBGE_MUNICIPIOS,\n",
    "\tIBGE_POLIGONO,\n",
    "\tMALHA_IBGE,\n",
    "\tFLOAT_COLUMNS,\n",
    "\tINT_COLUMNS,\n",
    "\tSTR_COLUMNS,\n",
    "\tCAT_COLUMNS,\n",
    ")\n",
    "from extracao.datasources.aeronautica import Aero\n",
    "from extracao.datasources.base import Base\n",
    "from extracao.datasources.mosaico import MONGO_URI\n",
    "from extracao.datasources.sitarweb import SQLSERVER_PARAMS, Radcom, Stel\n",
    "from extracao.datasources.smp import SMP\n",
    "from extracao.datasources.srd import SRD\n",
    "from extracao.datasources.telecom import Telecom\n",
    "from extracao.format import merge_on_frequency, LIMIT_FREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "load_dotenv(find_dotenv(), override=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Consolidada ANATEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Estacoes(Base):\n",
    "\t\"\"\"Classe auxiliar para agregar os dados originários da Anatel\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tsql_params: dict = SQLSERVER_PARAMS,\n",
    "\t\tmongo_uri: str = MONGO_URI,\n",
    "\t\tlimit: int = 0,\n",
    "\t\tparallel: bool = True,\n",
    "\t):\n",
    "\t\tself.sql_params = sql_params\n",
    "\t\tself.mongo_uri = mongo_uri\n",
    "\t\tself.limit = limit\n",
    "\t\tself.parallel = parallel\n",
    "\t\tself.init_data_sources()\n",
    "\n",
    "\t@property\n",
    "\tdef columns(self):\n",
    "\t\treturn COLS_SRD\n",
    "\n",
    "\tdef build_from_sources(self) -> pd.DataFrame:\n",
    "\t\treturn self._format([s.df() for s in self.sources.values()])\n",
    "\n",
    "\t@property\n",
    "\tdef stem(self):\n",
    "\t\treturn 'estacoes'\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _update_source(class_instance):\n",
    "\t\tclass_instance.update()\n",
    "\t\tclass_instance.save()\n",
    "\t\treturn class_instance\n",
    "\n",
    "\tdef init_data_sources(self):\n",
    "\t\tself.sources = {\n",
    "\t\t\t'telecom': Telecom(self.mongo_uri, self.limit),\n",
    "\t\t\t'smp': SMP(self.mongo_uri, self.limit),\n",
    "\t\t\t'srd': SRD(self.mongo_uri, self.limit),\n",
    "\t\t\t'stel': Stel(self.sql_params),\n",
    "\t\t\t'radcom': Radcom(self.sql_params),\n",
    "\t\t\t'aero': Aero(),\n",
    "\t\t}\n",
    "\n",
    "\tdef extraction(self) -> L:\n",
    "\t\tif self.parallel:\n",
    "\t\t\tsources = parallel(\n",
    "\t\t\t\tEstacoes._update_source,\n",
    "\t\t\t\tself.sources.values(),\n",
    "\t\t\t\tn_workers=len(self.sources),\n",
    "\t\t\t\tprogress=True,\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tsources = L(self._update_source(s) for s in self.sources.values())\n",
    "\t\treturn sources.attrgot('df')\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef verify_shapefile_folder():\n",
    "\t\t# Convert the file paths to Path objects\n",
    "\t\tshapefile_path = Path(IBGE_POLIGONO)\n",
    "\t\tparent_folder = shapefile_path.parent\n",
    "\t\tparent_folder.mkdir(exist_ok=True, parents=True)\n",
    "\t\tzip_file_path = parent_folder.with_suffix('.zip')\n",
    "\n",
    "\t\t# Check if all required files exist\n",
    "\t\trequired_files = L('.cpg', '.dbf', '.prj', '.shx').map(shapefile_path.with_suffix)\n",
    "\t\tif not all(required_files.map(Path.is_file)):\n",
    "\t\t\t# shutil.rmtree(str(shapefile_path.parent), ignore_errors=True)\n",
    "\t\t\tparent_folder.ls().map(Path.unlink)\n",
    "\t\t\t# Download and unzip the zipped folder\n",
    "\t\t\turllib.request.urlretrieve(MALHA_IBGE, zip_file_path)\n",
    "\t\t\twith ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "\t\t\t\tzip_ref.extractall(parent_folder)\n",
    "\t\t\tzip_file_path.unlink()\n",
    "\n",
    "\tdef fill_nan_coordinates(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame com os dados da Anatel\n",
    "\t) -> pd.DataFrame:  # DataFrame com as coordenadas validadas na base do IBGE\n",
    "\t\t\"\"\"Valida as coordenadas consultado a Base Corporativa do IBGE, excluindo o que já está no cache na versão anterior\"\"\"\n",
    "\n",
    "\t\tmunicipios = pd.read_csv(\n",
    "\t\t\tIBGE_MUNICIPIOS,\n",
    "\t\t\tusecols=['Código_Município', 'Latitude', 'Longitude'],\n",
    "\t\t\tdtype='string[pyarrow]',\n",
    "\t\t\tdtype_backend='pyarrow',\n",
    "\t\t)\n",
    "\n",
    "\t\tdf['Código_Município'] = df['Código_Município'].astype('string[pyarrow]')\n",
    "\n",
    "\t\tdf = pd.merge(\n",
    "\t\t\tdf,\n",
    "\t\t\tmunicipios,\n",
    "\t\t\ton='Código_Município',\n",
    "\t\t\thow='left',\n",
    "\t\t\tcopy=False,\n",
    "\t\t)\n",
    "\n",
    "\t\tnull_coords = df.Latitude_x.isna() | df.Longitude_x.isna()\n",
    "\n",
    "\t\tdf.loc[null_coords, ['Latitude_x', 'Longitude_x']] = df.loc[\n",
    "\t\t\tnull_coords, ['Latitude_y', 'Longitude_y']\n",
    "\t\t]\n",
    "\n",
    "\t\tlog = \"\"\"[(\"Colunas\", [\"Latitude\", \"Longitude\"]),\n",
    "\t\t           (\"Processamento\", \"Coordenadas Ausentes. Inserido coordenadas do Município\")]\"\"\"\n",
    "\t\tdf = self.register_log(df, log, null_coords)\n",
    "\n",
    "\t\tdf.rename(\n",
    "\t\t\tcolumns={\n",
    "\t\t\t\t'Latitude_x': 'Latitude',\n",
    "\t\t\t\t'Longitude_x': 'Longitude',\n",
    "\t\t\t\t'Latitude_y': 'Latitude_ibge',\n",
    "\t\t\t\t'Longitude_y': 'Longitude_ibge',\n",
    "\t\t\t},\n",
    "\t\t\tinplace=True,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn df\n",
    "\n",
    "\tdef intersect_coordinates_on_poligon(self, df: pd.DataFrame, check_municipio: bool = True):\n",
    "\t\tfor column in ['Latitude', 'Longitude']:\n",
    "\t\t\tdf[column] = pd.to_numeric(df[column], errors='coerce').astype('float')\n",
    "\t\tregions = gpd.read_file(IBGE_POLIGONO)\n",
    "\n",
    "\t\t# Convert pandas dataframe to geopandas df with geometry point given coordinates\n",
    "\t\tgdf_points = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude))\n",
    "\n",
    "\t\t# Set the same coordinate reference system (CRS) as the regions shapefile\n",
    "\t\tgdf_points.crs = regions.crs\n",
    "\n",
    "\t\t# Spatial join points to the regions\n",
    "\t\tgdf = gpd.sjoin(gdf_points, regions, how='inner', predicate='within')\n",
    "\n",
    "\t\tif check_municipio:\n",
    "\t\t\t# Check correctness of Coordinates\n",
    "\t\t\tcheck_coords = gdf.Código_Município != gdf.CD_MUN\n",
    "\n",
    "\t\t\tlog = \"\"\"[(\"Colunas\", [\"Código_Município\", \"Município\", \"UF\"]),\n",
    "\t\t\t\t  \t (\"Processamento\", \"Informações substituídas  pela localização correta das coordenadas.\")\t\t      \n",
    "\t\t\t\t  \"\"\"\n",
    "\t\t\tself.register_log(gdf, log, check_coords)\n",
    "\n",
    "\t\t\tgdf.drop(\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\t'Código_Município',\n",
    "\t\t\t\t\t'Município',\n",
    "\t\t\t\t\t'UF',\n",
    "\t\t\t\t\t'geometry',\n",
    "\t\t\t\t\t'AREA_KM2',\n",
    "\t\t\t\t\t'index_right',\n",
    "\t\t\t\t],\n",
    "\t\t\t\taxis=1,\n",
    "\t\t\t\tinplace=True,\n",
    "\t\t\t)\n",
    "\n",
    "\t\tgdf.rename(\n",
    "\t\t\tcolumns={\n",
    "\t\t\t\t'CD_MUN': 'Código_Município',\n",
    "\t\t\t\t'NM_MUN': 'Município',\n",
    "\t\t\t\t'SIGLA_UF': 'UF',\n",
    "\t\t\t},\n",
    "\t\t\tinplace=True,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn gdf\n",
    "\n",
    "\tdef validate_coordinates(self, df: pd.DataFrame, check_municipio: bool = True) -> pd.DataFrame:\n",
    "\t\t\"\"\"\n",
    "\t\tValidates the coordinates in the given DataFrame.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t        df: The DataFrame containing the coordinates to be validated.\n",
    "\t\t        check_municipio: A boolean indicating whether to check the municipality information (default: True).\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t        pd.DataFrame: The DataFrame with validated coordinates.\n",
    "\n",
    "\t\tRaises:\n",
    "\t\t        None\n",
    "\t\t\"\"\"\n",
    "\t\tself.verify_shapefile_folder()\n",
    "\t\tif check_municipio:\n",
    "\t\t\tdf = self.fill_nan_coordinates(df)\n",
    "\t\treturn self.intersect_coordinates_on_poligon(df, check_municipio)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _simplify_sources(df):\n",
    "\t\tdf['Fonte'] = df['Fonte'].str.replace(\n",
    "\t\t\t'ICAO-CANALIZACAO-VOR/ILS/DME | AISWEB-CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t\t'CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t)\n",
    "\t\tdf['Fonte'] = df['Fonte'].str.replace(\n",
    "\t\t\tr'(ICAO-)?(AISWEB-)?CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t\t'CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t\tregex=True,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn df\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2float(column: pd.Series) -> pd.Series:\n",
    "\t\treturn pd.to_numeric(\n",
    "\t\t\tcolumn.fillna('-1'),\n",
    "\t\t\tdowncast='float',\n",
    "\t\t\terrors='coerce',\n",
    "\t\t\tdtype_backend='pyarrow',\n",
    "\t\t)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2int(column: pd.Series) -> pd.Series:\n",
    "\t\treturn pd.to_numeric(\n",
    "\t\t\tcolumn.fillna('0'),\n",
    "\t\t\tdowncast='unsigned',\n",
    "\t\t\terrors='coerce',\n",
    "\t\t\tdtype_backend='pyarrow',\n",
    "\t\t)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2str(column: pd.Series) -> pd.Series:\n",
    "\t\tcolumn.replace('', '-1', inplace=True)\n",
    "\t\treturn column.astype('string', copy=False).fillna('-1')\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2cat(column: pd.Series) -> pd.Series:\n",
    "\t\tcolumn.replace('', '-1', inplace=True)\n",
    "\t\treturn column.fillna('-1').astype('category', copy=False)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _remove_invalid_frequencies(df):\n",
    "\t\tdf.sort_values(['Frequência', 'Latitude', 'Longitude'], ignore_index=True, inplace=True)\n",
    "\t\treturn df[df['Frequência'] <= LIMIT_FREQ]\n",
    "\t\t# TODO: save to discarded and log\n",
    "\t\t# log = f\"\"\"[(\"Colunas\", \"Frequência\"),\n",
    "\t\t# \t\t   (\"Processamento\", \"Frequência Inválida: Maior que {LIMIT_FREQ}\")\n",
    "\t\t# \t\t  \"\"\"\n",
    "\t\t# self.register_log(df, log, check_coords)\n",
    "\n",
    "\tdef _save(self, df: pd.DataFrame, folder: Union[str, Path], stem: str) -> pd.DataFrame:\n",
    "\t\t\"\"\"Format, Save and return a dataframe\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tfile = Path(f'{folder}/{stem}.parquet.gzip')\n",
    "\t\t\tdf.to_parquet(file, compression='gzip', index=False, engine='pyarrow')\n",
    "\t\texcept (ArrowInvalid, ArrowTypeError) as e:\n",
    "\t\t\traise e(f'Não foi possível salvar o arquivo parquet {file}') from e\n",
    "\t\treturn df\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _format_types(df):\n",
    "\t\tdf['Frequência'] = df['Frequência'].astype('double[pyarrow]')\n",
    "\t\tfor col in FLOAT_COLUMNS:\n",
    "\t\t\tdf[col] = Estacoes._cast2float(df[col])\n",
    "\t\tfor col in INT_COLUMNS:\n",
    "\t\t\tdf[col] = Estacoes._cast2int(df[col])\n",
    "\t\tfor col in CAT_COLUMNS:\n",
    "\t\t\tdf[col] = Estacoes._cast2cat(df[col])\n",
    "\t\tfor col in STR_COLUMNS:\n",
    "\t\t\tdf[col] = Estacoes._cast2str(df[col])\n",
    "\t\treturn df\n",
    "\n",
    "\tdef _format(\n",
    "\t\tself,\n",
    "\t\tdfs: List,  # List with the individual API sources\n",
    "\t) -> pd.DataFrame:  # Processed DataFrame\n",
    "\t\taero = dfs.pop()\n",
    "\t\tanatel = pd.concat(dfs, ignore_index=True)\n",
    "\t\tdf = merge_on_frequency(anatel, aero)\n",
    "\t\tdf = self.validate_coordinates(df)\n",
    "\t\tdf = Estacoes._simplify_sources(df)\n",
    "\t\tdf = Estacoes._format_types(df)\n",
    "\t\tdf = Estacoes._remove_invalid_frequencies(df)\n",
    "\t\treturn df.loc[:, self.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class Estacoes(Base):\n",
    "\t\"\"\"Classe auxiliar para agregar os dados originários da Anatel\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tsql_params: dict = SQLSERVER_PARAMS,\n",
    "\t\tmongo_uri: str = MONGO_URI,\n",
    "\t\tlimit: int = 0,\n",
    "\t\tparallel: bool = True,\n",
    "\t):\n",
    "\t\tself.sql_params = sql_params\n",
    "\t\tself.mongo_uri = mongo_uri\n",
    "\t\tself.limit = limit\n",
    "\t\tself.parallel = parallel\n",
    "\t\tself.init_data_sources()\n",
    "\n",
    "\t@property\n",
    "\tdef columns(self):\n",
    "\t\treturn COLS_SRD\n",
    "\n",
    "\tdef build_from_sources(self) -> pd.DataFrame:\n",
    "\t\treturn self._format([s.df() for s in self.sources.values()])\n",
    "\n",
    "\t@property\n",
    "\tdef stem(self):\n",
    "\t\treturn 'estacoes'\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _update_source(class_instance):\n",
    "\t\tclass_instance.update()\n",
    "\t\tclass_instance.save()\n",
    "\t\treturn class_instance\n",
    "\n",
    "\tdef init_data_sources(self):\n",
    "\t\tself.sources = {\n",
    "\t\t\t'telecom': Telecom(self.mongo_uri, self.limit),\n",
    "\t\t\t'smp': SMP(self.mongo_uri, self.limit),\n",
    "\t\t\t'srd': SRD(self.mongo_uri, self.limit),\n",
    "\t\t\t# 'stel': Stel(self.sql_params),\n",
    "\t\t\t# 'radcom': Radcom(self.sql_params),\n",
    "\t\t\t'aero': Aero(),\n",
    "\t\t}\n",
    "\n",
    "\tdef extraction(self) -> L:\n",
    "\t\tif self.parallel:\n",
    "\t\t\tsources = parallel(\n",
    "\t\t\t\tEstacoes._update_source,\n",
    "\t\t\t\tself.sources.values(),\n",
    "\t\t\t\tn_workers=len(self.sources),\n",
    "\t\t\t\tprogress=True,\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tsources = L(self._update_source(s) for s in self.sources.values())\n",
    "\t\treturn sources.attrgot('df')\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef verify_shapefile_folder():\n",
    "\t\t# Convert the file paths to Path objects\n",
    "\t\tshapefile_path = Path(IBGE_POLIGONO)\n",
    "\t\tparent_folder = shapefile_path.parent\n",
    "\t\tparent_folder.mkdir(exist_ok=True, parents=True)\n",
    "\t\tzip_file_path = parent_folder.with_suffix('.zip')\n",
    "\n",
    "\t\t# Check if all required files exist\n",
    "\t\trequired_files = L('.cpg', '.dbf', '.prj', '.shx').map(shapefile_path.with_suffix)\n",
    "\t\tif not all(required_files.map(Path.is_file)):\n",
    "\t\t\t# shutil.rmtree(str(shapefile_path.parent), ignore_errors=True)\n",
    "\t\t\tparent_folder.ls().map(Path.unlink)\n",
    "\t\t\t# Download and unzip the zipped folder\n",
    "\t\t\turllib.request.urlretrieve(MALHA_IBGE, zip_file_path)\n",
    "\t\t\twith ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "\t\t\t\tzip_ref.extractall(parent_folder)\n",
    "\t\t\tzip_file_path.unlink()\n",
    "\n",
    "\tdef fill_nan_coordinates(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame com os dados da Anatel\n",
    "\t) -> pd.DataFrame:  # DataFrame com as coordenadas validadas na base do IBGE\n",
    "\t\t\"\"\"Valida as coordenadas consultado a Base Corporativa do IBGE, excluindo o que já está no cache na versão anterior\"\"\"\n",
    "\n",
    "\t\tmunicipios = pd.read_csv(\n",
    "\t\t\tIBGE_MUNICIPIOS,\n",
    "\t\t\tusecols=['Código_Município', 'Latitude', 'Longitude'],\n",
    "\t\t\tdtype='string[pyarrow]',\n",
    "\t\t\tdtype_backend='pyarrow',\n",
    "\t\t)\n",
    "\n",
    "\t\tdf = pd.merge(\n",
    "\t\t\tdf.astype('string[pyarrow]'),\n",
    "\t\t\tmunicipios,\n",
    "\t\t\ton='Código_Município',\n",
    "\t\t\thow='left',\n",
    "\t\t\tcopy=False,\n",
    "\t\t)\n",
    "\n",
    "\t\tnull_coords = df.Latitude_x.isna() | df.Longitude_x.isna()\n",
    "\n",
    "\t\tdf.loc[null_coords, ['Latitude_x', 'Longitude_x']] = df.loc[\n",
    "\t\t\tnull_coords, ['Latitude_y', 'Longitude_y']\n",
    "\t\t]\n",
    "\n",
    "\t\tlog = \"\"\"[(\"Colunas\", [\"Latitude\", \"Longitude\"]),\n",
    "\t\t\t\t   (\"Processamento\", \"Coordenadas Ausentes. Inserido coordenadas do Município\")]\"\"\"\n",
    "\t\tdf = self.register_log(df, log, null_coords)\n",
    "\n",
    "\t\tdf.rename(\n",
    "\t\t\tcolumns={\n",
    "\t\t\t\t'Latitude_x': 'Latitude',\n",
    "\t\t\t\t'Longitude_x': 'Longitude',\n",
    "\t\t\t\t'Latitude_y': 'Latitude_ibge',\n",
    "\t\t\t\t'Longitude_y': 'Longitude_ibge',\n",
    "\t\t\t},\n",
    "\t\t\tinplace=True,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn df\n",
    "\n",
    "\tdef intersect_coordinates_on_poligon(self, df: pd.DataFrame, check_municipio: bool = True):\n",
    "\t\tfor column in ['Latitude', 'Longitude']:\n",
    "\t\t\tdf[column] = pd.to_numeric(df[column], errors='coerce').astype('float')\n",
    "\t\tregions = gpd.read_file(IBGE_POLIGONO)\n",
    "\n",
    "\t\t# Convert pandas dataframe to geopandas df with geometry point given coordinates\n",
    "\t\tgdf_points = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude))\n",
    "\n",
    "\t\t# Set the same coordinate reference system (CRS) as the regions shapefile\n",
    "\t\tgdf_points.crs = regions.crs\n",
    "\n",
    "\t\t# Spatial join points to the regions\n",
    "\t\tgdf = gpd.sjoin(gdf_points, regions, how='inner', predicate='within')\n",
    "\n",
    "\t\tif check_municipio:\n",
    "\t\t\t# Check correctness of Coordinates\n",
    "\t\t\tcheck_coords = gdf.Código_Município != gdf.CD_MUN\n",
    "\n",
    "\t\t\tlog = \"\"\"[(\"Colunas\", [\"Código_Município\", \"Município\", \"UF\"]),\n",
    "\t\t\t\t  \t (\"Processamento\", \"Informações substituídas  pela localização correta das coordenadas.\")\t\t      \n",
    "\t\t\t\t  \"\"\"\n",
    "\t\t\tself.register_log(gdf, log, check_coords)\n",
    "\n",
    "\t\t\tgdf.drop(\n",
    "\t\t\t\t['Código_Município', 'Município', 'UF', 'geometry', 'AREA_KM2'],\n",
    "\t\t\t\taxis=1,\n",
    "\t\t\t\tinplace=True,\n",
    "\t\t\t)\n",
    "\n",
    "\t\tgdf.rename(\n",
    "\t\t\tcolumns={\n",
    "\t\t\t\t'CD_MUN': 'Código_Município',\n",
    "\t\t\t\t'NM_MUN': 'Município',\n",
    "\t\t\t\t'SIGLA_UF': 'UF',\n",
    "\t\t\t},\n",
    "\t\t\tinplace=True,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn gdf\n",
    "\n",
    "\tdef validate_coordinates(self, df: pd.DataFrame, check_municipio: bool = True) -> pd.DataFrame:\n",
    "\t\t\"\"\"\n",
    "\t\tValidates the coordinates in the given DataFrame.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t                df: The DataFrame containing the coordinates to be validated.\n",
    "\t\t                check_municipio: A boolean indicating whether to check the municipality information (default: True).\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t                pd.DataFrame: The DataFrame with validated coordinates.\n",
    "\n",
    "\t\tRaises:\n",
    "\t\t                None\n",
    "\t\t\"\"\"\n",
    "\t\tself.verify_shapefile_folder()\n",
    "\t\tif check_municipio:\n",
    "\t\t\tdf = self.fill_nan_coordinates(df)\n",
    "\t\treturn self.intersect_coordinates_on_poligon(df, check_municipio)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _simplify_sources(df):\n",
    "\t\tdf['Fonte'] = df['Fonte'].str.replace(\n",
    "\t\t\t'ICAO-CANALIZACAO-VOR/ILS/DME | AISWEB-CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t\t'CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t)\n",
    "\t\tdf['Fonte'] = df['Fonte'].str.replace(\n",
    "\t\t\tr'(ICAO-)?(AISWEB-)?CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t\t'CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t\tregex=True,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn df\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2float(column: pd.Series) -> pd.Series:\n",
    "\t\treturn pd.to_numeric(\n",
    "\t\t\tcolumn, downcast='float', errors='coerce', dtype_backend='pyarrow'\n",
    "\t\t).fillna(-1.0)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2int(column: pd.Series) -> pd.Series:\n",
    "\t\treturn pd.to_numeric(\n",
    "\t\t\tcolumn, downcast='integer', errors='coerce', dtype_backend='pyarrow'\n",
    "\t\t).fillna(-1)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2str(column: pd.Series) -> pd.Series:\n",
    "\t\tcolumn.replace('', '-1', inplace=True)\n",
    "\t\treturn column.astype('string', copy=False).fillna('-1')\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2cat(column: pd.Series) -> pd.Series:\n",
    "\t\tcolumn.replace('', '-1', inplace=True)\n",
    "\t\treturn column.fillna('-1').astype('category', copy=False)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _remove_invalid_frequencies(df):\n",
    "\t\tdf.sort_values(['Frequência', 'Latitude', 'Longitude'], ignore_index=True, inplace=True)\n",
    "\t\treturn df[df['Frequência'] <= LIMIT_FREQ]\n",
    "\t\t# TODO: save to discarded and log\n",
    "\t\t# log = f\"\"\"[(\"Colunas\", \"Frequência\"),\n",
    "\t\t# \t\t   (\"Processamento\", \"Frequência Inválida: Maior que {LIMIT_FREQ}\")\n",
    "\t\t# \t\t  \"\"\"\n",
    "\t\t# self.register_log(df, log, check_coords)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _format_types(df):\n",
    "\t\tcols = ['Frequência', 'Latitude', 'Longitude']\n",
    "\t\tfor col in cols:\n",
    "\t\t\tdf[col] = Estacoes._cast2float(df[col])\n",
    "\t\tfor col in df.columns:\n",
    "\t\t\tif col not in cols:\n",
    "\t\t\t\tdf[col] = Estacoes._cast2cat(df[col])\n",
    "\t\treturn df\n",
    "\n",
    "\tdef _format(\n",
    "\t\tself,\n",
    "\t\tdfs: List,  # List with the individual API sources\n",
    "\t) -> pd.DataFrame:  # Processed DataFrame\n",
    "\t\taero = dfs.pop()\n",
    "\t\tanatel = pd.concat(dfs, ignore_index=True)\n",
    "\t\tdf = merge_on_frequency(anatel, aero)\n",
    "\t\tdf = self.validate_coordinates(df)\n",
    "\t\tdf = Estacoes._simplify_sources(df)\n",
    "\t\tdf = Estacoes._format_types(df)\n",
    "\t\tdf = Estacoes._remove_invalid_frequencies(df)\n",
    "\t\tdf = Estacoes._format_types(df)\n",
    "\t\treturn df.loc[:, self.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`=======`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class Estacoes(Base):\n",
    "\t\"\"\"Classe auxiliar para agregar os dados originários da Anatel\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tsql_params: dict = SQLSERVER_PARAMS,\n",
    "\t\tmongo_uri: str = MONGO_URI,\n",
    "\t\tlimit: int = 0,\n",
    "\t\tparallel: bool = True,\n",
    "\t):\n",
    "\t\tself.sql_params = sql_params\n",
    "\t\tself.mongo_uri = mongo_uri\n",
    "\t\tself.limit = limit\n",
    "\t\tself.parallel = parallel\n",
    "\t\tself.init_data_sources()\n",
    "\n",
    "\t@property\n",
    "\tdef columns(self):\n",
    "\t\treturn COLS_SRD\n",
    "\n",
    "\tdef build_from_sources(self) -> pd.DataFrame:\n",
    "\t\treturn self._format([s.df() for s in self.sources.values()])\n",
    "\n",
    "\t@property\n",
    "\tdef stem(self):\n",
    "\t\treturn 'estacoes'\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _update_source(class_instance):\n",
    "\t\tclass_instance.update()\n",
    "\t\tclass_instance.save()\n",
    "\t\treturn class_instance\n",
    "\n",
    "\tdef init_data_sources(self):\n",
    "\t\tself.sources = {\n",
    "\t\t\t'telecom': Telecom(self.mongo_uri, self.limit),\n",
    "\t\t\t'smp': SMP(self.mongo_uri, self.limit),\n",
    "\t\t\t'srd': SRD(self.mongo_uri, self.limit),\n",
    "\t\t\t'stel': Stel(self.sql_params),\n",
    "\t\t\t'radcom': Radcom(self.sql_params),\n",
    "\t\t\t'aero': Aero(),\n",
    "\t\t}\n",
    "\n",
    "\tdef extraction(self) -> L:\n",
    "\t\tif self.parallel:\n",
    "\t\t\tsources = parallel(\n",
    "\t\t\t\tEstacoes._update_source,\n",
    "\t\t\t\tself.sources.values(),\n",
    "\t\t\t\tn_workers=len(self.sources),\n",
    "\t\t\t\tprogress=True,\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tsources = L(self._update_source(s) for s in self.sources.values())\n",
    "\t\treturn sources.attrgot('df')\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef verify_shapefile_folder():\n",
    "\t\t# Convert the file paths to Path objects\n",
    "\t\tshapefile_path = Path(IBGE_POLIGONO)\n",
    "\t\tparent_folder = shapefile_path.parent\n",
    "\t\tparent_folder.mkdir(exist_ok=True, parents=True)\n",
    "\t\tzip_file_path = parent_folder.with_suffix('.zip')\n",
    "\n",
    "\t\t# Check if all required files exist\n",
    "\t\trequired_files = L('.cpg', '.dbf', '.prj', '.shx').map(shapefile_path.with_suffix)\n",
    "\t\tif not all(required_files.map(Path.is_file)):\n",
    "\t\t\t# shutil.rmtree(str(shapefile_path.parent), ignore_errors=True)\n",
    "\t\t\tparent_folder.ls().map(Path.unlink)\n",
    "\t\t\t# Download and unzip the zipped folder\n",
    "\t\t\turllib.request.urlretrieve(MALHA_IBGE, zip_file_path)\n",
    "\t\t\twith ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "\t\t\t\tzip_ref.extractall(parent_folder)\n",
    "\t\t\tzip_file_path.unlink()\n",
    "\n",
    "\tdef fill_nan_coordinates(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,  # DataFrame com os dados da Anatel\n",
    "\t) -> pd.DataFrame:  # DataFrame com as coordenadas validadas na base do IBGE\n",
    "\t\t\"\"\"Valida as coordenadas consultado a Base Corporativa do IBGE, excluindo o que já está no cache na versão anterior\"\"\"\n",
    "\n",
    "\t\tmunicipios = pd.read_csv(\n",
    "\t\t\tIBGE_MUNICIPIOS,\n",
    "\t\t\tusecols=['Código_Município', 'Latitude', 'Longitude'],\n",
    "\t\t\tdtype='string[pyarrow]',\n",
    "\t\t\tdtype_backend='pyarrow',\n",
    "\t\t)\n",
    "\n",
    "\t\tdf['Código_Município'] = df['Código_Município'].astype('string[pyarrow]')\n",
    "\n",
    "\t\tdf = pd.merge(\n",
    "\t\t\tdf,\n",
    "\t\t\tmunicipios,\n",
    "\t\t\ton='Código_Município',\n",
    "\t\t\thow='left',\n",
    "\t\t\tcopy=False,\n",
    "\t\t)\n",
    "\n",
    "\t\tnull_coords = df.Latitude_x.isna() | df.Longitude_x.isna()\n",
    "\n",
    "\t\tdf.loc[null_coords, ['Latitude_x', 'Longitude_x']] = df.loc[\n",
    "\t\t\tnull_coords, ['Latitude_y', 'Longitude_y']\n",
    "\t\t]\n",
    "\n",
    "\t\tlog = \"\"\"[(\"Colunas\", [\"Latitude\", \"Longitude\"]),\n",
    "\t\t\t\t   (\"Processamento\", \"Coordenadas Ausentes. Inserido coordenadas do Município\")]\"\"\"\n",
    "\t\tdf = self.register_log(df, log, null_coords)\n",
    "\n",
    "\t\tdf.rename(\n",
    "\t\t\tcolumns={\n",
    "\t\t\t\t'Latitude_x': 'Latitude',\n",
    "\t\t\t\t'Longitude_x': 'Longitude',\n",
    "\t\t\t\t'Latitude_y': 'Latitude_ibge',\n",
    "\t\t\t\t'Longitude_y': 'Longitude_ibge',\n",
    "\t\t\t},\n",
    "\t\t\tinplace=True,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn df\n",
    "\n",
    "\tdef intersect_coordinates_on_poligon(self, df: pd.DataFrame, check_municipio: bool = True):\n",
    "\t\tfor column in ['Latitude', 'Longitude']:\n",
    "\t\t\tdf[column] = pd.to_numeric(df[column], errors='coerce').astype('float')\n",
    "\t\tregions = gpd.read_file(IBGE_POLIGONO)\n",
    "\n",
    "\t\t# Convert pandas dataframe to geopandas df with geometry point given coordinates\n",
    "\t\tgdf_points = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude))\n",
    "\n",
    "\t\t# Set the same coordinate reference system (CRS) as the regions shapefile\n",
    "\t\tgdf_points.crs = regions.crs\n",
    "\n",
    "\t\t# Spatial join points to the regions\n",
    "\t\tgdf = gpd.sjoin(gdf_points, regions, how='inner', predicate='within')\n",
    "\n",
    "\t\tif check_municipio:\n",
    "\t\t\t# Check correctness of Coordinates\n",
    "\t\t\tcheck_coords = gdf.Código_Município != gdf.CD_MUN\n",
    "\n",
    "\t\t\tlog = \"\"\"[(\"Colunas\", [\"Código_Município\", \"Município\", \"UF\"]),\n",
    "\t\t\t\t  \t (\"Processamento\", \"Informações substituídas  pela localização correta das coordenadas.\")\t\t      \n",
    "\t\t\t\t  \"\"\"\n",
    "\t\t\tself.register_log(gdf, log, check_coords)\n",
    "\n",
    "\t\t\tgdf.drop(\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\t'Código_Município',\n",
    "\t\t\t\t\t'Município',\n",
    "\t\t\t\t\t'UF',\n",
    "\t\t\t\t\t'geometry',\n",
    "\t\t\t\t\t'AREA_KM2',\n",
    "\t\t\t\t\t'index_right',\n",
    "\t\t\t\t],\n",
    "\t\t\t\taxis=1,\n",
    "\t\t\t\tinplace=True,\n",
    "\t\t\t)\n",
    "\n",
    "\t\tgdf.rename(\n",
    "\t\t\tcolumns={\n",
    "\t\t\t\t'CD_MUN': 'Código_Município',\n",
    "\t\t\t\t'NM_MUN': 'Município',\n",
    "\t\t\t\t'SIGLA_UF': 'UF',\n",
    "\t\t\t},\n",
    "\t\t\tinplace=True,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn gdf\n",
    "\n",
    "\tdef validate_coordinates(self, df: pd.DataFrame, check_municipio: bool = True) -> pd.DataFrame:\n",
    "\t\t\"\"\"\n",
    "\t\tValidates the coordinates in the given DataFrame.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t                df: The DataFrame containing the coordinates to be validated.\n",
    "\t\t                check_municipio: A boolean indicating whether to check the municipality information (default: True).\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t                pd.DataFrame: The DataFrame with validated coordinates.\n",
    "\n",
    "\t\tRaises:\n",
    "\t\t                None\n",
    "\t\t\"\"\"\n",
    "\t\tself.verify_shapefile_folder()\n",
    "\t\tif check_municipio:\n",
    "\t\t\tdf = self.fill_nan_coordinates(df)\n",
    "\t\treturn self.intersect_coordinates_on_poligon(df, check_municipio)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _simplify_sources(df):\n",
    "\t\tdf['Fonte'] = df['Fonte'].str.replace(\n",
    "\t\t\t'ICAO-CANALIZACAO-VOR/ILS/DME | AISWEB-CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t\t'CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t)\n",
    "\t\tdf['Fonte'] = df['Fonte'].str.replace(\n",
    "\t\t\tr'(ICAO-)?(AISWEB-)?CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t\t'CANALIZACAO-VOR/ILS/DME',\n",
    "\t\t\tregex=True,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn df\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2float(column: pd.Series) -> pd.Series:\n",
    "\t\treturn pd.to_numeric(\n",
    "\t\t\tcolumn.fillna('-1'),\n",
    "\t\t\tdowncast='float',\n",
    "\t\t\terrors='coerce',\n",
    "\t\t\tdtype_backend='pyarrow',\n",
    "\t\t)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2int(column: pd.Series) -> pd.Series:\n",
    "\t\treturn pd.to_numeric(\n",
    "\t\t\tcolumn.fillna('0'),\n",
    "\t\t\tdowncast='unsigned',\n",
    "\t\t\terrors='coerce',\n",
    "\t\t\tdtype_backend='pyarrow',\n",
    "\t\t)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2str(column: pd.Series) -> pd.Series:\n",
    "\t\tcolumn.replace('', '-1', inplace=True)\n",
    "\t\treturn column.astype('string', copy=False).fillna('-1')\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _cast2cat(column: pd.Series) -> pd.Series:\n",
    "\t\tcolumn.replace('', '-1', inplace=True)\n",
    "\t\treturn column.fillna('-1').astype('category', copy=False)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _remove_invalid_frequencies(df):\n",
    "\t\tdf.sort_values(['Frequência', 'Latitude', 'Longitude'], ignore_index=True, inplace=True)\n",
    "\t\treturn df[df['Frequência'] <= LIMIT_FREQ]\n",
    "\t\t# TODO: save to discarded and log\n",
    "\t\t# log = f\"\"\"[(\"Colunas\", \"Frequência\"),\n",
    "\t\t# \t\t   (\"Processamento\", \"Frequência Inválida: Maior que {LIMIT_FREQ}\")\n",
    "\t\t# \t\t  \"\"\"\n",
    "\t\t# self.register_log(df, log, check_coords)\n",
    "\n",
    "\tdef _save(self, df: pd.DataFrame, folder: Union[str, Path], stem: str) -> pd.DataFrame:\n",
    "\t\t\"\"\"Format, Save and return a dataframe\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tfile = Path(f'{folder}/{stem}.parquet.gzip')\n",
    "\t\t\tdf.to_parquet(file, compression='gzip', index=False, engine='pyarrow')\n",
    "\t\texcept (ArrowInvalid, ArrowTypeError) as e:\n",
    "\t\t\traise e(f'Não foi possível salvar o arquivo parquet {file}') from e\n",
    "\t\treturn df\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _format_types(df):\n",
    "\t\tdf['Frequência'] = df['Frequência'].astype('double[pyarrow]')\n",
    "\t\tfor col in FLOAT_COLUMNS:\n",
    "\t\t\tdf[col] = Estacoes._cast2float(df[col])\n",
    "\t\tfor col in INT_COLUMNS:\n",
    "\t\t\tdf[col] = Estacoes._cast2int(df[col])\n",
    "\t\tfor col in CAT_COLUMNS:\n",
    "\t\t\tdf[col] = Estacoes._cast2cat(df[col])\n",
    "\t\tfor col in STR_COLUMNS:\n",
    "\t\t\tdf[col] = Estacoes._cast2str(df[col])\n",
    "\t\treturn df\n",
    "\n",
    "\tdef _format(\n",
    "\t\tself,\n",
    "\t\tdfs: List,  # List with the individual API sources\n",
    "\t) -> pd.DataFrame:  # Processed DataFrame\n",
    "\t\taero = dfs.pop()\n",
    "\t\tanatel = pd.concat(dfs, ignore_index=True)\n",
    "\t\tdf = merge_on_frequency(anatel, aero)\n",
    "\t\tdf = self.validate_coordinates(df)\n",
    "\t\tdf = Estacoes._simplify_sources(df)\n",
    "\t\tdf = Estacoes._format_types(df)\n",
    "\t\tdf = Estacoes._remove_invalid_frequencies(df)\n",
    "\t\treturn df.loc[:, self.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>>>>>> 'afae4c62807917569071bab4ddda2a74bb0eeadb'`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
