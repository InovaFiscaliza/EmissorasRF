{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp estacoes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatel\n",
    "\n",
    "> Este módulo consolida as bases da Anatel e realiza pós-processamento dos dados obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import urllib.request\n",
    "from typing import List\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from fastcore.foundation import L\n",
    "from fastcore.parallel import parallel\n",
    "from fastcore.xtras import Path\n",
    "\n",
    "from extracao.constants import (\n",
    "    COLS_SRD,\n",
    "    IBGE_MUNICIPIOS,\n",
    "    IBGE_POLIGONO,\n",
    "    MALHA_IBGE,\n",
    "    FLOAT_COLUMNS,\n",
    "    INT_COLUMNS,\n",
    "    STR_COLUMNS,\n",
    "    CAT_COLUMNS,\n",
    ")\n",
    "from extracao.datasources.aeronautica import Aero\n",
    "from extracao.datasources.base import Base\n",
    "from extracao.datasources.mosaico import MONGO_URI\n",
    "from extracao.datasources.sitarweb import SQLSERVER_PARAMS, Radcom, Stel\n",
    "from extracao.datasources.smp import SMP\n",
    "from extracao.datasources.srd import SRD\n",
    "from extracao.datasources.telecom import Telecom\n",
    "from extracao.format import merge_on_frequency, LIMIT_FREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Consolidada ANATEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class Estacoes(Base):\n",
    "    \"\"\"Classe auxiliar para agregar os dados originários da Anatel\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sql_params: dict = SQLSERVER_PARAMS,\n",
    "        mongo_uri: str = MONGO_URI,\n",
    "        limit: int = 0,\n",
    "        parallel: bool = True,\n",
    "    ):\n",
    "        self.sql_params = sql_params\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.limit = limit\n",
    "        self.parallel = parallel\n",
    "        self.init_data_sources()\n",
    "\n",
    "    @property\n",
    "    def columns(self):\n",
    "        return COLS_SRD\n",
    "\n",
    "    def build_from_sources(self) -> pd.DataFrame:\n",
    "        return self._format([s.df() for s in self.sources.values()])\n",
    "\n",
    "    @property\n",
    "    def stem(self):\n",
    "        return \"estacoes\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_source(class_instance):\n",
    "        class_instance.update()\n",
    "        class_instance.save()\n",
    "        return class_instance\n",
    "\n",
    "    def init_data_sources(self):\n",
    "        self.sources = {\n",
    "            \"telecom\": Telecom(self.mongo_uri, self.limit),\n",
    "            \"smp\": SMP(self.mongo_uri, self.limit),\n",
    "            \"srd\": SRD(self.mongo_uri, self.limit),\n",
    "            # 'stel': Stel(self.sql_params),\n",
    "            # 'radcom': Radcom(self.sql_params),\n",
    "            \"aero\": Aero(),\n",
    "        }\n",
    "\n",
    "    def extraction(self) -> L:\n",
    "        if self.parallel:\n",
    "            sources = parallel(\n",
    "                Estacoes._update_source,\n",
    "                self.sources.values(),\n",
    "                n_workers=len(self.sources),\n",
    "                progress=True,\n",
    "            )\n",
    "        else:\n",
    "            sources = L(self._update_source(s) for s in self.sources.values())\n",
    "        return sources.attrgot(\"df\")\n",
    "\n",
    "    @staticmethod\n",
    "    def verify_shapefile_folder():\n",
    "        # Convert the file paths to Path objects\n",
    "        shapefile_path = Path(IBGE_POLIGONO)\n",
    "        parent_folder = shapefile_path.parent\n",
    "        parent_folder.mkdir(exist_ok=True, parents=True)\n",
    "        zip_file_path = parent_folder.with_suffix(\".zip\")\n",
    "\n",
    "        # Check if all required files exist\n",
    "        required_files = L(\".cpg\", \".dbf\", \".prj\", \".shx\").map(\n",
    "            shapefile_path.with_suffix\n",
    "        )\n",
    "        if not all(required_files.map(Path.is_file)):\n",
    "            # shutil.rmtree(str(shapefile_path.parent), ignore_errors=True)\n",
    "            parent_folder.ls().map(Path.unlink)\n",
    "            # Download and unzip the zipped folder\n",
    "            urllib.request.urlretrieve(MALHA_IBGE, zip_file_path)\n",
    "            with ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(parent_folder)\n",
    "            zip_file_path.unlink()\n",
    "\n",
    "    def fill_nan_coordinates(\n",
    "        self,\n",
    "        df: pd.DataFrame,  # DataFrame com os dados da Anatel\n",
    "    ) -> pd.DataFrame:  # DataFrame com as coordenadas validadas na base do IBGE\n",
    "        \"\"\"Valida as coordenadas consultado a Base Corporativa do IBGE, excluindo o que já está no cache na versão anterior\"\"\"\n",
    "\n",
    "        municipios = pd.read_csv(\n",
    "            IBGE_MUNICIPIOS,\n",
    "            usecols=[\"Código_Município\", \"Latitude\", \"Longitude\"],\n",
    "            dtype=\"string[pyarrow]\",\n",
    "            dtype_backend=\"pyarrow\",\n",
    "        )\n",
    "\n",
    "        df[\"Código_Município\"] = df[\"Código_Município\"].astype(\"string[pyarrow]\")\n",
    "\n",
    "        df = pd.merge(\n",
    "            df,\n",
    "            municipios,\n",
    "            on=\"Código_Município\",\n",
    "            how=\"left\",\n",
    "            copy=False,\n",
    "        )\n",
    "\n",
    "        null_coords = df.Latitude_x.isna() | df.Longitude_x.isna()\n",
    "\n",
    "        df.loc[null_coords, [\"Latitude_x\", \"Longitude_x\"]] = df.loc[\n",
    "            null_coords, [\"Latitude_y\", \"Longitude_y\"]\n",
    "        ]\n",
    "\n",
    "        log = \"\"\"[(\"Colunas\", [\"Latitude\", \"Longitude\"]),\n",
    "\t\t           (\"Processamento\", \"Coordenadas Ausentes. Inserido coordenadas do Município\")]\"\"\"\n",
    "        df = self.register_log(df, log, null_coords)\n",
    "\n",
    "        df.rename(\n",
    "            columns={\n",
    "                \"Latitude_x\": \"Latitude\",\n",
    "                \"Longitude_x\": \"Longitude\",\n",
    "                \"Latitude_y\": \"Latitude_ibge\",\n",
    "                \"Longitude_y\": \"Longitude_ibge\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def intersect_coordinates_on_poligon(\n",
    "        self, df: pd.DataFrame, check_municipio: bool = True\n",
    "    ):\n",
    "        for column in [\"Latitude\", \"Longitude\"]:\n",
    "            df[column] = pd.to_numeric(df[column], errors=\"coerce\").astype(\"float\")\n",
    "        regions = gpd.read_file(IBGE_POLIGONO)\n",
    "\n",
    "        # Convert pandas dataframe to geopandas df with geometry point given coordinates\n",
    "        gdf_points = gpd.GeoDataFrame(\n",
    "            df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude)\n",
    "        )\n",
    "\n",
    "        # Set the same coordinate reference system (CRS) as the regions shapefile\n",
    "        gdf_points.crs = regions.crs\n",
    "\n",
    "        # Spatial join points to the regions\n",
    "        gdf = gpd.sjoin(gdf_points, regions, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "        if check_municipio:\n",
    "            # Check correctness of Coordinates\n",
    "            check_coords = gdf.Código_Município != gdf.CD_MUN\n",
    "\n",
    "            log = \"\"\"[(\"Colunas\", [\"Código_Município\", \"Município\", \"UF\"]),\n",
    "\t\t\t\t  \t (\"Processamento\", \"Informações substituídas  pela localização correta das coordenadas.\")\t\t      \n",
    "\t\t\t\t  \"\"\"\n",
    "            self.register_log(gdf, log, check_coords)\n",
    "\n",
    "            gdf.drop(\n",
    "                [\n",
    "                    \"Código_Município\",\n",
    "                    \"Município\",\n",
    "                    \"UF\",\n",
    "                    \"geometry\",\n",
    "                    \"AREA_KM2\",\n",
    "                    \"index_right\",\n",
    "                ],\n",
    "                axis=1,\n",
    "                inplace=True,\n",
    "            )\n",
    "\n",
    "        gdf.rename(\n",
    "            columns={\n",
    "                \"CD_MUN\": \"Código_Município\",\n",
    "                \"NM_MUN\": \"Município\",\n",
    "                \"SIGLA_UF\": \"UF\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        return gdf\n",
    "\n",
    "    def validate_coordinates(\n",
    "        self, df: pd.DataFrame, check_municipio: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Validates the coordinates in the given DataFrame.\n",
    "\n",
    "        Args:\n",
    "                df: The DataFrame containing the coordinates to be validated.\n",
    "                check_municipio: A boolean indicating whether to check the municipality information (default: True).\n",
    "\n",
    "        Returns:\n",
    "                pd.DataFrame: The DataFrame with validated coordinates.\n",
    "\n",
    "        Raises:\n",
    "                None\n",
    "        \"\"\"\n",
    "        self.verify_shapefile_folder()\n",
    "        if check_municipio:\n",
    "            df = self.fill_nan_coordinates(df)\n",
    "        return self.intersect_coordinates_on_poligon(df, check_municipio)\n",
    "\n",
    "    @staticmethod\n",
    "    def _simplify_sources(df):\n",
    "        df[\"Fonte\"] = df[\"Fonte\"].str.replace(\n",
    "            \"ICAO-CANALIZACAO-VOR/ILS/DME | AISWEB-CANALIZACAO-VOR/ILS/DME\",\n",
    "            \"CANALIZACAO-VOR/ILS/DME\",\n",
    "        )\n",
    "        df[\"Fonte\"] = df[\"Fonte\"].str.replace(\n",
    "            r\"(ICAO-)?(AISWEB-)?CANALIZACAO-VOR/ILS/DME\",\n",
    "            \"CANALIZACAO-VOR/ILS/DME\",\n",
    "            regex=True,\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _cast2float(column: pd.Series) -> pd.Series:\n",
    "        return pd.to_numeric(\n",
    "            column.fillna(\"-1\"),\n",
    "            downcast=\"float\",\n",
    "            errors=\"coerce\",\n",
    "            dtype_backend=\"pyarrow\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _cast2int(column: pd.Series) -> pd.Series:\n",
    "        return pd.to_numeric(\n",
    "            column.fillna(\"0\"),\n",
    "            downcast=\"unsigned\",\n",
    "            errors=\"coerce\",\n",
    "            dtype_backend=\"pyarrow\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _cast2str(column: pd.Series) -> pd.Series:\n",
    "        column.replace(\"\", \"-1\", inplace=True)\n",
    "        return column.astype(\"string\", copy=False).fillna(\"-1\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _cast2cat(column: pd.Series) -> pd.Series:\n",
    "        column.replace(\"\", \"-1\", inplace=True)\n",
    "        return column.fillna(\"-1\").astype(\"category\", copy=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_invalid_frequencies(df):\n",
    "        df.sort_values(\n",
    "            [\"Frequência\", \"Latitude\", \"Longitude\"], ignore_index=True, inplace=True\n",
    "        )\n",
    "        return df[df[\"Frequência\"] <= LIMIT_FREQ]\n",
    "        # TODO: save to discarded and log\n",
    "        # log = f\"\"\"[(\"Colunas\", \"Frequência\"),\n",
    "        # \t\t   (\"Processamento\", \"Frequência Inválida: Maior que {LIMIT_FREQ}\")\n",
    "        # \t\t  \"\"\"\n",
    "        # self.register_log(df, log, check_coords)\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_types(df):\n",
    "        df[\"Frequência\"] = df[\"Frequência\"].astype(\"double[pyarrow]\")\n",
    "        for col in FLOAT_COLUMNS:\n",
    "            df[col] = Estacoes._cast2float(df[col])\n",
    "        for col in INT_COLUMNS:\n",
    "            df[col] = Estacoes._cast2int(df[col])\n",
    "        for col in CAT_COLUMNS:\n",
    "            df[col] = Estacoes._cast2cat(df[col])\n",
    "        for col in STR_COLUMNS:\n",
    "            df[col] = Estacoes._cast2str(df[col])\n",
    "        return df\n",
    "\n",
    "    def _format(\n",
    "        self,\n",
    "        dfs: List,  # List with the individual API sources\n",
    "    ) -> pd.DataFrame:  # Processed DataFrame\n",
    "        aero = dfs.pop()\n",
    "        anatel = pd.concat(dfs, ignore_index=True)\n",
    "        df = merge_on_frequency(anatel, aero)\n",
    "        df = self.validate_coordinates(df)\n",
    "        df = Estacoes._simplify_sources(df)\n",
    "        df = Estacoes._format_types(df)\n",
    "        df = Estacoes._remove_invalid_frequencies(df)\n",
    "        return df.loc[:, self.columns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
