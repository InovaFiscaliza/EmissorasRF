{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp estacoes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatel\n",
    "\n",
    "> Este módulo consolida as bases da Anatel e realiza pós-processamento dos dados obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import urllib.request\n",
    "from typing import List\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from fastcore.foundation import L\n",
    "from fastcore.parallel import parallel\n",
    "from fastcore.xtras import Path\n",
    "\n",
    "from extracao.constants import COLS_SRD, IBGE_MUNICIPIOS, IBGE_POLIGONO, MALHA_IBGE\n",
    "from extracao.datasources.aeronautica import Aero\n",
    "from extracao.datasources.base import Base\n",
    "from extracao.datasources.mosaico import MONGO_URI\n",
    "from extracao.datasources.sitarweb import SQLSERVER_PARAMS, Radcom, Stel\n",
    "from extracao.datasources.smp import SMP\n",
    "from extracao.datasources.srd import SRD\n",
    "from extracao.datasources.telecom import Telecom\n",
    "from extracao.format import merge_on_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Consolidada ANATEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class Estacoes(Base):\n",
    "    \"\"\"Classe auxiliar para agregar os dados originários da Anatel\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sql_params: dict = SQLSERVER_PARAMS,\n",
    "        mongo_uri: str = MONGO_URI,\n",
    "        limit: int = 0,\n",
    "        parallel: bool = True,\n",
    "    ):\n",
    "        self.sql_params = sql_params\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.limit = limit\n",
    "        self.parallel = parallel\n",
    "        self.init_data_sources()\n",
    "\n",
    "    @property\n",
    "    def columns(self):\n",
    "        return COLS_SRD\n",
    "\n",
    "    def build_from_sources(self) -> pd.DataFrame:\n",
    "        return self._format([s.df() for s in self.sources.values()])\n",
    "\n",
    "    @property\n",
    "    def stem(self):\n",
    "        return \"anatel\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_source(class_instance):\n",
    "        class_instance.update()\n",
    "        class_instance.save()\n",
    "        return class_instance\n",
    "\n",
    "    def init_data_sources(self):\n",
    "        self.sources = {\n",
    "            \"telecom\": Telecom(self.mongo_uri, self.limit),\n",
    "            \"smp\": SMP(self.mongo_uri, self.limit),\n",
    "            \"srd\": SRD(self.mongo_uri),\n",
    "            \"stel\": Stel(self.sql_params),\n",
    "            \"radcom\": Radcom(self.sql_params),\n",
    "            \"aero\": Aero(),\n",
    "        }\n",
    "\n",
    "    def extraction(self) -> L:\n",
    "        if self.parallel:\n",
    "            sources = parallel(\n",
    "                Estacoes._update_source,\n",
    "                self.sources.values(),\n",
    "                n_workers=len(self.sources),\n",
    "                progress=True,\n",
    "            )\n",
    "        else:\n",
    "            sources = L(self._update_source(s) for s in self.sources.values())\n",
    "        return sources.attrgot(\"df\")\n",
    "\n",
    "    @staticmethod\n",
    "    def verify_shapefile_folder():\n",
    "        # Convert the file paths to Path objects\n",
    "        shapefile_path = Path(IBGE_POLIGONO)\n",
    "        parent_folder = shapefile_path.parent\n",
    "        parent_folder.mkdir(exist_ok=True, parents=True)\n",
    "        zip_file_path = parent_folder.with_suffix(\".zip\")\n",
    "\n",
    "        # Check if all required files exist\n",
    "        required_files = L(\".cpg\", \".dbf\", \".prj\", \".shx\").map(\n",
    "            shapefile_path.with_suffix\n",
    "        )\n",
    "        if not all(required_files.map(Path.is_file)):\n",
    "            # shutil.rmtree(str(shapefile_path.parent), ignore_errors=True)\n",
    "            parent_folder.ls().map(Path.unlink)\n",
    "            # Download and unzip the zipped folder\n",
    "            urllib.request.urlretrieve(MALHA_IBGE, zip_file_path)\n",
    "            with ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(parent_folder)\n",
    "            zip_file_path.unlink()\n",
    "\n",
    "    def fill_nan_coordinates(\n",
    "        self,\n",
    "        df: pd.DataFrame,  # DataFrame com os dados da Anatel\n",
    "    ) -> pd.DataFrame:  # DataFrame com as coordenadas validadas na base do IBGE\n",
    "        \"\"\"Valida as coordenadas consultado a Base Corporativa do IBGE, excluindo o que já está no cache na versão anterior\"\"\"\n",
    "\n",
    "        municipios = pd.read_csv(\n",
    "            IBGE_MUNICIPIOS,\n",
    "            usecols=[\"Código_Município\", \"Latitude\", \"Longitude\"],\n",
    "            dtype=\"string[pyarrow]\",\n",
    "            dtype_backend=\"pyarrow\",\n",
    "        )\n",
    "\n",
    "        df = pd.merge(df, municipios, on=\"Código_Município\", how=\"left\", copy=False)\n",
    "\n",
    "        null_coords = df.Latitude_x.isna() | df.Longitude_x.isna()\n",
    "\n",
    "        df.loc[null_coords, [\"Latitude_x\", \"Longitude_x\"]] = df.loc[\n",
    "            null_coords, [\"Latitude_y\", \"Longitude_y\"]\n",
    "        ]\n",
    "\n",
    "        log = \"\"\"[(\"Colunas\", [\"Latitude\", \"Longitude\"]),\n",
    "\t\t           (\"Processamento\", \"Coordenadas Ausentes. Inserido coordenadas do Município\")]\"\"\"\n",
    "        df = self.register_log(df, log, null_coords)\n",
    "\n",
    "        df.rename(\n",
    "            columns={\n",
    "                \"Latitude_x\": \"Latitude\",\n",
    "                \"Longitude_x\": \"Longitude\",\n",
    "                \"Latitude_y\": \"Latitude_ibge\",\n",
    "                \"Longitude_y\": \"Longitude_ibge\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def intersect_coordinates_on_poligon(\n",
    "        self, df: pd.DataFrame, check_municipio: bool = True\n",
    "    ):\n",
    "        for column in [\"Latitude\", \"Longitude\"]:\n",
    "            df[column] = pd.to_numeric(df[column], errors=\"coerce\").astype(\"float\")\n",
    "        regions = gpd.read_file(IBGE_POLIGONO)\n",
    "\n",
    "        # Convert pandas dataframe to geopandas df with geometry point given coordinates\n",
    "        gdf_points = gpd.GeoDataFrame(\n",
    "            df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude)\n",
    "        )\n",
    "\n",
    "        # Set the same coordinate reference system (CRS) as the regions shapefile\n",
    "        gdf_points.crs = regions.crs\n",
    "\n",
    "        # Spatial join points to the regions\n",
    "        gdf = gpd.sjoin(gdf_points, regions, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "        if check_municipio:\n",
    "            # Check correctness of Coordinates\n",
    "            check_coords = gdf.Código_Município != gdf.CD_MUN\n",
    "\n",
    "            log = \"\"\"[(\"Colunas\", [\"Código_Município\", \"Município\", \"UF\"]),\n",
    "\t\t\t\t  \t (\"Processamento\", \"Informações substituídas  pela localização correta das coordenadas.\")\t\t      \n",
    "\t\t\t\t  \"\"\"\n",
    "            self.register_log(gdf, log, check_coords)\n",
    "\n",
    "            gdf.drop([\"Código_Município\", \"Município\", \"UF\"], axis=1, inplace=True)\n",
    "\n",
    "        gdf.rename(\n",
    "            columns={\n",
    "                \"CD_MUN\": \"Código_Município\",\n",
    "                \"NM_MUN\": \"Município\",\n",
    "                \"SIGLA_UF\": \"UF\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        return gdf\n",
    "\n",
    "    def validate_coordinates(\n",
    "        self, df: pd.DataFrame, check_municipio: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Validates the coordinates in the given DataFrame.\n",
    "\n",
    "        Args:\n",
    "                df: The DataFrame containing the coordinates to be validated.\n",
    "                check_municipio: A boolean indicating whether to check the municipality information (default: True).\n",
    "\n",
    "        Returns:\n",
    "                pd.DataFrame: The DataFrame with validated coordinates.\n",
    "\n",
    "        Raises:\n",
    "                None\n",
    "        \"\"\"\n",
    "        self.verify_shapefile_folder()\n",
    "        if check_municipio:\n",
    "            df = self.fill_nan_coordinates(df)\n",
    "        return self.intersect_coordinates_on_poligon(df, check_municipio)\n",
    "\n",
    "    def _format(\n",
    "        self,\n",
    "        dfs: List,  # List with the individual API sources\n",
    "    ) -> pd.DataFrame:  # Processed DataFrame\n",
    "        aero = dfs.pop()\n",
    "        anatel = pd.concat(dfs, ignore_index=True)\n",
    "        df = self.validate_coordinates(merge_on_frequency(anatel, aero))\n",
    "        df.sort_values(\n",
    "            [\"Frequência\", \"Latitude\", \"Longitude\"], ignore_index=True, inplace=True\n",
    "        )\n",
    "        return df.loc[:, self.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: fals\n",
    "SQLSERVER_PARAMS = dict(\n",
    "\tdriver='{ODBC Driver 17 for SQL Server}',\n",
    "\tserver='ANATELBDRO05',\n",
    "\tdatabase='SITARWEB',\n",
    "\ttrusted_conn=False,\n",
    "\tmult_results=True,\n",
    "\tencrypt=False,\n",
    "\tusername=os.environ['USERNAME'],\n",
    "\tpassword=os.environ['PASSWORD'],\n",
    "\ttimeout=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = Estacoes(sql_params=SQLSERVER_PARAMS, limit=100000, parallel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsilva/rfdatahub/extracao/datasources/sitarweb.py:54: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(self.query, self.connect(), dtype='category')\n",
      "/home/rsilva/rfdatahub/extracao/datasources/sitarweb.py:54: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(self.query, self.connect(), dtype='category')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfs = st.extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequência</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Entidade</th>\n",
       "      <th>Fonte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1143</td>\n",
       "      <td>-32.340056</td>\n",
       "      <td>-54.223889</td>\n",
       "      <td>NDB - MELO</td>\n",
       "      <td>AISGEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2000</td>\n",
       "      <td>-22.923333</td>\n",
       "      <td>-42.071500</td>\n",
       "      <td>NDB - CABO FRIO</td>\n",
       "      <td>AISGEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2050</td>\n",
       "      <td>-5.386167</td>\n",
       "      <td>-35.531000</td>\n",
       "      <td>NDB - MAXARANGUAPE</td>\n",
       "      <td>AISGEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2100</td>\n",
       "      <td>-19.561000</td>\n",
       "      <td>-46.964667</td>\n",
       "      <td>NDB - ARAXA</td>\n",
       "      <td>AISGEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2250</td>\n",
       "      <td>-25.408667</td>\n",
       "      <td>-54.621667</td>\n",
       "      <td>NDB - ITAIPU</td>\n",
       "      <td>AISGEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>995.0000</td>\n",
       "      <td>-23.631380</td>\n",
       "      <td>-46.652060</td>\n",
       "      <td>DME - AEROPORTO DE CONGONHAS 34X</td>\n",
       "      <td>AISGEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>995.0000</td>\n",
       "      <td>-5.771808</td>\n",
       "      <td>-35.350192</td>\n",
       "      <td>DME - SAO GONCALO 34X</td>\n",
       "      <td>AISGEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>995.0000</td>\n",
       "      <td>-9.336946</td>\n",
       "      <td>-54.952428</td>\n",
       "      <td>DME - CACHIMBO 34X</td>\n",
       "      <td>AISGEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>995.0000</td>\n",
       "      <td>-9.509805</td>\n",
       "      <td>-35.786944</td>\n",
       "      <td>DME - IMC 34X</td>\n",
       "      <td>AISGEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>997.0000</td>\n",
       "      <td>-15.878367</td>\n",
       "      <td>-47.906864</td>\n",
       "      <td>DME - BRASILIA 36X</td>\n",
       "      <td>AISGEO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2995 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Frequência   Latitude  Longitude                          Entidade  \\\n",
       "0         0.1143 -32.340056 -54.223889                       NDB - MELO    \n",
       "1         0.2000 -22.923333 -42.071500                  NDB - CABO FRIO    \n",
       "2         0.2050  -5.386167 -35.531000               NDB - MAXARANGUAPE    \n",
       "3         0.2100 -19.561000 -46.964667                      NDB - ARAXA    \n",
       "4         0.2250 -25.408667 -54.621667                     NDB - ITAIPU    \n",
       "...          ...        ...        ...                               ...   \n",
       "2990    995.0000 -23.631380 -46.652060  DME - AEROPORTO DE CONGONHAS 34X   \n",
       "2991    995.0000  -5.771808 -35.350192             DME - SAO GONCALO 34X   \n",
       "2992    995.0000  -9.336946 -54.952428                DME - CACHIMBO 34X   \n",
       "2993    995.0000  -9.509805 -35.786944                     DME - IMC 34X   \n",
       "2994    997.0000 -15.878367 -47.906864                DME - BRASILIA 36X   \n",
       "\n",
       "       Fonte  \n",
       "0     AISGEO  \n",
       "1     AISGEO  \n",
       "2     AISGEO  \n",
       "3     AISGEO  \n",
       "4     AISGEO  \n",
       "...      ...  \n",
       "2990  AISGEO  \n",
       "2991  AISGEO  \n",
       "2992  AISGEO  \n",
       "2993  AISGEO  \n",
       "2994  AISGEO  \n",
       "\n",
       "[2995 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aero = dfs.pop()\n",
    "aero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fonte\n",
       "ICAO                                             1871\n",
       "ICAO | AISWEB                                     265\n",
       "AISGEO                                            230\n",
       "ICAO-CANALIZACAO-VOR                              174\n",
       "RADARES                                           154\n",
       "AISWEB                                            103\n",
       "AISWEB-CANALIZACAO-VOR                             78\n",
       "ICAO-CANALIZACAO-VOR | AISWEB-CANALIZACAO-VOR      48\n",
       "ICAO | AISGEO                                      46\n",
       "REDEMET                                            26\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aero.Fonte.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "O Agrupamento por colunas únicas não tem o comprimento esperado: 0!= 2110",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/rsilva/rfdatahub/nbs/04_estacoes.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsas.anatel.gov.br/home/rsilva/rfdatahub/nbs/04_estacoes.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m anatel \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dfs, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsas.anatel.gov.br/home/rsilva/rfdatahub/nbs/04_estacoes.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m merge_on_frequency(anatel, aero)\n",
      "File \u001b[0;32m/home/rsilva/rfdatahub/extracao/format.py:198\u001b[0m, in \u001b[0;36mmerge_on_frequency\u001b[0;34m(df_left, df_right, on, cols2merge)\u001b[0m\n\u001b[1;32m    194\u001b[0m df_both_left \u001b[39m=\u001b[39m df_both\u001b[39m.\u001b[39mgroupby(filter_left_cols, as_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mfirst()\n\u001b[1;32m    195\u001b[0m df_both_right \u001b[39m=\u001b[39m df_both\u001b[39m.\u001b[39mgroupby(filter_right_cols, as_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mfirst()\n\u001b[1;32m    197\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mlen\u001b[39m(df_both_left) \u001b[39m==\u001b[39m intersection_left\n\u001b[1;32m    199\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mO Agrupamento por colunas únicas não tem o comprimento esperado: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(df_both_left)\u001b[39m}\u001b[39;00m\u001b[39m!= \u001b[39m\u001b[39m{\u001b[39;00mintersection_left\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    202\u001b[0m     \u001b[39mlen\u001b[39m(df_both_right) \u001b[39m==\u001b[39m intersection_right\n\u001b[1;32m    203\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(df_both_right)\u001b[39m}\u001b[39;00m\u001b[39m!= \u001b[39m\u001b[39m{\u001b[39;00mintersection_right\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m df_both_far_left \u001b[39m=\u001b[39m df_both_left[df_both_left\u001b[39m.\u001b[39mDistance \u001b[39m>\u001b[39m MAX_DIST]\n",
      "\u001b[0;31mAssertionError\u001b[0m: O Agrupamento por colunas únicas não tem o comprimento esperado: 0!= 2110"
     ]
    }
   ],
   "source": [
    "anatel = pd.concat(dfs, ignore_index=True)\n",
    "df = merge_on_frequency(anatel, aero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/rsilva/rfdatahub/extracao/format.py\u001b[0m(198)\u001b[0;36mmerge_on_frequency\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    196 \u001b[0;31m\tassert (\n",
      "\u001b[0m\u001b[0;32m    197 \u001b[0;31m                \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_both_left\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mintersection_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 198 \u001b[0;31m\t), f'O Agrupamento por colunas únicas não tem o comprimento esperado: {len(df_both_left)}!= {intersection_left}'\n",
      "\u001b[0m\u001b[0;32m    199 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    200 \u001b[0;31m\tassert (\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frequência', 'Entidade_x', 'Fistel', 'Número_Estação', 'Município', 'Código_Município', 'UF', 'Latitude_x', 'Longitude_x', 'Classe', 'Num_Serviço', 'Classe_Emissão', 'Largura_Emissão(kHz)', 'Validade_RF', 'Status', 'Fonte_x', 'Multiplicidade', 'Log', 'Altura_Estação', 'Potência_Transmissor(W)', 'Ganho_Antena(dBd)', 'Ângulo_Elevação_Antena', 'Azimute_Antena', 'Altura_Antena(m)', 'Atenuação_Linha(db/100m)', 'Perdas_Acessórias_Linha(db)', 'Padrão_Antena(dBd)']\n",
      "['Frequência', 'Entidade_x', 'Fistel', 'Número_Estação', 'Município', 'Código_Município', 'UF', 'Latitude_x', 'Longitude_x', 'Classe', 'Num_Serviço', 'Classe_Emissão', 'Largura_Emissão(kHz)', 'Validade_RF', 'Status', 'Fonte_x', 'Multiplicidade', 'Log', 'Altura_Estação', 'Potência_Transmissor(W)', 'Ganho_Antena(dBd)', 'Ângulo_Elevação_Antena', 'Azimute_Antena', 'Altura_Antena(m)', 'Atenuação_Linha(db/100m)', 'Perdas_Acessórias_Linha(db)', 'Padrão_Antena(dBd)']\n"
     ]
    }
   ],
   "source": [
    "%debug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
