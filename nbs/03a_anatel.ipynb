{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp outorgadas\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatel\n",
    "\n",
    "> Este módulo consolida as bases da Anatel e realiza pós-processamento dos dados obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "from extracao.datasources.sitarweb import Stel, Radcom, SQLSERVER_PARAMS\n",
    "from extracao.datasources.mosaico import MONGO_URI\n",
    "from extracao.datasources.srd import SRD\n",
    "from extracao.datasources.telecom import Telecom\n",
    "from extracao.datasources.smp import SMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Consolidada ANATEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Outorgadas:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_coords(\n",
    "    row: pd.Series,  # Linha de um DataFrame\n",
    "    connector: pyodbc.Connection = None,  # Conector de Banco de Dados\n",
    ") -> List[str]:  # DataFrame com dados do município\n",
    "    \"\"\"Valida os dados de coordenadas e município em `row` no polígono dos municípios em banco corporativ do IBGE\"\"\"\n",
    "\n",
    "    mun, cod, lat, long = (\n",
    "        row.Município,\n",
    "        row.Código_Município,\n",
    "        row.Latitude,\n",
    "        row.Longitude,\n",
    "    )\n",
    "    is_valid = \"-1\"\n",
    "    conn = connect_db() if connector is None else connector\n",
    "    crsr = conn.cursor()\n",
    "    sql = SQL_VALIDA_COORD.format(long, lat, cod)\n",
    "    crsr.execute(sql)\n",
    "    result = crsr.fetchone()\n",
    "    if result is not None:\n",
    "        mun = result.NO_MUNICIPIO\n",
    "        lat = result.NU_LATITUDE\n",
    "        long = result.NU_LONGITUDE\n",
    "        is_valid = result.COORD_VALIDA\n",
    "    if connector is None:\n",
    "        del conn\n",
    "    return [str(mun), str(lat), str(long), str(is_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cached_df(df: pd.DataFrame, df_cache: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Mescla ambos dataframes eliminando os excluídos (existentes somente em df_cache)\"\"\"\n",
    "\n",
    "    # Merge dataframes based on all columns except \"Coords_Valida_IBGE\"\n",
    "    merged = pd.merge(\n",
    "        df_cache,\n",
    "        df,\n",
    "        on=list(df.columns),\n",
    "        how=\"outer\",\n",
    "        indicator=True,\n",
    "        copy=False,\n",
    "        validate=\"one_to_one\",\n",
    "    ).astype(\"string\")\n",
    "\n",
    "    # Exclude rows only present in df_cache\n",
    "    df_cache = merged[merged[\"_merge\"] != \"left_only\"]\n",
    "\n",
    "    # inplace=True not working\n",
    "    df_cache.loc[:, [\"Latitude\", \"Longitude\"]] = df_cache.loc[\n",
    "        :, [\"Latitude\", \"Longitude\"]\n",
    "    ].fillna(\"-1\")\n",
    "\n",
    "    # # Drop the _merge column\n",
    "    return df_cache.drop(columns=\"_merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validar_coords_base(\n",
    "    df: pd.DataFrame,  # DataFrame com os dados da Anatel\n",
    "    df_cache: pd.DataFrame,  # DataFrame validado anteriormente, usado como cache\n",
    "    connector: pyodbc.Connection = None,  # Conector de Banco de Dados\n",
    ") -> pd.DataFrame:  # DataFrame com as coordenadas validadas na base do IBGE\n",
    "    \"\"\"Valida as coordenadas consultado a Base Corporativa do IBGE, excluindo o que já está no cache na versão anterior\"\"\"\n",
    "\n",
    "    df_cache = update_cached_df(df.astype(\"string\"), df_cache.astype(\"string\"))\n",
    "\n",
    "    municipios = pd.read_csv(\n",
    "        Path(__file__).parent / \"arquivos\" / \"municipios_coordenadas.csv\",\n",
    "        usecols=[\"codigo_ibge\"],\n",
    "        dtype=\"string\",\n",
    "    )\n",
    "\n",
    "    # valida os códigos de municípios\n",
    "    valid_cod_mun = df_cache.Código_Município.isin(municipios.codigo_ibge)\n",
    "\n",
    "    df_cache.loc[~valid_cod_mun, \"Coords_Valida_IBGE\"] = \"-1\"\n",
    "\n",
    "    subset = df_cache.Coords_Valida_IBGE.isna()\n",
    "\n",
    "    if linhas := list(\n",
    "        df_cache.loc[\n",
    "            subset, [\"Município\", \"Código_Município\", \"Latitude\", \"Longitude\"]\n",
    "        ].itertuples()\n",
    "    ):\n",
    "        func = partialler(validar_coords, connector=connector)\n",
    "\n",
    "        # Gambiarra para evitar compartilhamento da mesma conexão de banco em diferentes threads\n",
    "        n_workers = 1 if connector is not None else 20\n",
    "\n",
    "        ibge = [\n",
    "            \"Município_IBGE\",\n",
    "            \"Latitude_IBGE\",\n",
    "            \"Longitude_IBGE\",\n",
    "            \"Coords_Valida_IBGE\",\n",
    "        ]\n",
    "\n",
    "        df_cache.loc[subset, ibge] = parallel(\n",
    "            func, linhas, threadpool=True, n_workers=n_workers, progress=True\n",
    "        )\n",
    "\n",
    "    df_cache = df_cache.astype(\"string\")\n",
    "\n",
    "    df_cache.loc[df_cache.Coords_Valida_IBGE == \"-1\", \"Coords_Valida_IBGE\"] = pd.NA\n",
    "\n",
    "    return df_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_base(\n",
    "    conn: pyodbc.Connection,  # Objeto de conexão de banco\n",
    "    clientMongoDB: MongoClient,  # Objeto de conexão com o MongoDB\n",
    "    folder: Union[str, Path],  # Pasta onde salvar os arquivos\n",
    "    conn_threads: bool = False,  # Flag para criar uma conexão de banco por thread\n",
    ") -> pd.DataFrame:  # DataFrame com os dados atualizados\n",
    "    # sourcery skip: use-fstring-for-concatenation\n",
    "    \"\"\"Wrapper que atualiza opcionalmente lê e atualiza as 4 bases indicadas anteriormente, as combina e salva o arquivo consolidado na folder `folder`\"\"\"\n",
    "    stel = update_stel(conn, folder)\n",
    "    radcom = update_radcom(conn, folder)\n",
    "    mosaico = update_srd(clientMongoDB, folder)\n",
    "    telecom = update_telecom(clientMongoDB, folder)\n",
    "\n",
    "    df = (\n",
    "        pd.concat([mosaico, radcom, stel, telecom])\n",
    "        .sort_values([\"Frequência\", \"Latitude\", \"Longitude\"])\n",
    "        .reset_index(drop=True)\n",
    "    ).astype(\"string\")\n",
    "\n",
    "    # inplace not working!\n",
    "    df.loc[:, [\"Latitude\", \"Longitude\"]] = df.loc[:, [\"Latitude\", \"Longitude\"]].fillna(\n",
    "        \"0\"\n",
    "    )\n",
    "    try:\n",
    "        df_cache = _read_df(folder, \"base\")\n",
    "    except FileNotFoundError:\n",
    "        df_cache = pd.DataFrame(columns=df.columns.to_list() + [\"Coords_Valida_IBGE\"])\n",
    "\n",
    "    connector = None if conn_threads else conn\n",
    "\n",
    "    df_cache = _validar_coords_base(df, df_cache, connector)\n",
    "\n",
    "    return _save_df(df_cache, folder, \"base\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
