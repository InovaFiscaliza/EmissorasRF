# AUTOGENERATED! DO NOT EDIT! File to edit: ..\nbs\updates.ipynb.

# %% auto 0
__all__ = ['connect_db', 'clean_mosaico', 'update_radcom', 'update_stel', 'update_mosaico', 'update_licenciamento', 'update_base']

# %% ..\nbs\updates.ipynb 2
from decimal import Decimal, getcontext
from typing import Union
from urllib.request import urlretrieve, URLError
import xmltodict
from zipfile import ZipFile

import pandas as pd
import pyodbc
from rich.console import Console
from pyarrow import ArrowInvalid, ArrowTypeError
from unidecode import unidecode
from fastcore.xtras import Path
from fastcore.foundation import L
from fastcore.test import test_eq
from tqdm.auto import tqdm
import pyodbc
from pymongo import MongoClient

from .constants import *
from .format import parse_bw, format_types, input_coordenates
from .functionsdb import ConsultaSRD

getcontext().prec = 5

# %% ..\nbs\updates.ipynb 4
def connect_db(server: str = 'ANATELBDRO05', # Servidor do Banco de Dados
               database: str = 'SITARWEB', # Nome do Banco de Dados
               trusted_conn: str = 'yes', # Conexão Segura: yes | no
               mult_results: bool = True, # Múltiplos Resultados
              )->pyodbc.Connection:
    """Conecta ao Banco `server` e retorna o 'cursor' (iterador) do Banco"""
    return pyodbc.connect(
        "Driver={ODBC Driver 17 for SQL Server};"
        f"Server={server};"
        f"Database={database};"
        f"Trusted_Connection={trusted_conn};"
        f"MultipleActiveResultSets={mult_results};",
        timeout=TIMEOUT,
    )

# %% ..\nbs\updates.ipynb 6
def _parse_estações(row: dict)->dict:
    """Given a row in a MongoDB ( a dict of dicts ), it travels some keys and return a subset dict"""
    
    d = {k.replace('@', '').lower():row[k] for k in ("@SiglaServico", "@id", "@state",
        "@entidade",
        "@fistel",
        "@cnpj",
        "@municipio",
        "@uf")}
    entidade = row.get('entidade', {})
    d.update({k.replace('@', '').lower():entidade[k] for k in ('@num_servico', '@habilitacao_DataValFreq')})
    administrativo = row.get('administrativo', {})
    d['numero_estacao'] = administrativo.get('@numero_estacao')
    estacao = row.get('estacao_principal', {})
    d.update({k.replace('@', '').lower():estacao[k] for k in ('@latitude', '@longitude')})
    return d

# %% ..\nbs\updates.ipynb 7
def _read_estações(path: Union[str, Path]) -> pd.DataFrame:
    """Read the zipped xml file `Estações.zip` from MOSAICO and returns a dataframe"""
    
    with ZipFile(path) as myzip:
        with myzip.open('estacao_rd.xml') as myfile:
            estacoes = xmltodict.parse(myfile.read())
            
    assert 'estacao_rd' in estacoes, "The xml file inside estacoes.zip is not in the expected format"
    assert 'row' in estacoes['estacao_rd'], "The xml file inside estacoes.zip is not in the expected format"
    
    df = pd.DataFrame(L(estacoes['estacao_rd']['row']).map(_parse_estações))
    df = df[df.state.str.contains("-C1$|-C2$|-C3$|-C4$|-C7|-C98$")].reset_index(drop=True)
    df = df.loc[:, COL_ESTACOES]
    df.columns = NEW_ESTACOES    
    for c in df.columns:
        df.loc[df[c] == "", c] = pd.NA
    return df

# %% ..\nbs\updates.ipynb 8
def _parse_pb(row: dict)->dict:
    """Given a row in the MongoDB file canais.zip ( a dict of dicts ), it travels some keys and return a subset dict"""
    return {unidecode(k).lower().replace("@", ""): v  for k,v in row.items()}

# %% ..\nbs\updates.ipynb 9
def _read_plano_basico(path: Union[str, Path]) -> pd.DataFrame:
    """Read the zipped xml file `Plano_Básico.zip` from MOSAICO and returns a dataframe"""    
    df = L()
    with ZipFile(path) as myzip:
        with myzip.open('plano_basicoTVFM.xml') as myfile:
            pbtvfm = xmltodict.parse(myfile.read())
        with myzip.open('plano_basicoAM.xml') as myfile:
            pbam = xmltodict.parse(myfile.read())
        with myzip.open('secundariosTVFM.xml') as myfile:
            stvfm = xmltodict.parse(myfile.read())
        with myzip.open('secundariosAM.xml') as myfile:
            sam = xmltodict.parse(myfile.read())    
            
    for base in (pbtvfm, stvfm, pbam, sam):
        assert 'plano_basico' in base, "The xml files inside canais.zip is not in the expected format"
        assert 'row' in base['plano_basico'], "The xml file inside canais.zip is not in the expected format"
        df.extend(L(base['plano_basico']['row']).map(_parse_pb))
        
    df = pd.DataFrame(df)
    df = df.loc[df.pais == "BRA", COL_PB].reset_index(drop=True)    
    df.columns = NEW_PB
    df = df[df.Status.str.contains("-C1$|-C2$|-C3$|-C4$|-C7|-C98$")].reset_index(drop=True)
    df.loc[:, 'Frequência'] = df.Frequência.str.replace(',', '.')
    for c in df.columns:
        df.loc[df[c] == '', c] = pd.NA
    return df    

# %% ..\nbs\updates.ipynb 10
def clean_mosaico(df: pd.DataFrame, # DataFrame com os dados de Estações e Plano_Básico mesclados 
                pasta: Union[str, Path], # Pasta com os dados de municípios para imputar coordenadas ausentes
) -> pd.DataFrame: # DataFrame com os dados mesclados e limpos
    """Clean the merged dataframe with the data from the MOSAICO page"""
    COLS = [c for c in df.columns if "_x" in c]
    for col in COLS:
        col_x = col
        col_y = col.split("_")[0] + "_y"
        df.loc[df[col_x].isna(), col_x] = df.loc[df[col_x].isna(), col_y]
        df.loc[df[col_y].isna(), col_y] = df.loc[df[col_y].isna(), col_x]
        if df[col_x].notna().sum() > df[col_y].notna().sum():
            a, b = col_x, col_y
        else:
            a, b = col_y, col_x
        df.drop(b, axis=1, inplace=True)
        df.rename({a: a[:-2]}, axis=1, inplace=True)

    # df.loc[df.Latitude_Transmissor == "", "Latitude_Transmissor"] = df.loc[
    #     df.Latitude_Transmissor == "", "Latitude_Estação"
    # ]
    # df.loc[df.Longitude_Transmissor == "", "Longitude_Transmissor"] = df.loc[
    #     df.Longitude_Transmissor == "", "Longitude_Estação"
    # ]
    
    # df.loc[df.Latitude_Transmissor.isna(), "Latitude_Transmissor"] = df.loc[
    #     df.Latitude_Transmissor.isna(), "Latitude_Estação"
    # ]
    # df.loc[df.Longitude_Transmissor.isna(), "Longitude_Transmissor"] = df.loc[
    #     df.Longitude_Transmissor.isna(), "Longitude_Estação"
    # ]
    # df.drop(["Latitude_Estação", "Longitude_Estação"], axis=1, inplace=True)
    # df.rename(
    #     columns={
    #         "Latitude_Transmissor": "Latitude",
    #         "Longitude_Transmissor": "Longitude",
    #     },
    #     inplace=True,
    # )

    df = input_coordenates(df, pasta)
    df.loc[:, "Frequência"] = df.Frequência.str.replace(",", ".")    
    df = df[df.Frequência.notna()].reset_index(drop=True)

    # Removido o código abaixo devido a inconsistência no Mosaico
    # if freq_nans := df[df.Frequência.isna()].Id.tolist():
    #     complement_df = scrape_dataframe(freq_nans)
    #     df.loc[
    #         df.Frequência.isna(),
    #         [
    #             "Status",
    #             "Entidade",
    #             "Fistel",
    #             "Frequência",
    #             "Classe",
    #             "Num_Serviço",
    #             "Município",
    #             "UF",
    #         ],
    #     ] = complement_df.values

    df.loc[:, "Frequência"] = df.Frequência.astype("float")
    df.loc[df.Serviço == "OM", "Frequência"] = df.loc[
        df.Serviço == "OM", "Frequência"
    ].apply(lambda x: Decimal(x) / Decimal(1000))
    df.loc[:, "Validade_RF"] = df.Validade_RF.astype("string").str.slice(0, 10)
    return df.drop_duplicates(keep="first").reset_index(drop=True)

# %% ..\nbs\updates.ipynb 12
def _save_df(df: pd.DataFrame, folder: Union[str, Path], stem: str) -> pd.DataFrame:
    """Format, Save and return a dataframe"""
    df = format_types(df, stem)
    df = df.drop_duplicates(keep='first').reset_index(drop=True)
    df = df.dropna(subset=['Latitude', 'Longitude']).reset_index(drop=True)
    try:
        file = Path(f"{folder}/{stem}.parquet.gzip")
        df.to_parquet(file, compression="gzip", index=False)
    except (ArrowInvalid, ArrowTypeError):
        file.unlink(missing_ok=True)
        try:
            file = Path(f"{folder}/{stem}.fth")
            df.to_feather(file)
        except (ArrowInvalid, ArrowTypeError):
            file.unlink(missing_ok=True)
            try:
                file = Path(f"{folder}/{stem}.xlsx")
                with pd.ExcelWriter(file) as wb:
                    df.to_excel(
                        wb, sheet_name="DataBase", engine="openpyxl", index=False
                    )
            except Exception as e:
                raise ValueError(f"Could not save {stem} to {file}") from e
    return df



# %% ..\nbs\updates.ipynb 13
def update_radcom(
        conn: pyodbc.Connection, # Objeto de conexão de banco
        folder: Union[str, Path] # Pasta onde salvar os arquivos
        
) -> pd.DataFrame: # DataFrame com os dados atualizados
    """Atualiza a tabela local retornada pela query `RADCOM`"""
    console = Console()
    with console.status(
        "[cyan]Lendo o Banco de Dados de Radcom...", spinner="earth"
    ) as status:
        try:            
            df = pd.read_sql_query(RADCOM, conn)
            return _save_df(df, folder, "radcom")
        except pyodbc.OperationalError as e:
            status.console.log(
                "Não foi possível abrir uma conexão com o SQL Server. Esta conexão somente funciona da rede cabeada!"
            )
            raise ConnectionError from e

# %% ..\nbs\updates.ipynb 14
def update_stel(
        conn: pyodbc.Connection, # Objeto de conexão de banco
        folder:Union[str, Path] # Pasta onde salvar os arquivos        
) -> pd.DataFrame: # DataFrame com os dados atualizados
    """Atualiza a tabela local retornada pela query `STEL`"""
    console = Console()
    with console.status(
        "[red]Lendo o Banco de Dados do STEL. Processo Lento, aguarde...",
        spinner="bouncingBall",
    ) as status:
        try:            
            df = pd.read_sql_query(STEL, conn)
            return _save_df(df, folder, "stel")
        except pyodbc.OperationalError as e:
            status.console.log(
                "Não foi possível abrir uma conexão com o SQL Server. Esta conexão somente funciona da rede cabeada!"
            )
            raise ConnectionError from e

# %% ..\nbs\updates.ipynb 16
def update_mosaico(        
        mongo_client: MongoClient, # Objeto de conexão com o MongoDB
        folder: Union[str, Path] # Pasta onde salvar os arquivos
) -> pd.DataFrame: # DataFrame com os dados atualizados
    """Efetua a query na tabela de Radiodifusão no banco mongoDB `mongo_client` e atualiza o arquivo local"""
    console = Console()
    with console.status(
        "Consolidando os dados do Mosaico...", spinner="clock"
    ) as status:  
        
        df = ConsultaSRD(mongo_client)
        df = clean_mosaico(df, folder)
    return _save_df(df, folder, "mosaico")

# %% ..\nbs\updates.ipynb 17
def update_licenciamento(mongo_client: MongoClient, # Objeto de conexão com o MongoDB
                         folder: Union[str, Path] # Pasta onde salvar os arquivos
)-> pd.DataFrame: # DataFrame com os dados atualizados
    """Efetua a query na tabela `licenciamento` no banco mongoDB `mongo_client` e atualiza o arquivo local"""
    database = mongo_client["sms"]
    collection = database["licenciamento"]

    query = {"$and" : [
        {u"DataExclusao" : None
        },
        {u"DataValidade": {
                u"$nin": [
                    u"",
                    None
                ]
            }
        },
        {
            u"NumServico": {
                u"$nin": [
                    u"010",
                    u"045",
                    u"171",
                    u"450",
                    u"750",
                    u"",
                    None
                ]
            }
        },
        {
            u"FreqTxMHz": {
                u"$nin": [
                    None,
                    u""
                ]
            }
        },
        {
            u"Latitude": {
                u"$nin": [
                    None,
                    u""
                ]
            }
        },
        {
            u"Longitude": {
                u"$nin": [
                    None,
                    u""
                ]
            }
        },
        {
            u"FreqTxMHz": {
                u"$type": 1.0
            }
        },
        {
            u"Latitude": {
                u"$type": 1.0
            }
        },
        {
            u"Longitude": {
                u"$type": 1.0
            }
        }
    ]
    }

    c = collection.find(query, projection={k:1.0 for k in LICENCIAMENTO.keys()})

    result = L()
    for doc in tqdm(c.batch_size(1000000000)):
        result.append(doc)

    df = pd.json_normalize(result)

    df.drop('_id', axis=1, inplace=True)

    df.rename(LICENCIAMENTO, axis=1, inplace=True)

    return df

    # for c in df.columns:
    #     if c not in ('Frequência', 'Latitude', 'Longitude'):
    #         df[c] = df[c].astype('string')



# %% ..\nbs\updates.ipynb 19
def update_base(
    conn: pyodbc.Connection, # Objeto de conexão de banco
    clientMongoDB: MongoClient, # Ojeto de conexão com o MongoDB
    folder: Union[str, Path] # Pasta onde salvar os arquivos    
) -> pd.DataFrame: # DataFrame com os dados atualizados
    # sourcery skip: use-fstring-for-concatenation
    """Wrapper que atualiza opcionalmente lê e atualiza as três bases indicadas anteriormente, as combina e salva o arquivo consolidado na folder `folder`"""
    stel = update_stel(conn, folder, ).loc[:, TELECOM]
    radcom = update_radcom(conn, folder).loc[:, SRD]
    mosaico = update_mosaico(clientMongoDB, folder).loc[:, RADIODIFUSAO]    
    radcom["Num_Serviço"] = "231"
    radcom["Status"] = "RADCOM"
    radcom["Classe_Emissão"] = pd.NA
    radcom["Largura_Emissão"] = BW_MAP["231"]
    radcom["Entidade"] = radcom.Entidade.str.rstrip().str.lstrip()
    radcom["Validade_RF"] = pd.NA
    radcom["Fonte"] = "SRD"
    stel["Status"] = "L"
    stel["Entidade"] = stel.Entidade.str.rstrip().str.lstrip()
    stel["Fonte"] = "STEL"
    mosaico["Fonte"] = "MOS"
    mosaico["Classe_Emissão"] = pd.NA
    mosaico["Largura_Emissão"] = mosaico.Num_Serviço.map(BW_MAP)
    rd = (
        pd.concat([mosaico, radcom, stel])
        .sort_values(["Frequência", "Latitude", "Longitude"])
        .reset_index(drop=True)
    )
    rd = rd.drop_duplicates(keep="first").reset_index(drop=True)
    rd["BW(kHz)"] = rd.Largura_Emissão.astype('string').fillna('-1').apply(parse_bw)
    return _save_df(rd, folder, "base")
