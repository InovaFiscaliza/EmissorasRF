# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03a_anatel.ipynb.

# %% auto 0
__all__ = ['Outorgadas']

# %% ../nbs/03a_anatel.ipynb 3
from typing import List, Tuple
from functools import cached_property
import pandas as pd
from dotenv import find_dotenv, load_dotenv
from fastcore.xtras import Path
from fastcore.parallel import parallel, partialler
from fastcore.foundation import L
from pyarrow import ArrowInvalid


from .datasources.sitarweb import Stel, Radcom, SQLSERVER_PARAMS
from .datasources.mosaico import MONGO_URI
from .datasources.srd import SRD
from .datasources.telecom import Telecom
from .datasources.smp import SMP
from .datasources.base import Base
from .datasources.aeronautica import Aero
from .datasources.connectors import SQLServer
from .constants import COLUNAS, SQL_VALIDA_COORD

# %% ../nbs/03a_anatel.ipynb 4
load_dotenv(find_dotenv())

# %% ../nbs/03a_anatel.ipynb 6
class Outorgadas(Base):
    """Classe auxiliar para agregar os dados originários da Anatel"""

    def __init__(
        self,
        sql_params: dict = SQLSERVER_PARAMS,
        mongo_uri: str = MONGO_URI,
        limit: int = 0,
        n_workers: int = 10,
    ):
        self.sql_params = sql_params
        self.mongo_uri = mongo_uri
        self.limit = limit
        self.n_workers = n_workers

    @property
    def columns(self):
        return COLUNAS

    @cached_property
    def df_cache(self) -> pd.DataFrame:
        try:
            df = self._read(self.stem)
        except ValueError:
            df = pd.DataFrame(columns=self.columns)
        return df

    @property
    def stem(self):
        return "anatel"

    @staticmethod
    def _update_instance(class_instance):
        class_instance.update()
        class_instance.save()
        return class_instance.df

    @cached_property
    def extraction(self) -> L:
        sources = [
            # Aero(),
            #            Stel(self.sql_params),
            #            Radcom(self.sql_params),
            SRD(self.mongo_uri),
            Telecom(self.mongo_uri, self.limit),
            SMP(self.mongo_uri, self.limit),
        ]
        return parallel(
            Outorgadas._update_instance, sources, n_workers=len(sources), progress=True
        )

    def update_cached_df(
        self,
        df: pd.DataFrame,
    ) -> pd.DataFrame:
        """Mescla ambos dataframes eliminando os excluídos (existentes somente em df_cache)"""

        if self.df_cache.empty:
            return df

        # Merge dataframes based on all columns except "Coords_Valida_IBGE"
        merged = pd.merge(
            self.df_cache,
            df,
            on=list(df.columns),
            how="outer",
            indicator=True,
            copy=False,
            validate="one_to_one",
        )

        # Exclude rows only present in df_cache
        df_cache = merged[merged["_merge"] != "left_only"]

        # inplace=True not working
        df_cache.loc[:, ["Latitude", "Longitude"]] = df_cache.loc[
            :, ["Latitude", "Longitude"]
        ].fillna(-1)

        # Drop the _merge column
        df_cache = df_cache.drop(columns="_merge")

        # Write the new dataframe to cache
        self.df_cache = df_cache

        return self.df_cache

    @staticmethod
    def intersect_coordinates_on_poligon(
        row: pd.Series,  # DataFrame row
        sql_params: dict,  # Connection parameters to pass to SQLServer
    ) -> Tuple:  # Tuple com dados do município
        """Valida os dados de coordenadas e município em `row` no polígono dos municípios em banco corporativo do IBGE"""

        mun, cod, lat, long = (
            row.Município,
            row.Código_Município,
            row.Latitude,
            row.Longitude,
        )
        is_valid = -1
        conn = SQLServer(sql_params).connect()
        crsr = conn.cursor()
        sql = SQL_VALIDA_COORD.format(long, lat, cod)
        crsr.execute(sql)
        result = crsr.fetchone()
        if result is not None:
            mun = result.NO_MUNICIPIO
            lat = result.NU_LATITUDE
            long = result.NU_LONGITUDE
            is_valid = result.COORD_VALIDA
        conn.close()
        return mun, lat, long, is_valid

    def validate_coordinates(
        self,
        df: pd.DataFrame,  # DataFrame com os dados da Anatel
    ) -> pd.DataFrame:  # DataFrame com as coordenadas validadas na base do IBGE
        """Valida as coordenadas consultado a Base Corporativa do IBGE, excluindo o que já está no cache na versão anterior"""

        df_cache = self.update_cached_df(df)

        municipios = pd.read_csv(
            Path(__file__).parent / "datasources" / "arquivos" / "municipios.csv",
            usecols=["codigo_ibge"],
            dtype="string",
        )

        # valida os códigos de municípios
        valid_cod_mun = df_cache.Código_Município.isin(municipios.codigo_ibge)

        df_cache.loc[~valid_cod_mun, "Coords_Valida_IBGE"] = -1

        subset = df_cache.Coords_Valida_IBGE.isna()

        if linhas := list(
            df_cache.loc[
                subset, ["Município", "Código_Município", "Latitude", "Longitude"]
            ].itertuples()
        ):
            ibge = [
                "Município_IBGE",
                "Latitude_IBGE",
                "Longitude_IBGE",
                "Coords_Valida_IBGE",
            ]

            func = partialler(
                Outorgadas.intersect_coordinates_on_poligon, sql_params=self.sql_params
            )

            df_cache.loc[subset, ibge] = parallel(
                func, linhas, threadpool=True, n_workers=self.n_workers, progress=True
            )

        df_cache.loc[df_cache.Coords_Valida_IBGE == -1, "Coords_Valida_IBGE"] = pd.NA

        return df_cache

    def _format(
        self,
        dfs: List,  # List with the individual API sources
    ) -> pd.DataFrame:  # Processed DataFrame
        df = pd.concat(dfs, ignore_index=True).sort_values(
            ["Frequência", "Latitude", "Longitude"], ignore_index=True
        )

        # inplace not working!
        df.loc[:, ["Latitude", "Longitude"]] = (
            df.loc[:, ["Latitude", "Longitude"]].astype("string").fillna("0")
        )

        return df
